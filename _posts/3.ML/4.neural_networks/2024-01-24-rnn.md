---
order: 3
title: Recurrent Neural Networks
date: 2024-01-24
categories: [3.MACHINE LEARNING TECHS, 4.neural networks]
tags: [machine learning, deep learning, neural networks, rnn, time series, nlp]
math: true
description: >-
    Based on the following lectures <br>
    (1) “Intro. to Deep Learning (2023-2)” by Prof. Seong Man An, Dept. of Data Science, The Grad. School, Kookmin Univ. <br>
    (2) “Text Analytics (2024-1)” by Prof. Je Hyuk Lee, Dept. of Data Science, The Grad. School, Kookmin Univ.
image:
  path: /_post_refer_img/3.ML/4.neural_networks/Thumbnail.jpg
---

## Why? Recurrent-Net
-----

- Time series data is data\\
where there is a `sequence between features`:

    ![01](/_post_refer_img/3.ML/4.neural_networks/03-01.png){: width="100%"}

- `Fully connected layers` treat the positions of input features equally,\\
so they do not structurally reflect `order information` between features:

    ![02](/_post_refer_img/3.ML/4.neural_networks/03-02.png){: width="100%"}

- RNN(`R`ecurrent `N`eural `N`etworks) involves `preprocessing` operations\\
that preserve `sequence information`:

    ![03](/_post_refer_img/3.ML/4.neural_networks/03-03.png){: width="100%"}


## Vanilla RNN
-----

![04](/_post_refer_img/3.ML/4.neural_networks/03-04.png){: width="100%"}

- update hidden state $\mathbf{z}_{t}$:

    $$\begin{aligned}
    \mathbf{z}_{t}
    &= \text{tanh}(\mathbf{U}\cdot\mathbf{x}_{t}+\mathbf{W}\cdot\mathbf{z}_{t-1}+\mathbf{b}_{h})
    \end{aligned}$$

    - $\text{tanh}$ : activation function
    - $\mathbf{x}_{t}$ : input value @ $t$
    - $\mathbf{U}$ : weight matrix of input value @ $t$
    - $\mathbf{z}_{t-1}$ : hidden state @ $t-1$
    - $\mathbf{W}$ : weight matrix of hidden state @ $t-1$
    - $\mathbf{b}_{h}$ : bias

- print output $\mathbf{y}_{t}$

    $$\begin{aligned}
    \mathbf{y}_{t}
    &= \text{softmax}(\mathbf{V}\cdot\mathbf{z}_{t}+\mathbf{b}_{o})
    \end{aligned}$$

    - $\text{softmax}$ : activation function
    - $\mathbf{z}_{t}$ : hidden state @ $t$
    - $\mathbf{V}$ : weight matrix of hidden state @ $t$
    - $\mathbf{b}_{o}$ : bias

## LSTM
-----

- vanilla rnn suffers from the problems of `long-term dependencies`:

    ![05](/_post_refer_img/3.ML/4.neural_networks/03-05.png){: width="100%"}

    - `Long-term dependencies` are problems in which the `initial order information` is not preserved as the sequence gets longer due to the `vanishing gradient`.

- LSTM(`L`ong `S`hort-`T`erm `M`emory) is technique to alleviate vanishing gradient through `gate adjustment`:

    ![07](/_post_refer_img/3.ML/4.neural_networks/03-07.jpeg){: width="100%"}

    - `forget gate`: generate `forget rule`
    - `input gate`: generate `remember rule` and `cell state update`
    - `cell state`: `determine` how much to remember and how much to forget
    - `output gate`: generate `hidden state` and `output`

- forget gate:

    ![08](/_post_refer_img/3.ML/4.neural_networks/03-08.png){: width="100%"}

- input gate:

    ![09](/_post_refer_img/3.ML/4.neural_networks/03-09.png){: width="100%"}

- cell state:

    ![10](/_post_refer_img/3.ML/4.neural_networks/03-10.png){: width="100%"}

- output gate:

    ![11](/_post_refer_img/3.ML/4.neural_networks/03-11.png){: width="100%"}

-----

## Sourse

- https://dgkim5360.tistory.com/entry/understanding-long-short-term-memory-lstm-kr