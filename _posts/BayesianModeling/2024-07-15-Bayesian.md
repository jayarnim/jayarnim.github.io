---
order: 1
title: What? Bayesian
date: 2024-07-15
categories: [Statistical Techs, Bayesian Modeling]
tags: [Statistics, Bayesian, Optimization, MLE]
math: true
description: >-
    Based on the lecture “Bayesian Modeling (2024-1)” by Prof. Yeo Jin Chung, Dept. of AI, Big Data & Management, College of Business Administration, Kookmin Univ.
image:
    path: /_post_refer_img/BayesianModeling/Thumbnail.jpeg
---

## Compare Frequentist and Bayesian
-----

- **불확실성(Uncertainty)** : 정보 부족 또는 모호성으로 인해 결과나 상황을 완전히 알거나 예측할 수 없는 상태
	- **우발적 불확실성(Aleatoric Uncertainty)** : 데이터 생성 과정에 내재된 변동성이나 관측의 무작위성으로 인해 발생하는 불확실성
	- **지식적 불확실성(Epistemic Uncertainty)** : 인간 지식 체계의 결함에서 발생하는 불확실성

- **확률(Probability)** : 주어진 모수 하에서 데이터가 관찰될 가능성을 나타내는 값
	- **빈도주의(Frequentist)** : 특정 사건이 무한히 반복되었을 때 나타나는 상대적 빈도

        $$
        P(X)
        $$

	- **베이지안(Bayesian)** : 주어진 지식 체계 하에서 특정 사건이 발생할 것이라는 믿음 혹은 확신의 척도

        $$
        P(X \mid \theta)
        $$

- **모수(Parameter)** : 데이터 생성 과정의 근본적인 원리를 설명하는 상수

	- **빈도주의(Frequentist)**: 

        $$
        \theta=\mu, \sigma^2, p, \cdots
        $$

		> **모수는 밝혀지지 않았으나 객관적이고 절대적인 진리(상수)로서 존재한다.** 데이터는 모수의 정체가 무엇이라는 주장의 근거로 작용하며, 이 주장의 신뢰성은 표본의 크기와 품질에 의해 전적으로 결정된다. 따라서 데이터의 불확실성(Aleatoric Uncertainty)을 모델링하는 것만으로도 모수 추정을 위한 충분한 근거가 된다.

	- **베이지안(Bayesian)**: 

        $$
        \theta \sim F
        $$

		> 모수는 객관적이고 절대적인 진리일 수 있다. 그러나 인간의 불완전한 지식 체계 하에서는 **그 값을 정확히 알거나 확실하게 추정할 수 없다.** 따라서 **모수는 확률로 표현되어야 한다.** 이 표현은 (1) 주어진 지식 체계 하에서 (2) 관측된 데이터에 근거했을 때 모수가 어떠할 것이라는 믿음을 나타낸다. <br> 모수를 추정할 때는 데이터가 내재적으로 가지는 불확실성(Aleatoric Uncertainty)뿐만 아니라, 기존 지식이 가지는 불확실성(Epistemic Uncertainty)까지 함께 모델링해야 한다.

## Sample Size: The Necessity for Bayesian
-----

- **대수의 법칙과 작은 수의 혼란**

    > 표본의 크기는 결코 크지 않다. 만일 N이 충분한 추정을 얻기에 부족하다면 더 많은 데이터(또는 더 많은 가정)을 확보해야 한다. 그러나 일단 N이 충분히 크다면 데이터를 나눠 더 많은 것(가령 여론조사에서 전국적으로 훌륭한 추정을 얻었다면 남과 여, 지역별, 연령대별 그룹 등으로 나눠 추정할 수 있다)을 얻을 수 있다. N은 충분하지 않다. 만약 충분하더라도 여러분은 이미 더 많은 데이터가 필요한 다음 문제에 직면하기 때문이다. (Andrew Gelman)

    - **대수의 법칙(Law of Large Numbers)** : 무작위 실험을 반복해서 수행할수록 관측된 상대적 빈도(추정량)가 이론적 확률(모수)에 가까워진다는 법칙
    - **작은 수의 혼란(Law of Small Numbers)** : 적은 표본에서도 대수의 법칙이 적용될 것이라고 잘못 가정하거나 소규모 데이터로 도출된 결과를 일반화하려는 오류

- **If the sample size is sufficient,**

    $$
    \lim_{n \to \infty} P(\theta \mid X_{1}, X_{2}, \cdots, X_{n}) \to \mathcal{N}(\theta_{\text{true}}, \frac{\sigma^2}{n})
    $$

    - 사건 발생에 대한 믿음은 신규 관측치에 의해 끊임없이 갱신되어 참값을 중심으로 집중됨
    - 이는 빈도주의적 추정과 베이지안 추론의 결과가 장기적으로 일치함을 의미함

- **If the sample size is not large enough,**
    - **빈도주의자(Frequentist)** : 신뢰수준을 상향 조정하여 표본의 변동성에 따른 모수 추정 과정 상의 불확실성을 모델링함

        $$
        \text{CI}_{\theta}=\bar{X}\pm Z\cdot\frac{\sigma}{\sqrt{n}}
        $$

        - **신뢰구간** : 표본을 반복적으로 무작위추출하여 계산했을 때 참 모수를 포함할 비율이 신뢰수준에 해당하는 구간으로서, 가령 95% 신뢰수준에서는 100번의 표본 추출로 신뢰구간을 계산했을 때, 약 95개의 신뢰구간이 참 모수를 포함함

    - **베이지안(Bayesian)** : 모수의 정체를 알 수 없음에서 비롯된 모수에 대한 믿음의 불확실성을 확률로써 표현함

        $$
        P(\theta \mid X)=\frac{P(X \mid \theta)\cdot P(\theta)}{P(X)}
        $$

## Bayesian Framework
-----

### Modules

- **사전 확률 분포(Prior Probability Distribution)** : 사전 지식의 불확실성을 모델링하는 모듈로서, 사전 정보를 기초로 설정된 초기 믿음

    $$
    \theta \sim P
    $$

- **우도(Likelihood or Likelihood Function)** : 데이터의 불확실성을 모델링하는 모듈로서, 모수가 $\theta$ 로 주어졌을 때, 관측된 데이터 $\mathcal{D}$ 가 실현될 가능성

    $$\begin{aligned}
    \mathcal{L}(\theta)
    =P(\mathcal{D} \mid \theta)
    =\prod_{i=1}^{n}{f(X_{i} \mid \theta)}
    \end{aligned}$$

- **사후 확률 분포(Posterior Probability Distribution)** : 사건 발생 후, 추가된 정보(발생된 데이터)를 고려하여 갱신된 믿음

    $$
    \theta \mid \mathcal{D} \sim P
    $$

### Bayes' Theorem

- **Bayes' Theorem**

    > 사상 $A_{1},A_{2},\cdots,A_{n}$ 이 표본공간 $S$ 의 분할이고, $P(A_{i})>0,P(X)>0$ 이면 $X$ 에 대한 $A_{k}$ 의 조건부 확률은 다음과 같음

    $$\begin{aligned}
    P(A_{k} \mid X)
    &=\frac{P(A_{k} \cap X)}{P(X)}\\
    &=\frac{P(A_{k})\cdot P(X \mid A_{k})}{P(X)}\\
    &=\frac{P(A_{k})\cdot P(X \mid A_{k})}{\sum_{i=1}^{n}{P(A_{i}) \cdot P(X \mid A_{i})}} \propto P(A_{k})\cdot P(X \mid A_{k})
    \end{aligned}$$

    - $P(A_{k} \mid X)$ : 사건 $X$ 발생 조건부 $A_k$ 발생 확률
    - $P(A_{k})$ : 사건 $A_{k}$ 발생 확률
    - $P(X \mid A_{k})$ : 사건 $A_{k}$ 발생 조건부 $X$ 발생 확률
    - $P(X)$ : 사건 $X$ 발생 확률

- **Application to Posterior Estimation**

    $$
    P(\theta \mid \mathcal{D})=\frac{P(\theta) \cdot P(\mathcal{D} \mid \theta)}{P(\mathcal{D})} \propto \pi(\theta) \cdot \mathcal{L}(\theta)
    $$

    - $\theta$ : 모수
    - $\mathcal{D}$ : 관측된 데이터
    - $P(\theta \mid \mathcal{D})$ : $\theta$ 의 사후 확률 분포
    - $\pi(\theta)=P(\theta)$ : $\theta$ 의 사전 확률 분포
    - $\mathcal{L}(\theta)=P(\mathcal{D} \mid \theta)$ : $\theta$ 에 대한 $\mathcal{D}$ 의 우도
    - $P(\mathcal{D})$ : $\mathcal{D}$ 의 주변 확률

### Posterior Estimation

- **Bayes' Theorem**

- [**`M`arkov `C`hain `M`onte `C`arlo**](https://jayarnim.github.io/posts/MCMC/) : Sampling

- [**`V`ariational `I`nference**](https://jayarnim.github.io/posts/VI/) : Parametric Approximation

- [**`S`tein `V`ariational `G`radient `D`escent**](https://jayarnim.github.io/posts/SVGD/) : Non-Parametric Approximation

## `[example]` 동전 던지기
-----

> 동전을 던졌을 때 앞면이 나올 확률을 $\theta$ 라고 하자. $\theta$ 에 대한 정보가 아무것도 없다고 가정하자. 즉, $\theta$ 는 0과 1 사이의 무작위수일 것이라고 믿어지고 있다. 동전을 두 번 던졌는데 두 번 다 앞면이 나왔다. 그렇다면 $\theta$ 에 대한 믿음은 어떻게 변화할까?

![01](/_post_refer_img/BayesianModeling/01-01.png){: width="100%"}

- **문제에서 주어진 정보**

    $$\begin{aligned}
    X \mid \theta &\sim \text{Bin}(n,\theta)\\
    \theta &\sim \text{Uniform}(0,1)
    \end{aligned}$$

- **목적 함수 정의**

    $$
    P(\theta \mid X)=\frac{\pi(\theta)P(X \mid \theta)}{P(X)} \propto \pi(\theta)P(X \mid \theta)
    $$

    - $\pi(\theta)$ : $\theta$ 에 대한 사전확률

        $$
        \pi(\theta)=1 \quad \because \theta \sim \text{Uniform}(0,1)
        $$

    - $P(X \mid \theta)$ : $\theta$ 에 대한 $X$ 의 우도함수 혹은 확률질량함수

        $$
        P(X \mid \theta)=\frac{n!}{X!(n-X)!} \cdot \theta^{X} \cdot (1-\theta)^{n-X} \quad \because X \mid \theta \sim \text{Bin}(n,\theta)
        $$

    - $P(X)$ : $X$ 의 주변확률로서 정규화 상수($\theta$ 에 대한 함수가 아니므로 이후 생략)

        $$\begin{aligned}
        P(X)
        &= \int_{0}^{1}{\frac{n!}{X!(n-X)!} \cdot \theta^{X} \cdot (1-\theta)^{n-X}}\text{d}\theta\\
        &= \frac{n!}{X!(n-X)!} \cdot \int_{0}^{1}{\theta^{X} \cdot (1-\theta)^{n-X}}\text{d}\theta\\
        &= \frac{n!}{X!(n-X)!} \cdot \text{B}(X+1,n-X+1)\\
        &= \frac{n!}{X!(n-X)!} \cdot \frac{\Gamma(X+1)\Gamma(n-X+1)}{\Gamma(n+2)} \quad \because (X+1),(n-X+1) \in \mathbf{R}^{+}\\
        &= \frac{n!}{X!(n-X)!} \cdot \frac{X!(n-X)!}{(n+1)!} \quad \because (X+1),(n-X+1) \in \mathbf{Z}^{+}\\
        &= \frac{1}{n+1}
        \end{aligned}$$

        - $\text{B}(\alpha,\beta)$ : 베타함수

            $$\begin{aligned}
            \text{B}(\alpha,\beta)
            &= \int_{0}^{1}{t^{\alpha-1}(1-t)^{\beta-1}}\text{d}t\\
            &= \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)} \quad \text{where}\; \alpha,\beta \in \mathbf{R}^{+}
            \end{aligned}$$

        - $\Gamma(n)$ : 감마함수

            $$\begin{aligned}
            \Gamma(n)
            &= \int_{0}^{\infty}{t^{(n-1)}e^{-t}}\text{d}t\\
            &= (n-1)! \quad \text{where}\; n \in \mathbf{Z}^{+}
            \end{aligned}$$

- **결론** : $\theta$ 의 사후확률은 베타분포 $\text{Beta}(3,1)$ 을 따름

    $$\begin{aligned}
    P(\theta \mid X)
    &\propto \pi(\theta)P(X \mid \theta)\\
    &= \frac{n!}{X!(n-X)!} \cdot \theta^{X} \cdot (1-\theta)^{n-X}\\
    \\
    \therefore P(\theta \mid X)
    &\sim \text{Beta}(X+1,n-X+1)
    \end{aligned}$$

    - $\text{Beta}(\alpha,\beta)$ : 베타분포

        $$
        f(x)
        = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{\text{B}(\alpha,\beta)}
        \sim \text{Beta}(\alpha,\beta)
        $$