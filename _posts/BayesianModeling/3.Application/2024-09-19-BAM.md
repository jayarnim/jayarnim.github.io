---
order: 8
title: Bayesian Attention Modules
date: 2024-09-19
categories: [BAYES, 3.bayes applications]
tags: [Bayesian, Deep Learning, Attention Mechanism, Variational Inference]
math: true
description: >-
    Fan, X., Zhang, S., Chen, B., & Zhou, M.<br>
    (2020).<br>
    <a href=" https://doi.org/10.48550/arXiv.2010.10604">Bayesian attention modules.</a><br>
    Advances in Neural Information Processing Systems, 33, 16362-16376.
image:
    path: /_post_refer_img/BayesianModeling/3.Application/Thumbnail.jpg
---

## Bayesian Attention Modules
-----

- **불확실성(Uncertainty)**: 어떠한 사건에 대하여 확신할 수 없는 상태
    - **우발적 불확실성(Aleatoric Uncertainty)**: 관측치에 내재된 잡음 혹은 무작위성으로 인해 발생하는 불확실성

    - **인식적 불확실성(Epistemic Uncertainty)**: 정보 불완전성으로 인하여 발생하는 불확실성
        - **정보 불완전성(Data Uncertainty)**: 관측 데이터가 불완전하거나(Incomplete), 불충분하거나(Insufficient), 왜곡된(Distorted) 상황
        - **파라미터 불확실성(Parameter Uncertainty)**: 주어진 모형 안에서 최적 파라미터를 유일하게 식별할 수 없는 문제
        - **구조적 불확실성(Structural Uncertainty)**: 데이터 생성 메커니즘을 기술하는 함수 구조를 확신할 수 없어 특정 모형을 가정할 수 없는 문제
        - **표현의 불확실성(Latent Representation Uncertainty)**: 잠재공간의 비식별성으로 인해 치역(관측값)만으로 정의역(잠재요인)을 유일하게 결정하여 표현할 수 없는 문제

    - **존재론적 불확실성(Ontic Uncertainty)**: 모형이 모사하고자 하는 실제 메커니즘이 본질적으로 불확실하여 정보가 증가하더라도 제거될 수 없는 불확실성

- **문제 의식**: 요청 정보와 참조 정보 간 대응 관계는 관측값만으로는 드러나지 않는 잠재 구조로서, 표현의 불확실성을 내포함

- **Bayesian Attention Modules**: 어텐션 스코어에 실증적 베이지안 프레임워크를 적용하여 요청 정보와 참조 정보 간 관계 대응의 불확실성을 반영하는 방법론
    - 어텐션 스코어를 확정적인 값을 취하는 상수가 아니라 확률 분포를 따르는 확률변수로 가정함
    - 요청 정보와 참조 정보 간 유사도 함수 값의 지수변환을 어텐션 스코어의 기대값으로 간주함
    - 참조 정보에 대한 개별 요청 정보의 집중도를 사전 정보(해당 참조 정보에 대한 요청 정보 전반의 집중도)로 규제함

## Notation
-----

- $y$: response variable
- $q,k,v,\cdots \in X$: explanatory variables
- $q,k,v$: query, key, value
- $S$: random variable of attention score
- $s$: sample of attention score
- $\Omega$: random variable of attention weight
- $\omega$: sample of attention weight
- $P_{\theta}(\cdot)$: posterior dist.
- $Q_{\phi}(\cdot)$: approx. dist.
- $\Pi_{\eta}(\cdot)$: prior dist.
- $\mathcal{L}(\cdot)$: likelihood
- $\mathbf{h}$: linear transformation vector
- $\mathbf{W}$: linear transformation matrix
- $\mathbf{b}$: bias vector

## How to Modeling
-----

- attention score distribution must be defined over non-negative random variables

    - if Approx. is `Weibull Dist.`, Prior must be `Gamma Dist.`
        - Weibull: $S \sim \mathrm{Weibull}(k,\lambda)$ ($k$ is hyper-parameter)
        - Gamma: $S \sim \mathrm{Gamma}(\alpha,\beta)$ ($\beta$ is hyper-parameter)

    - if Approx. is `Lognormal Dist.`, Prior must be `Lognormal Dist.`
        - Lognormal: $S \sim \mathrm{Lognormal}(\mu,\sigma^{2})$ ($\sigma$ is hyper-parameter)

- function values of the attention scores are used to compute the parameters of the Approx. dist. $Q_{\phi}(S)$

    $$\begin{aligned}
    \mathbb{E}\left[S_{i,j}\right]=\exp{\psi_{i,j}} \quad \mathrm{for} \quad \psi_{i,j}=f(q_{i},k_{j})
    \end{aligned}$$

    - Weibull: $\lambda=\exp{\psi} / \Gamma(1+1/k)$
    - Lognormal: $\mu=\psi - \sigma^{2}/2$

- Prior $\Pi_{\eta}(S)$ is contextual dist. based on keys

    $$\begin{aligned}
    \mathbb{E}\left[S_{j}\right]=\exp{\psi_{j}} \quad \mathrm{for} \quad \psi_{j}=\mathrm{softmax}\left[\mathbf{h}^{T}(\mathbf{W}k_{j}+\mathbf{b})\right]
    \end{aligned}$$

    - Gamma: $\alpha=\exp{\psi}\cdot\beta$
    - Lognormal: $\mu=\psi - \sigma^{2}/2$

- attention weights are derived through L1-normalization, rather than softmax, in order to reflect uncertainty

    $$\begin{aligned}
    \omega_{i}
    &= \frac{s_{i}}{\sum_{l \ne i}{s_{l}}}
    \end{aligned}$$

- bayesian framework

    $$\begin{aligned}
    P_{\theta}(S \mid y,X)
    = \frac{\mathcal{L}(y \mid S, X)\Pi(S)}{P(y \mid X)}
    \end{aligned}$$

    - posterior: $P_{\Theta}(S \mid y,X)$
    - likelihood: $\mathcal{L}(y \mid S, X)$
    - prior: $\Pi_{\eta}(S)$

- variational inference

    $$\begin{aligned}
    \mathrm{ELBO}
    = \mathbb{E}_{S \sim Q_{\phi}}\left[\log{\mathcal{L}(y \mid X)}\right]-\mathrm{KL}\left[Q_{\phi}(S) \parallel \Pi_{\eta}(S)\right]
    \end{aligned}$$

    - approx: $Q_{\phi}(S) \approx P_{\theta}(S \mid y,X)$