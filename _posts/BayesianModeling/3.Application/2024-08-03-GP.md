---
order: 5
title: Gaussian Process
date: 2024-08-03
categories: [BAYES, 3.bayes applications]
tags: [Bayesian, Stochastic Process, Nonparametric Estimation, Gaussian Process]
math: true
image:
    path: /_post_refer_img/BayesianModeling/3.Application/Thumbnail.jpg
---

## Non-Parametric Density Estimation
-----

### Non-Parametric Method

- **모수 추정(Parametric Estimation)** : 데이터가 모수로써 정의되는 특정한 형태로 분포되었다고 가정하고, 소수의 모수를 추정함으로써 데이터 분포를 추정하는 방법

    - **`EXAMPLE` Probability Density Estimation** <br> 확률변수 $X$ 는 평균을 $\mu$, 분산을 $\sigma^{2}$ 으로 하는 가우시안 분포에 따라 분포되어 있음

        $$\begin{aligned}
        X \sim \mathcal{N}\left(\mu, \sigma^{2}\right)
        \end{aligned}$$

    - **`EXAMPLE` Regression Analysis** <br> 한국인 남성의 키($Y$)는 몸무게($X$)와 선형 관계에 있음

        $$\begin{aligned}
        Y = \alpha \cdot X + \beta
        \end{aligned}$$

- **비모수 추정(Non-Parametric Estimation)** : 데이터가 특정한 형태로 분포되었다고 가정하지 않고, 주어진 데이터를 토대로 직접 추정하는 방법
    - Histogram Density Estimation
    - **Kernel Density Estimation**
    - k-Nearest Neighbors Density Estimation
    - Maximum Entropy Density Estimation
    - Regression-Based Density Estimation

### KDE

- **커널 밀도 추정(`K`ernel `D`ensity `E`stimation; KDE)** : 각 데이터 주변에서 작은 확률 분포(Kernel)를 만든 후, 이를 합산하여 전체 밀도를 추정하는 방법

    ![02](/_post_refer_img/BayesianModeling/3.Application/05-02.png){: width="100%"}

- **확률 밀도 함수(Probability Density Function)**

    $$\begin{aligned}
    f(x)
    &= \frac{1}{h} \cdot \frac{1}{n} \cdot \sum_{i=1}^{n}{K\left(\frac{x-x_{i}}{h}\right)}
    \end{aligned}$$

    - $n$ : 데이터 포인트 갯수
    - $x_{i}$ : 개별 데이터 포인트
    - $h$ : 밴드위스(Bandwidth)
    - $K\left(\cdot\right)$ : 커널 함수(Kernel Function)

- **밴드위스(Bandwidth)** : 커널의 너비를 조절하는 하이퍼파라미터로서, 값이 클수록 전역적 패턴을, 작을수록 국소적 패턴을 포착함

    ![01](/_post_refer_img/BayesianModeling/3.Application/05-01.png){: width="100%"}

- **커널 함수(Kernel Function)** : 특정 데이터 포인트 주변의 밀도를 조절하는 함수

    | Name | Function |
    |---|---|
    | Gaussian | $$K(u) = \displaystyle\frac{1}{\sqrt{2\pi}} \exp\left(-\displaystyle\frac{u^2}{2}\right)$$ |
    | Epanechnikov | $$\begin{aligned}K(u)=\begin{cases}\displaystyle\frac{3}{4}(1-u^{2}), \quad &\text{if} \quad \vert u \vert \le 1 \\0, \quad &\text{otherwise}\end{cases}\end{aligned}$$ |
    | Tophat | $$\begin{aligned}K(u) = \begin{cases}\displaystyle\frac{1}{2}, \quad & \text{if} \quad \vert u \vert \le 1 \\0, \quad & \text{otherwise}\end{cases}\end{aligned}$$ |
    | Logistic | $$K(u) = \displaystyle\frac{1}{e^{u} + e^{-u} + 2}$$ |
    | Silverman | $$K(u) = \displaystyle\frac{1}{2} \cdot \exp\left(-\displaystyle\frac{\vert u \vert}{\sqrt{2}}\right) \cdot \cos\left(\displaystyle\frac{\vert u \vert}{\sqrt{2}}\right)$$ |
    | Laplacian | $$K(u) = \displaystyle\frac{1}{2} \cdot \exp(-\vert u \vert)$$ |

## MVN
-----

### Multi-Variate Gaussian Distribution

- **다변량 가우시안 분포(`M`ulti-`V`ariate Gaussian/`N`ormal Distribution; MVN)** : 유한 차원의 상관된 가우시안 확률변수 벡터에 대해 정의되는 가우시안 분포

    ![02](/_post_refer_img/BayesianModeling/3.Application/05-03.png){: width="100%"}

- **Definition**

    $$\begin{aligned}
    \mathbf{X} \sim \mathcal{N}\left(\mu, \Sigma\right)
    \end{aligned}$$

    - $$\mathbf{X}= \begin{pmatrix}X_{1} & X_{2} & \cdots & X_{N}\end{pmatrix}^{T}$$ : Multi-Variable

        - $$X_{i} \sim \mathcal{N}\left(\mu_{i}, \sigma_{i}^{2}\right)$$ : The elements are random variables that follow individual Gaussian dist.

    - $$\mu=\begin{pmatrix}\mu_{1} & \mu_{2} & \cdots & \mu_{N}\end{pmatrix}^{T}$$ : Mean Vector
    - $$\Sigma=\begin{pmatrix}\Sigma_{11} & \Sigma_{12} & \cdots & \Sigma_{1N}\\ \Sigma_{21} & \Sigma_{22} & \cdots & \Sigma_{2N}\\ \vdots & \vdots & \ddots & \vdots\\ \Sigma_{N1} & \Sigma_{N2} & \cdots & \Sigma_{NN} \end{pmatrix}$$ : Covariance Matrix

- **Probability Density Function**

    $$\begin{aligned}
    f(\mathbf{X})
    &= \frac{1}{(2\pi)^{n/2} \vert \Sigma \vert^{1/2}} \exp{\left[-\frac{1}{2}\underbrace{(\mathbf{X}-\mu)^{T}\Sigma^{-1}(\mathbf{X}-\mu)}_{\text{Mahalanobis Distance}}\right]}
    \end{aligned}$$

    - **마할라노비스 거리(Mahalanobis Distance)** : 상관관계가 존재하는 다변량 데이터에서, 하나의 데이터 포인트가 특정 분포 혹은 군집에서 얼마나 떨어져 있는지를 측정하는 개념으로서, 단순 좌표 상의 거리뿐만 아니라 데이터 분포(분산-공분산 구조)를 반영하여 측정함

        $$\begin{aligned}
        D_{M}(x)
        &= \sqrt{(x-\mu)^{T}\Sigma^{-1}(x-\mu)}
        \end{aligned}$$

        - $\sqrt{(x-\mu)^{T}(x-\mu)}$ : 데이터 포인트와 특정 분포 중심점 간 편차로서 유클리드 거리
        - $\Sigma$ : 특정 분포의 공분산 행렬로서 데이터의 분산과 변수 간 상관관계를 포함하며, 이를 반영하여 방향성과 크기에 따라 거리를 조정함

### Conditional Probability

$$\begin{aligned}
Z_{2} \mid Z_{1} \sim \mathcal{N}\left(\mu_{2} + \Sigma_{21}\Sigma_{11}^{-1}(Z_{1}-\mu_{1}), \Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12}\right)
\end{aligned}$$

- Multi-Variate Gaussian Dist.

    $$\begin{aligned}
    \mathbf{Z}=\begin{bmatrix}Z_{1} \\ Z_{2}\end{bmatrix} \sim \mathcal{N}\left(\begin{bmatrix}\mu_{1} \\ \mu_{2}\end{bmatrix}, \begin{bmatrix}\Sigma_{11} & \Sigma_{12}\\ \Sigma_{21} & \Sigma_{22}\end{bmatrix}\right)
    \end{aligned}$$

- Probability Density Function

    $$\begin{aligned}
    f(\mathbf{Z})
    &= \frac{1}{(2\pi)^{n/2} \vert \Sigma \vert^{1/2}} \exp{\left[-\frac{1}{2}\underbrace{(\mathbf{Z}-\mu)^{T}\Sigma^{-1}(\mathbf{Z}-\mu)}_{\text{Mahalanobis Distance}}\right]}
    \end{aligned}$$

- Inv-Covariance Matrix

    $$\begin{aligned}
    \Sigma^{-1}
    &= \begin{bmatrix}
    \Sigma_{11}^{-1}+\Sigma_{11}^{-1}\Sigma_{12} \cdot \mathbf{M} \cdot \Sigma_{12}\Sigma_{11}^{-1} & -\Sigma_{11}^{-1}\Sigma_{12}\mathbf{M}\\
    -\mathbf{M}\Sigma_{21}\Sigma_{11}^{-1} & \mathbf{M}
    \end{bmatrix}
    \end{aligned}$$

    - $$\mathbf{M}=\left(\Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12}\right)^{-1}$$ : Schur Complement

- Exponent Formula Expansion

    $$\begin{aligned}
    (\mathbf{Z}-\mu)^{T}\Sigma^{-1}(\mathbf{Z}-\mu)
    &= \underbrace{(Z_{1}-\mu_{1})^{T}\Sigma_{11}^{-1}(Z_{1}-\mu_{1})}_{\text{Mahalanobis Distance of } Z_{1}}\\
    &\quad + \underbrace{\Bigg[Z_{2}-\bigg\{\mu_{2}+\Sigma_{21}\Sigma_{11}^{-1}(Z_{1}-\mu_{1})\bigg\}\Bigg]^{T}\mathbf{M}\Bigg[Z_{2}-\bigg\{\mu_{2}+\Sigma_{21}\Sigma_{11}^{-1}(Z_{1}-\mu_{1})\bigg\}\Bigg]}_{\text{Conditional Mahalanobis Distance of }Z_{2} \mid Z_{1}}
    \end{aligned}$$

- Therefore,

    $$\begin{aligned}
    \text{E}\left[Z_{2} \mid Z_{1}\right]
    &= \mu_{2} + \Sigma_{21}\Sigma_{11}^{-1}(Z_{1}-\mu_{1})\\
    \text{Var}\left[Z_{2} \mid Z_{1}\right]
    &= \Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12}
    \end{aligned}$$

## Gaussian Process
-----

### Gaussian Process

- **확률 과정(Stochastic Process)**
    
    > 어떤 시점 혹은 위치 지표 $t$ 에 대하여 확률 변수들의 집합 $$\{X_{t} \mid t \in T\}$$ 이 주어졌을 때 이를 확률 과정이라고 한다. 즉, 확률 과정은 인덱스를 따라 변화하는 확률변수들의 집합으로서, 각 인덱스 $$t \in T$$ 에 대응되는 확률변수 $$X_{t}$$ 가 주어져, 인덱스 집합 $$T$$ 위에서 확률변수의 집합이 구성되는 구조이다.

- **가우시안 프로세스(`G`aussian `P`rocess; GP)** : 입력 공간 $\mathcal{X}$ 의 각 지점 $x_{1}, x_{2}, \cdots, x_{N}$ 에서 정의되는 확률변수 $f_{1}, f_{2}, \cdots, f_{N}$ 에 대하여, 유한 개의 집합으로 구성된 벡터 $\mathbf{f} = \begin{bmatrix} f_{1} & f_{2} & \cdots & f_{N} \end{bmatrix}$ 가 항상 다변량 가우시안 분포를 이루는 비모수 확률 과정

    ![04](/_post_refer_img/BayesianModeling/3.Application/05-04.png){: width="100%"}

    $$\begin{aligned}
    \mathbf{f} \sim \mathcal{N}\left(m(x), K(x, x^{\prime})\right)
    \end{aligned}$$

    - $$m(x)$$ : 평균 함수(Mean Function)
    - $$K(x, x^{\prime})$$ : 공분산 함수(Covariance Function)

- **커널 함수(Kernel Function)** : 두 입력 벡터 간 유사도 측정 함수로서 공분산 행렬을 구성함

    | Name | Function |
    |---|---|
    | Linear | $$X \cdot X^{\prime}$$ |
    | Polynomial | $$\left(X \cdot X^{\prime} + \beta\right)^{d}$$ |
    | RBF | $$\exp{\left[-\displaystyle\frac{\Vert X-X^{\prime} \Vert^{2}}{2\ell^{2}}\right]}$$ |
    | Laplacian | $$\exp{\left[-\gamma \Vert X-X^{\prime} \Vert_{1}\right]}$$ |
    | Exponential | $$\exp{\left[-\gamma \Vert X-X^{\prime} \Vert_{2}\right]}$$ |
    | Matern | $$\displaystyle\frac{2^{1-\nu}}{\Gamma\left(\nu\right)} \cdot \left(\displaystyle\frac{\sqrt{2\nu} \Vert X-X^{\prime} \Vert}{\ell}\right)^{\nu} \cdot K_{\nu}\left(\displaystyle\frac{\sqrt{2\nu} \Vert X-X^{\prime} \Vert}{\ell}\right)$$ |
    | Periodic | $$\exp\left[\displaystyle\frac{-2\sin^2\left(\pi \cdot \vert X-X^{\prime} \vert/p\right)}{\ell^2}\right]$$ |
    | Rational Quadratic | $$\left( 1 + \displaystyle\frac{\vert X-X^{\prime} \vert^2}{2 \alpha \ell^2} \right)^{-\alpha}$$ |

    - **대칭성(Symmetry)**

        $$
        K\left(x,x^{\prime}\right)=K\left(x^{\prime},x\right)
        $$

    - **양의 반정치성(`P`ositive `S`emi-`D`efiniteness, PSD)**

        $$
        \sum_{i}\sum_{j}{\alpha_{i} \cdot \alpha_{j} \cdot K\left(x_{i},x_{j}\right)} \ge 0
        $$

### Bayesian Framework

- **함수 분포(Function Distribution)**

    > 각 지점에서 정의되는 확률변수들로 구성된 벡터 $\mathbf{f}$ 가 다변량 가우시안 분포 $\mathcal{N}(\mu,\Sigma)$ 를 따른다는 것은, 각 지점 $x_{1},x_{2},\cdots,x_{N}$ 을 설명변수, 각 지점에서 정의되는 확률변수 $f_{1},f_{2},\cdots,f_{N}$ 들을 종속변수로 취하는 함수 $f(x)$ 를 하나의 확률변수로 간주하는 것으로 이해될 수 있다.

- prior is gaussian process:

    $$\begin{aligned}
    f(\cdot) \sim \mathcal{GP}(m(\cdot), K(\cdot,\cdot))
    \end{aligned}$$

- likelihood is gaussian dist.:

    $$\begin{gathered}
    \mathbf{y}
    = f(\mathbf{X}) + \epsilon, \quad \epsilon \sim \mathcal{N}\left(\mathbf{0}, \sigma_{N}^{2}\mathbf{I}\right)\\
    \Updownarrow\\
    \mathbf{y} \mid \mathbf{X}, f(\cdot)
    \sim \mathcal{N}\left(f(\mathbf{X}), \sigma_{N}^{2}\mathbf{I}\right)
    \end{gathered}$$

- evidence:

    $$\begin{aligned}
    p(\mathbf{y} \mid \mathbf{X})
    &= \int{p(\mathbf{y} \mid \mathbf{X}, f(\cdot)) \cdot p(f(\cdot))}\text{d}f(\cdot)
    \end{aligned}$$

- bayes' theorem:

    $$\begin{aligned}
    p(f(\cdot) \mid \mathcal{D})
    &= \frac{p(\mathbf{y} \mid \mathbf{X}, f(\cdot)) \cdot p(f(\cdot))}{p(\mathbf{y} \mid \mathbf{X})}
    \end{aligned}$$

### Posterior Predictive Dist.

- **MVN(`M`ulti-`V`ariate `N`ormal Distribution)** : 기존 데이터와 신규 데이터를 하나의 결합 가우시안 분포로 표현할 수 있음

    $$\begin{aligned}
    \begin{bmatrix}\mathbf{y} \\ f(\mathbf{x}_{*})\end{bmatrix}
    \sim \mathcal{N} \left(\begin{bmatrix}m(\mathbf{X}) \\ m(\mathbf{x}_{*})\end{bmatrix},\begin{bmatrix}K(\mathbf{X},\mathbf{X}^{\prime}) + \sigma^{2}_{N}\mathbf{I} & K(\mathbf{X},\mathbf{x}_{*}) \\ K(\mathbf{x}_{*},\mathbf{X}) & K(\mathbf{x}_{*}, \mathbf{x}_{*})\end{bmatrix}\right)
    \end{aligned}$$
    
    - $$K(\mathbf{X},\mathbf{X}^{\prime}) \in \mathbb{R}^{N \times N}$$: 관측 데이터의 공분산 행렬
    - $$K(\mathbf{X},\mathbf{x}_{*}) \in \mathbb{R}^{N \times 1}$$: 관측 데이터와 신규 데이터 간 공분산 벡터
    - $$K(\mathbf{x}_{*}, \mathbf{x}_{*}) \in \mathbb{R}$$: 신규 데이터의 분산

- **Posterior Predictive Dist.** : 신규 데이터 예측값을 조건부 분포로 도출할 수 있음

    $$\begin{aligned}
    f(\mathbf{x}_{*}) \mid \mathbf{x}_{*}, \mathcal{D}
    &\sim \mathcal{N}(\mu_{*}, \sigma_{*}^{2})
    \end{aligned}$$

    - Posterior Mean:

        $$\begin{aligned}
        \mu_{*}
        &= K(\mathbf{x}_{*},\mathbf{X})\left[K(\mathbf{X},\mathbf{X}^{\prime}) + \sigma_{N}^{2}\mathbf{I}\right]^{-1}\mathbf{y}
        \end{aligned}$$

    - Posterior Var.:

        $$\begin{aligned}
        \sigma_{*}^{2}
        &= K(\mathbf{x}_{*},\mathbf{x}_{*}) - K(\mathbf{x}_{*},\mathbf{X})\left[K(\mathbf{X},\mathbf{X}^{\prime}) + \sigma_{N}^{2}\mathbf{I}\right]^{-1}K(\mathbf{X},\mathbf{x}_{*})
        \end{aligned}$$