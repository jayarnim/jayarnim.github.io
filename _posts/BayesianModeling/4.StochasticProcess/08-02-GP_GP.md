---
order: 2
title: GP (1) Gaussian Process
date: 2025-08-02
categories: [BAYES, 4.stochastic process]
tags: [Bayesian, Stochastic Process, Time Series, Nonparametric Estimation, Gaussian Process, Multi-Variate Gaussian Dist.]
math: true
image:
    path: /_post_refer_img/BayesianModeling/4.StochasticProcess/Thumbnail.jpg
---

## Non-Parametric Density Estimation
-----

### Non-Parametric Method

- **모수 추정(Parametric Estimation)** : 데이터가 모수로써 정의되는 특정한 형태로 분포되었다고 가정하고, 소수의 모수를 추정함으로써 데이터 분포를 추정하는 방법

    - **`EXAMPLE` Probability Density Estimation** <br> 확률변수 $X$ 는 평균을 $\mu$, 분산을 $\sigma^{2}$ 으로 하는 가우시안 분포에 따라 분포되어 있음

        $$\begin{aligned}
        X \sim \mathcal{N}\left(\mu, \sigma^{2}\right)
        \end{aligned}$$

    - **`EXAMPLE` Regression Analysis** <br> 한국인 남성의 키($Y$)는 몸무게($X$)와 선형 관계에 있음

        $$\begin{aligned}
        Y = \alpha \cdot X + \beta
        \end{aligned}$$

- **비모수 추정(Non-Parametric Estimation)** : 데이터가 특정한 형태로 분포되었다고 가정하지 않고, 주어진 데이터를 토대로 직접 추정하는 방법
    - Histogram Density Estimation
    - **Kernel Density Estimation**
    - k-Nearest Neighbors Density Estimation
    - Maximum Entropy Density Estimation
    - Regression-Based Density Estimation

### KDE

- **커널 밀도 추정(`K`ernel `D`ensity `E`stimation; KDE)** : 각 데이터 주변에서 작은 확률 분포(Kernel)를 만든 후, 이를 합산하여 전체 밀도를 추정하는 방법

    ![02](/_post_refer_img/BayesianModeling/4.StochasticProcess/05-02.png){: width="100%"}

- **확률 밀도 함수(Probability Density Function)**

    $$\begin{aligned}
    f(x)
    &= \frac{1}{h} \cdot \frac{1}{n} \cdot \sum_{i=1}^{n}{K\left(\frac{x-x_{i}}{h}\right)}
    \end{aligned}$$

    - $n$ : 데이터 포인트 갯수
    - $x_{i}$ : 개별 데이터 포인트
    - $h$ : 밴드위스(Bandwidth)
    - $K\left(\cdot\right)$ : 커널 함수(Kernel Function)

- **밴드위스(Bandwidth)** : 커널의 너비를 조절하는 하이퍼파라미터로서, 값이 클수록 전역적 패턴을, 작을수록 국소적 패턴을 포착함

    ![01](/_post_refer_img/BayesianModeling/4.StochasticProcess/05-01.png){: width="100%"}

- **커널 함수(Kernel Function)** : 특정 데이터 포인트 주변의 밀도를 조절하는 함수

    | Name | Function |
    |---|---|
    | Gaussian | $$K(u) = \displaystyle\frac{1}{\sqrt{2\pi}} \exp\left(-\displaystyle\frac{u^2}{2}\right)$$ |
    | Epanechnikov | $$\begin{aligned}K(u)=\begin{cases}\displaystyle\frac{3}{4}(1-u^{2}), \quad &\text{if} \quad \vert u \vert \le 1 \\0, \quad &\text{otherwise}\end{cases}\end{aligned}$$ |
    | Tophat | $$\begin{aligned}K(u) = \begin{cases}\displaystyle\frac{1}{2}, \quad & \text{if} \quad \vert u \vert \le 1 \\0, \quad & \text{otherwise}\end{cases}\end{aligned}$$ |
    | Logistic | $$K(u) = \displaystyle\frac{1}{e^{u} + e^{-u} + 2}$$ |
    | Silverman | $$K(u) = \displaystyle\frac{1}{2} \cdot \exp\left(-\displaystyle\frac{\vert u \vert}{\sqrt{2}}\right) \cdot \cos\left(\displaystyle\frac{\vert u \vert}{\sqrt{2}}\right)$$ |
    | Laplacian | $$K(u) = \displaystyle\frac{1}{2} \cdot \exp(-\vert u \vert)$$ |

## Gaussian Process
-----

- **가우시안 프로세스(`G`aussian `P`rocess; GP)** : 입력 공간 $\mathcal{X}$ 의 각 지점 $x_{1}, x_{2}, \cdots, x_{N}$ 에서 정의되는 확률변수 $f_{1}, f_{2}, \cdots, f_{N}$ 에 대하여, 유한 개의 집합으로 구성된 벡터 $\mathbf{f} = \begin{bmatrix} f_{1} & f_{2} & \cdots & f_{N} \end{bmatrix}$ 가 항상 다변량 가우시안 분포 $\mathcal{N}(\mu, \Sigma)$ 를 이루는 비모수 확률 과정

    ![04](/_post_refer_img/BayesianModeling/4.StochasticProcess/05-04.png){: width="100%"}

- **Definition**

    $$\begin{aligned}
    \mathbf{f} \sim \mathcal{N}\left(\mu, \Sigma\right)
    \end{aligned}$$

    - $$\mathbf{f}$$ : multi-variable

        $$\begin{aligned}
        \mathbf{f} := \begin{bmatrix}
        f_{1}\\ f_{2}\\ \vdots\\ f_{N}
        \end{bmatrix}
        \end{aligned}$$

        - $f_{i} \sim \mathcal{N}(\mu_{i}, \sigma_{i}^{2})$

    - $$\mu$$ : mean vector

        $$\begin{aligned}
        \mu := \begin{bmatrix}
        m(x_{1})\\ m(x_{2})\\ \vdots\\ m(x_{N})
        \end{bmatrix}
        \end{aligned}$$

        - $m(x)$ is mean function

    - $$\Sigma$$ : covariance matrix

        $$\begin{aligned}
        \Sigma := \begin{bmatrix}
        k(x_{1},x_{1}) & k(x_{1},x_{2}) & \cdots & k(x_{1},x_{N})\\
        k(x_{2},x_{1}) & k(x_{2},x_{2}) & \cdots & k(x_{2},x_{N})\\ 
        \vdots & \vdots & \ddots & \vdots\\
        k(x_{N},x_{1}) & k(x_{N},x_{2}) & \cdots & k(x_{N},x_{N})
        \end{bmatrix}
        \end{aligned}$$

        - $k(x, x^{\prime})$ is covariance function

- **커널 함수(Kernel Function)** : 두 입력 벡터 간 유사도 측정 함수로서 공분산 행렬을 구성함

    | Name | Function |
    |---|---|
    | Linear | $$X \cdot X^{\prime}$$ |
    | Polynomial | $$\left(X \cdot X^{\prime} + \beta\right)^{d}$$ |
    | RBF | $$\exp{\left[-\displaystyle\frac{\Vert X-X^{\prime} \Vert^{2}}{2\ell^{2}}\right]}$$ |
    | Laplacian | $$\exp{\left[-\gamma \Vert X-X^{\prime} \Vert_{1}\right]}$$ |
    | Exponential | $$\exp{\left[-\gamma \Vert X-X^{\prime} \Vert_{2}\right]}$$ |
    | Matern | $$\displaystyle\frac{2^{1-\nu}}{\Gamma\left(\nu\right)} \cdot \left(\displaystyle\frac{\sqrt{2\nu} \Vert X-X^{\prime} \Vert}{\ell}\right)^{\nu} \cdot K_{\nu}\left(\displaystyle\frac{\sqrt{2\nu} \Vert X-X^{\prime} \Vert}{\ell}\right)$$ |
    | Periodic | $$\exp\left[\displaystyle\frac{-2\sin^2\left(\pi \cdot \vert X-X^{\prime} \vert/p\right)}{\ell^2}\right]$$ |
    | Rational Quadratic | $$\left( 1 + \displaystyle\frac{\vert X-X^{\prime} \vert^2}{2 \alpha \ell^2} \right)^{-\alpha}$$ |

    - 대칭성(Symmetry):

        $$
        K\left(x,x^{\prime}\right)=K\left(x^{\prime},x\right)
        $$

    - 양의 반정치성(`P`ositive `S`emi-`D`efiniteness, PSD):

        $$
        \sum_{i}\sum_{j}{\alpha_{i} \cdot \alpha_{j} \cdot K\left(x_{i},x_{j}\right)} \ge 0
        $$

## Function Distribution
-----

- 함수 분포(Function Distribution):

    > 각 지점에서 정의되는 확률변수들로 구성된 벡터 $\mathbf{f}$ 가 다변량 가우시안 분포 $\mathcal{N}(\mu,\Sigma)$ 를 따른다는 것은, 각 지점 $x_{1},x_{2},\cdots,x_{N}$ 을 설명변수, 각 지점에서 정의되는 확률변수 $f_{1},f_{2},\cdots,f_{N}$ 들을 종속변수로 취하는 함수 $f(x)$ 를 하나의 확률변수로 간주하는 것으로 이해될 수 있다.

- prior is gaussian process:

    $$\begin{aligned}
    f(\cdot) \sim \mathcal{GP}(m(\cdot), K(\cdot,\cdot))
    \end{aligned}$$

- likelihood is multi-variate gaussian dist.:

    $$\begin{gathered}
    \mathbf{y}
    = f(\mathbf{X}) + \epsilon, \quad \epsilon \sim \mathcal{N}\left(\mathbf{0}, \sigma_{N}^{2}\mathbf{I}\right)\\
    \Updownarrow\\
    \mathbf{y} \mid \mathbf{X}, f(\cdot)
    \sim \mathcal{N}\left(f(\mathbf{X}), \sigma_{N}^{2}\mathbf{I}\right)
    \end{gathered}$$

- evidence:

    $$\begin{aligned}
    p(\mathbf{y} \mid \mathbf{X})
    &= \int{p(\mathbf{y} \mid \mathbf{X}, f(\cdot)) \cdot p(f(\cdot))}\text{d}f(\cdot)
    \end{aligned}$$

- bayes' theorem:

    $$\begin{aligned}
    p(f(\cdot) \mid \mathbf{y},\mathbf{X})
    &= \frac{p(\mathbf{y} \mid \mathbf{X}, f(\cdot)) \cdot p(f(\cdot))}{p(\mathbf{y} \mid \mathbf{X})}
    \end{aligned}$$

- posterior predictive dist.:

    $$\begin{aligned}
    p(f(\mathbf{X}_{*}) \mid \mathbf{y}, \mathbf{X})
    &= \int{\underbrace{p(f(\mathbf{X}_{*}) \mid \mathbf{X}_{*}, f(\cdot))}_{\text{likelihood}} \cdot \underbrace{p(f(\cdot) \mid \mathbf{y}, \mathbf{X})}_{\text{posterior}} \underbrace{\text{d}f(\cdot)}_{\text{refer. measure}}}
    \end{aligned}$$

## Finite-Dimensional Marginalization
-----

- 유한 차원 주변화(Finite-Dimensional Marginalization):

    > 적분 변수 $f(\cdot)$ 는 무한 차원 객체이기 때문에, 그 기준 측도 $\text{d}f(\cdot)$ 는 르베그 측도와 같이 일반적인 방식으로는 정의될 수 없다. 이에 따라 $\text{d}f(\cdot)$ 을 기준으로 측정되는 적분 값은 수치적으로 계산될 수 없다. 따라서 $f(\cdot)$ 를 직접 다루는 대신, **유한 차원 마진값(finite marginals)** $f(\mathbf{X})$ 을 다룬다.

- prior is multi-variate gaussian dist.:

    $$\begin{aligned}
    f(\mathbf{X}) \sim \mathcal{N}(m(\mathbf{X}), K(\mathbf{X},\mathbf{X}^{\prime}))
    \end{aligned}$$

- likelihood is multi-variate gaussian dist.:

    $$\begin{gathered}
    \mathbf{y}
    = f(\mathbf{X}) + \epsilon, \quad \epsilon \sim \mathcal{N}\left(\mathbf{0}, \sigma_{N}^{2}\mathbf{I}\right)\\
    \Updownarrow\\
    \mathbf{y} \mid f(\mathbf{X})
    \sim \mathcal{N}\left(f(\mathbf{X}), \sigma_{N}^{2}\mathbf{I}\right)
    \end{gathered}$$

- evidence:

    $$\begin{aligned}
    p(\mathbf{y} \mid \mathbf{X})
    &= \int{p(\mathbf{y} \mid f(\mathbf{X})) \cdot p(f(\mathbf{X}))}\text{d}f(\mathbf{X})
    \end{aligned}$$

- bayes' theorem:

    $$\begin{aligned}
    p(f(\mathbf{X}) \mid \mathbf{y},\mathbf{X})
    &= \frac{p(\mathbf{y} \mid f(\mathbf{X})) \cdot p(f(\mathbf{X}))}{p(\mathbf{y} \mid \mathbf{X})}
    \end{aligned}$$

- posterior predictive dist.:

    $$\begin{aligned}
    p(f(\mathbf{X}_{*}) \mid \mathbf{y}, \mathbf{X})
    &= \int{\underbrace{p(f(\mathbf{X}_{*}) \mid \mathbf{X}_{*}, f(\mathbf{X}))}_{\text{likelihood}} \cdot \underbrace{p(f(\mathbf{X}) \mid \mathbf{y}, \mathbf{X})}_{\text{posterior}} \underbrace{\text{d}f(\mathbf{X})}_{\text{refer. measure}}}
    \end{aligned}$$

## MVN Application
-----

- **MVN(`M`ulti-`V`ariate `N`ormal Distribution)** : 기존 데이터 $$(\mathbf{X}, \mathbf{y})$$ 와 신규 데이터 $$(\mathbf{X}_{*}, \mathbf{y}_{*})$$ 를 하나의 결합 가우시안 분포로 표현할 수 있음

    $$\begin{aligned}
    \begin{bmatrix}\mathbf{y} \\ f(\mathbf{X}_{*})\end{bmatrix}
    \sim \mathcal{N} \left(\begin{bmatrix}m(\mathbf{X}) \\ m(\mathbf{X}_{*})\end{bmatrix},\begin{bmatrix}K(\mathbf{X},\mathbf{X}^{\prime}) + \sigma^{2}_{N}\mathbf{I} & K(\mathbf{X},\mathbf{X}_{*}) \\ K(\mathbf{X}_{*},\mathbf{X}) & K(\mathbf{X}_{*}, \mathbf{X}_{*})\end{bmatrix}\right)
    \end{aligned}$$
    
    - $$K(\mathbf{X},\mathbf{X}^{\prime}) \in \mathbb{R}^{N \times N}$$: 관측 데이터 $$(\mathbf{X},\mathbf{y})$$ 의 공분산 행렬

        $$\begin{aligned}
        K(\mathbf{X},\mathbf{X}^{\prime})
        &= \begin{bmatrix}
        k(x_{1},x_{1}) & k(x_{1},x_{2}) & \cdots & k(x_{1},x_{N})\\
        k(x_{2},x_{1}) & k(x_{2},x_{2}) & \cdots & k(x_{2},x_{N})\\
        \vdots & \vdots & \ddots & \vdots\\
        k(x_{N},x_{1}) & k(x_{N},x_{2}) & \cdots & k(x_{N},x_{N})
        \end{bmatrix}
        \end{aligned}$$

    - $$K(\mathbf{X},\mathbf{X}_{*}) \in \mathbb{R}^{N}$$: 관측 데이터 $$(\mathbf{X},\mathbf{y})$$ 와 신규 데이터 $$(\mathbf{X}_{*},\mathbf{y}_{*})$$ 간 공분산 벡터

        $$\begin{aligned}
        K(\mathbf{X},\mathbf{X}_{*})
        &= \begin{bmatrix}
        k(x_{1},x_{*})\\
        k(x_{2},x_{*})\\
        \vdots\\
        k(x_{N},x_{*})
        \end{bmatrix}
        \end{aligned}$$

    - $$K(\mathbf{X}_{*}, \mathbf{X}_{*}) \in \mathbb{R}$$: 신규 데이터 $$(\mathbf{X}_{*},\mathbf{y}_{*})$$ 에 대한 사전 예측 분포(Prior Predictvie/Marginal Distribution)의 분산

        $$\begin{aligned}
        f(\mathbf{X}_{*}) \sim \mathcal{N}(m(\mathbf{X}_{*}), K(\mathbf{X}_{*}, \mathbf{X}_{*}))
        \end{aligned}$$

- **Posterior Predictive Dist.** : 신규 데이터 예측값 $f(\mathbf{X}_{*})$ 을 조건부 분포로 도출할 수 있음

    $$\begin{aligned}
    f(\mathbf{X}_{*}) \mid \mathbf{X}_{*}, \mathbf{y}, \mathbf{X}
    &\sim \mathcal{N}(\mu_{*}, \sigma_{*}^{2})
    \end{aligned}$$

    - Posterior Mean:

        $$\begin{aligned}
        \mu_{*}
        &= K(\mathbf{X}_{*},\mathbf{X})\left[K(\mathbf{X},\mathbf{X}^{\prime}) + \sigma_{N}^{2}\mathbf{I}\right]^{-1}\mathbf{y}
        \end{aligned}$$

    - Posterior Var.:

        $$\begin{aligned}
        \sigma_{*}^{2}
        &= K(\mathbf{X}_{*},\mathbf{X}_{*}) - K(\mathbf{X}_{*},\mathbf{X})\left[K(\mathbf{X},\mathbf{X}^{\prime}) + \sigma_{N}^{2}\mathbf{I}\right]^{-1}K(\mathbf{X},\mathbf{X}_{*})
        \end{aligned}$$