---
order: 16
title: DACR
date: 2024-08-21
categories: [Research Interest, Recommender System]
tags: [Paper Review, Data Mining, Recommender System, Collaborative Filtering, Latent Factor Model, Deep Learning, Attention Mechanism]
math: true
description: >-
    <ul type="square">
    <li><strong>Title</strong>: <a href="https://doi.org/10.3390/app122010594"><code>Deep Collaborative Recommendation Algorithm Based on Attention Mechanism</code></a></li>
    <li><strong>Published</strong>: <em>2022</em></li>
    <li><strong>Data Set</strong>:
        <ul>
        <li><code><a href="https://grouplens.org/datasets/movielens/">MovieLens</a></code></li>
        <li><code><a href="https://cseweb.ucsd.edu/~jmcauley/datasets.html#amazon_reviews">Amazon(Music)</a></code></li>
        <li><code><a href="http://millionsongdataset.com/lastfm/">Last.fm</a></code></li>
        </ul>
    </li>
    </ul>
image:
    path: /_post_refer_img/RecommenderSystem/Thumbnail.jpg
---

## Previous Research
-----

- **Implicit Feedback Problem**

    > 암시적 피드백은 선호/비선호 모호성 문제를 지님. 즉, 양성 데이터가 반드시 선호를 나타내지 않고, 반대로 음성 데이터가 반드시 비선호를 나타내지 않음. 때문에 암시적 피드백을 활용할 때는 암시적 피드백에 내재된 잡음을 적절히 거르고 함축되어 있는 잠재 정보를 적절히 부각하는 절차가 수반되어야 함.

- **Previous Research**
    - **`MF` based Latent Factor Models** : 사용자와 아이템 간 비선형 상호작용 포착 어려움
    - **`MLP` based Latent Factor Models** : 사용자, 아이템 저랭크 표현 학습의 간결성 및 효율성 보장 어려움
    - **`ATTN` Based Latent Factor Models** : 암시적 피드백에 함축되어 있는 잠재 정보 포착 가능하나, 다양한 차원을 종합적으로 통합하는데 미흡함

## DACR
-----

- **DACR(`D`eep `C`ollaborative `R`ecommendation Algorithm Based on `A`ttention Mechanism)** : 다차원적 상호작용을 고려한, 표현 학습과 매칭 학습을 결합한 앙상블 모형

    ![02](/_post_refer_img/RecommenderSystem/16-02.png){: width="100%"}

    - **`R`epresentation `L`earning Module** : 사용자와 아이템의 표현에 초점을 맞추어 학습함
    - **`M`atching Function `L`earning Module** : 사용자와 아이템 간 다차원적 상호작용에 초점을 맞추어 학습함
    - **Fusion Module** : 표현 학습과 매칭 학습의 결과들을 종합하여 최종 예측값을 출력함
    - **ATTN** : 암시적 피드백에 내재된 잡음을 적절히 거르고 함축되어 있는 잠재 정보를 적절히 부각함
        - **Representation** : 사용자와 아이템 표현에서, 중요한 차원을 부각하고, 덜 중요한 차원을 억제함
        - **Matching Function** : 다차원적 상호작용 중에서, 중요한 상호작용을 부각하고, 덜 중요한 상호작용을 억제함

## How to Modeling
-----

![01](/_post_refer_img/RecommenderSystem/16-01.png){: width="100%"}

- $\oplus$ : Vector Concatenation
- $\odot$ : Element-wise Product
- $\mathbf{R} \in \mathbb{R}^{M\times N}$ : User-Item Interaction Matrix
- $M$ : Number of User
- $N$ : Number of Item
- $K$ : Latent Factor Dimension

### Representation Learning

- **User Representation**
    - User Attentional Representation:

        $$
        \mathbf{U} = \text{Self-ATTN}\left[\mathbf{R}\right]
        $$

    - User-Latent Factor Vector for Representation Module:

        $$
        \overrightarrow{\mathbf{P}}^{(RL)} = \text{MLP}_{\text{ReLU}}\left[\mathbf{U}\right]
        $$

- **Item Representation**
    - Item Attentional Representation:

        $$
        \mathbf{V} = \text{Self-ATTN}\left[\mathbf{R}^{T}\right]
        $$

    - Item-Latent Factor Matrix for Representation Module:

        $$
        \overrightarrow{\mathbf{Q}}^{(RL)} = \text{MLP}_{\text{ReLU}}\left[\mathbf{V}\right]
        $$

- **User-Item Predictive Matrix** for Representation Module:

    $$
    \mathbf{R}^{(RL)} = \mathbf{P}^{(RL)} \odot \mathbf{Q}^{(RL)}
    $$

### Matching Function Learning

- **User-Item Concatenation:**

    $$
    \mathbf{X}
    =\mathbf{P}^{(ML)} \oplus \mathbf{Q}^{(ML)}
    $$

    - $$\mathbf{P}^{(ML)} \in \mathbb{R}^{M \times K}$$ : User-Latent Factor Matrix for Matching Module
    - $$\mathbf{Q}^{(ML)} \in \mathbb{R}^{N \times K}$$ : Item-Latent Factor Matrix for Matching Module

- **User-Item Pair Attentional Interaction Matrix** Across Different Dimensions:

    $$
    \mathbf{Y}
    = \text{Self-ATTN}\left[\mathbf{Z}\right]
    $$

- **User-Item Predictive Matrix** for Matching Module:

    $$
    \mathbf{R}^{(ML)}
    = \text{MLP}_{\text{ReLU}}\left[\mathbf{X} \oplus \mathbf{Y}\right]
    $$

### Fusion

- **User-Item Predict Score:**

    $$
    \hat{\mathbf{R}}
    =\sigma\left[\mathbf{R}^{(RL)} \oplus \mathbf{R}^{(ML)}\right]
    $$