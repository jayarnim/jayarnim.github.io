---
order: 18
title: ExpoMF
date: 2024-07-24
categories: [Research Interest, Recommender System]
tags: [Paper Review, Data Mining, Recommender System, Collaborative Filtering, Latent Factor Model, Implicit Feedback, Weight Approach, Bayesian]
math: true
description: >-
    <ul type="square">
    <li><strong>Title</strong>: <a href="https://doi.org/10.1145/2872427.2883090"><code>Modeling User Exposure in Recommendation</code></a></li>
    <li><strong>Published</strong>: <em>2016</em></li>
        <li><strong>Data Set</strong>:
        <ul>
        <li><code><a href="https://snap.stanford.edu/data/loc-Gowalla.html">Gowalla</a></code></li>
        <li><a href="https://www.kaggle.com/datasets/ryanholbrook/the-million-songs-dataset"><code>Million Song</code></a></li>
        </ul>
    </li>
    </ul>
image:
    path: /_post_refer_img/RecommenderSystem/Thumbnail.jpg
---

## Previous Research
-----

- **Problems with implicit feedback**

    > 암시적 피드백 하에서는 관측된 아이템을 선호 아이템, 미관측된 아이템을 비선호 아이템으로서 추정함. 하지만 관측 행위는 선호 외에 불호와 우연성을, 미관측 행위는 비선호 외에 미노출이라는 경우의 수를 포괄하고 있음. 따라서 미관측된 아이템을 단순히 비선호 아이템으로 치부하는 것은 무리가 있음.

- **WMF(`W`eighted `M`atrix `F`actorization)** : Confidence Weighted Approach

    $$\begin{aligned}
    \overrightarrow{\mathbf{p}}, \overrightarrow{\mathbf{q}}
    &= \text{arg}\min_{\Theta}{\sum_{(u,i)}{c_{u,i} \cdot \left(r_{u,i} - \overrightarrow{\mathbf{p}}_{u} \cdot \overrightarrow{\mathbf{q}}_{i}\right)^{2} + \lambda_{\Theta}\Vert \Theta \Vert^{2}}}
    \end{aligned}$$

    - $$c_{u,i}:=1+\alpha \cdot r_{u,i}$$ : Confience Weight

- **BPR(`B`ayesian `P`ersonalized `R`anking)** : Pairwise Learning Approach

    $$\begin{aligned}
    \Theta
    &= \text{arg}\min_{\Theta}{\sum_{(u,i,j)}{-\ln{\sigma\left(\hat{r}_{u,i} - \hat{r}_{u,j}\right)} + \lambda_{\Theta}\Vert\Theta\Vert^{2}}}
    \end{aligned}$$

## EXMF
-----

- **EXMF(`Ex`posure `M`atrix `F`actorization)** : 미관측 아이템에 대한 사용자의 노출 여부를 명시적으로 모델링하는 방법론
    - **WMF** : 미관측 항목에 낮은 신뢰도를 부여하고 있으나, 이는 모든 미관측 항목을 단순히 하향 가중하는 직관적인 접근법임
    - **BPR** : 관측 항목과 미관측 항목 간 순위에 격차를 벌리고 있으나, 이는 미관측 원인 중 하나인 노출 여부를 명시적으로 다루는 접근법이 아님
    - **EXMF** : 사용자 노출 여부를 명시적으로 모델링함으로써 사용자가 관측하지 않은 이유를 설명하고자 함

### How to Modeling

- 사용자 $i$ 가 아이템 $j$ 를 클릭할 가능성

    $$\begin{aligned}
    p\left(r_{i,j},y_{i,j} \mid \mu_{i,j}, \overrightarrow{\mathbf{u}}_{i}, \overrightarrow{\mathbf{v}}_{j} ; \lambda\right)
    &= \underbrace{p\left(y_{i,j} \mid \mu_{i,j}\right)}_{\text{Exposure Prob.}} \cdot \overbrace{p\left(r_{i,j} \mid \overrightarrow{\mathbf{u}}_{i}, \overrightarrow{\mathbf{v}}_{j} ; \lambda\right)^{y_{i,j}}}^{\text{Click Prob. under Exposure}} \cdot \underbrace{\mathbf{I}\left[r_{i,j}=0\right]^{1-y_{i,j}}}_{\text{non-Click Prob. under non-Exposure}}
    \end{aligned}$$

    - $$r_{i,j} = \big\{1,0\big\} \in \mathbf{R}_{M \times N}$$ : 클릭 변수
    - $$y_{i,j} = \big\{1,0\big\} \in \mathbf{Y}_{M \times N}$$ : 노출 변수

- $$p\left(y_{i,j} \mid \mu_{i,j}\right)$$ : 사용자 $i$ 가 아이템 $j$ 에 노출될 가능성

    $$\begin{aligned}
    y_{i,j} &\sim \text{Bernoulli}\left(\mu_{u,i}\right)
    \end{aligned}$$

- $$p\left(r_{i,j} \mid \overrightarrow{\mathbf{u}}_{i}, \overrightarrow{\mathbf{v}}_{j} ; \lambda\right)^{y_{i,j}}$$ : 사용자 $i$ 가 아이템 $j$ 에 노출되었을 때, 아이템을 클릭할 가능성

    $$\begin{aligned}
    r_{i,j} \mid y_{i,j}=1 &\sim \mathcal{N}\left(\overrightarrow{\mathbf{u}}_{i} \cdot \overrightarrow{\mathbf{v}}_{j}, \lambda^{-1}\right)
    \end{aligned}$$

- $$\mathbf{I}\left[r_{i,j}=0\right]^{1-y_{i,j}}$$ : 사용자 $i$ 가 아이템 $j$ 에 노출되지 않았을 때, 아이템을 클릭하지 않을 가능성

    $$\begin{aligned}
    r_{i,j} \mid y_{i,j}=0
    &\sim \delta_{0}\\
    &\approx \mathcal{N}\left(\epsilon, \lambda^{-1}\right) 
    \end{aligned}$$

    - $\delta_{x=k}: p\left(x=k\right)=1$

- Objective Function

    $$\begin{aligned}
    \mathcal{J}
    &= \sum_{(i,j)}{\log{p\left(r_{i,j},y_{i,j} \mid \mu_{i,j}, \overrightarrow{\mathbf{u}}_{i}, \overrightarrow{\mathbf{v}}_{j} ; \lambda\right)}}\\
    &= \sum_{(i,j)}{\log{\text{Bernoulli}\left(y_{i,j}\mid \mu_{i,j}\right)} + y_{i,j} \cdot \log{\mathcal{N}\left(r_{i,j} \mid \overrightarrow{\mathbf{u}}_{i} \cdot \overrightarrow{\mathbf{v}}_{j}, \lambda^{-1}\right)} + (1-y_{u,i}) \cdot \cancel{\log{\mathbf{I}\left[r_{i,j}=0\right]}}}\\
    &= \sum_{(i,j)}{\log{\text{Bernoulli}\left(y_{i,j}\mid \mu_{i,j}\right)} + y_{i,j} \cdot \log{\mathcal{N}\left(r_{i,j} \mid \overrightarrow{\mathbf{u}}_{i} \cdot \overrightarrow{\mathbf{v}}_{j}, \lambda^{-1}\right)}}
    \end{aligned}$$

- Maximum Likelihood Estimation

    $$\begin{aligned}
    \mu, \overrightarrow{\mathbf{u}}, \overrightarrow{\mathbf{v}}
    &= \text{arg}\max_{\Theta}{\mathcal{J}}
    \end{aligned}$$

### Optimizer

- **E-M Algorithm** : 잠재 변수의 기대값을 추정하고, 이를 바탕으로 학습 파라미터를 최적화하는 방법론
    - E-Step(Expectation Step) : 잠재 변수의 기대값 추정
    - M-Step(Maximization Step) : 잠재 변수의 기대값 추정치를 바탕으로 학습 파라미터 갱신

- **E-Step(Expectation Step)** : 사용자 $i$ 가 아이템 $j$ 에 노출되었는지 여부의 기대값 $\mathbb{E}\left[y_{i,j}=1\right]$ 으로서 사용자가 특정 아이템에 노출될 가능성의 추정치

    $$\begin{aligned}
    \mathbb{E}\left[y_{i,j}=1\right]
    &= p\left(y_{i,j}=1 \mid r_{i,j}\right)\\
    &= \frac{\overbrace{p\left(r_{i,j} \mid y_{i,j}=1\right)}^{\text{Likelihood}} \cdot \overbrace{p\left(y_{i,j}=1 \mid \mu_{i,j}\right)}^{\text{Prior}}}{\underbrace{p\left(r_{i,j} \mid \mu_{i,j}\right)}_{\text{Posterior}}}\\
    &= \frac{\mu_{i,j} \cdot \mathcal{N}\left(r_{u,i} \mid \overrightarrow{\mathbf{p}}_{i} \cdot \overrightarrow{\mathbf{q}}_{j}, \lambda^{-1}\right)}{\mu_{i,j} \cdot \mathcal{N}\left(r_{u,i} \mid \overrightarrow{\mathbf{p}}_{i} \cdot \overrightarrow{\mathbf{q}}_{j}, \lambda^{-1}\right) + \left(1 - \mu_{i,j}\right)}
    \end{aligned}$$

- **M-Step(Maximization Step)** : User and Item-Latent Factors updated using ALS

    - User-Latent Factor

        $$\begin{aligned}
        \overrightarrow{\mathbf{u}}_{i}
        \leftarrow \left(\lambda \sum_{j}{\mathbb{E}\left[y_{i,j}=1\right] \cdot \overrightarrow{\mathbf{v}}_{j} \otimes \overrightarrow{\mathbf{v}}_{j}} + \gamma_{\Theta}\mathbf{I}\right)^{-1} \left(\lambda \sum_{j}{\mathbb{E}\left[y_{i,j}=1\right] \cdot r_{i,j} \cdot \overrightarrow{\mathbf{v}}_{j}}\right)
        \end{aligned}$$

    - Item-Latent Factor

        $$\begin{aligned}
        \overrightarrow{\mathbf{v}}_{j}
        \leftarrow \left(\lambda \sum_{i}{\mathbb{E}\left[y_{i,j}=1\right] \cdot \overrightarrow{\mathbf{u}}_{i} \otimes \overrightarrow{\mathbf{u}}_{i}} + \gamma_{\Theta}\mathbf{I}\right)^{-1} \left(\lambda \sum_{i}{\mathbb{E}\left[y_{i,j}=1\right] \cdot r_{i,j} \cdot \overrightarrow{\mathbf{u}}_{i}}\right)
        \end{aligned}$$