---
order: 19
title: AIMF
date: 2024-08-07
categories: [Research Interest, Recommender System]
tags: [Paper Review, Data Mining, Recommender System, Collaborative Filtering, Latent Factor Model, Deep Learning, Attention Mechanism]
math: true
description: >-
    <ul type="square">
    <li><strong>Title</strong>: <a href="https://doi.org/10.3390/sym16030267"><code>Matrix Factorization Recommendation Algorithm Based on Attention Interaction</code></a></li>
    <li><strong>Published</strong>: <em>2024</em></li>
    <li><strong>Data Set</strong>: <code><a href="https://grouplens.org/datasets/movielens/">MovieLens</a></code></li>
    </ul>
image:
    path: /_post_refer_img/RecommenderSystem/Thumbnail.jpg
---

## AIMF
-----

- **AIMF(`A`ttention `I`nteraction `M`atrix `F`actorization)**

## How to Modeling
-----

![02](/_post_refer_img/RecommenderSystem/19-02.png){: width="100%"}

### User-Item Attention

- Non-negative Matrix Factorization:

    $$\begin{aligned}
    \hat{\mathbf{P}},\hat{\mathbf{Q}} = \text{arg} \min{\Vert \mathbf{R}^{*} - \mathbf{P} \cdot \mathbf{Q} \Vert^{2}}
    \end{aligned}$$

    - $\mathbf{R}^{*}=\text{MinMax}\left[\mathbf{R}\right]$
    - $\mathbf{P}_{M \times D} > 0$ : User-Linear Latent Factor Matrix
    - $\mathbf{Q}_{N \times D} > 0$ : Item-Linear Latent Factor Matrix

- MLP:

    ![03](/_post_refer_img/RecommenderSystem/19-03.png){: width="100%"}

    - $$\mathbf{P}_{M \times D}^{*} = \text{MLP}_{\text{ReLU}}\left[\hat{\mathbf{P}}_{M \times D}\right]$$ : User-NonLinear Latent Factor Matrix
    - $$\mathbf{Q}_{N \times D}^{*} = \text{MLP}_{\text{ReLU}}\left[\hat{\mathbf{Q}}_{N \times D}\right]$$ : Item-NonLinear Latent Factor Matrix

- Attention

    - Attention Matrix:

        $$\begin{aligned}
        \mathcal{Q}_{M \times D}
        &= \mathbf{P}_{M \times D}^{*} \cdot \mathbf{W}^{(\mathcal{Q})}_{D \times D}\\
        \mathcal{K}_{N \times D}
        &= \mathbf{Q}_{N \times D}^{*} \cdot \mathbf{W}^{(\mathcal{K})}_{D \times D}\\
        \mathcal{V}_{N \times D}
        &= \mathbf{Q}_{N \times D}^{*} \cdot \mathbf{W}^{(\mathcal{V})}_{D \times D}
        \end{aligned}$$

    - User-Item Attention Vector from User $u$ to Item $i$:

        $$\begin{aligned}
        \text{UI-ATTN}\left[\mathcal{Q}_{u}, \mathcal{K}, \mathcal{V}\right]
        &= \sum_{i=1}^{N}{\omega_{u,i} \cdot \mathcal{V}_{i}}\\
        &= \sum_{i=1}^{N}{\text{Softmax}\left[\alpha_{u,i}\right] \cdot \mathcal{V}_{i}}\\
        &= \sum_{i=1}^{N}{\frac{\exp\left[\mathcal{Q}_{u} \cdot \mathcal{K}_{i}\right]}{\sum_{j=1}^{N}{\exp\left[\mathcal{Q}_{u} \cdot \mathcal{K}_{j}\right]}} \cdot \mathcal{V}_{i}}
        \end{aligned}$$

- Silde Attention

    ![01](/_post_refer_img/RecommenderSystem/19-01.png){: width="100%"}

    - Redefinition $\mathcal{K}$ for Silde Attention:

        $$\begin{aligned}
        \mathcal{K}^{+}
        &= \text{Concat}\Big[\mathcal{K}, \mathcal{K}[:, :-1]\Big]
        \end{aligned}$$

    - Window $\tau$ is:

        $$\begin{aligned}
        \mathcal{K}^{+\tau}
        &= \mathcal{K}^{+}[:, \tau : \tau + D]
        \end{aligned}$$

    - Attention Score from User $u$ to Item $i$ at Window $\tau$:

        $$\begin{aligned}
        \alpha^{(\tau)}_{u,i}
        &= \mathcal{Q}_{u} \cdot \mathcal{K}^{+\tau}_{i}
        \end{aligned}$$

    - Average Attention Score from User $u$ to Item:

        $$\begin{aligned}
        \alpha_{u,i}
        &= \frac{1}{\vert T \vert}\sum_{\tau}{\alpha^{(\tau)}_{u,i}}
        \end{aligned}$$

        - $\tau \in T$

### User Self-Attention

