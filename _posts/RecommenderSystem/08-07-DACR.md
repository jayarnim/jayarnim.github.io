---
order: 20
title: DACR
date: 2024-08-07
categories: [Research Interest, Recommender System]
tags: [Paper Review, Data Mining, Recommender System, Collaborative Filtering, Latent Factor Model, Deep Learning, Attention Mechanism]
math: true
description: >-
    <ul type="square">
    <li><strong>Title</strong>: <a href="https://doi.org/10.3390/app122010594"><code>Deep Collaborative Recommendation Algorithm Based on Attention Mechanism</code></a></li>
    <li><strong>Published</strong>: <em>2022</em></li>
    <li><strong>Data Set</strong>:
        <ul>
        <li><code><a href="https://grouplens.org/datasets/movielens/">MovieLens</a></code></li>
        <li><code><a href="https://cseweb.ucsd.edu/~jmcauley/datasets.html#amazon_reviews">Amazon(Music)</a></code></li>
        <li><code><a href="http://millionsongdataset.com/lastfm/">Last.fm</a></code></li>
        </ul>
    </li>
    </ul>
image:
    path: /_post_refer_img/RecommenderSystem/Thumbnail.jpg
---

### Notation

- $\oplus$ : Vector Concatenation
- $\odot$ : Element-wise Product
- $\mathbf{R} \in \mathbb{R}^{M\times N}$ : User-Item Interaction Matrix
- $M$ : Number of User
- $N$ : Number of Item
- $K$ : Latent Factor Dimension

### Representation Learning

- $\overrightarrow{\alpha}_{u} \in \mathbb{R}^{D}$ : 사용자 초기 표현 벡터
- $\overrightarrow{\beta}_{i} \in \mathbb{R}^{D}$ : 아이템 초기 표현 벡터
- $\overrightarrow{\mathbf{a}}_{u} \in \mathbb{R}^{D}=\text{Self-ATTN}\left[\overrightarrow{\alpha}_{u}\right]$ : 사용자 주의 표현 벡터로서, 상대적으로 중요한 차원은 부각하고, 덜 중요한 차원은 억제함
- $\overrightarrow{\mathbf{b}}_{i} \in \mathbb{R}^{D}=\text{Self-ATTN}\left[\overrightarrow{\beta}_{i}\right]$ : 아이템 주의 표현 벡터로서, 상대적으로 중요한 차원은 부각하고, 덜 중요한 차원은 억제함
- $\overrightarrow{\mathbf{p}}_{u}^{(RL)} \in \mathbb{R}^{K}=\text{MLP}_{\text{ReLU}}\left[\overrightarrow{\mathbf{a}}_{u}\right]$ : 표현 학습 부문 사용자-잠재요인 벡터
- $\overrightarrow{\mathbf{q}}_{i}^{(RL)} \in \mathbb{R}^{K}=\text{MLP}_{\text{ReLU}}\left[\overrightarrow{\mathbf{b}}_{i}\right]$ : 표현 학습 부문 아이템-잠재요인 벡터
- $\overrightarrow{\mathbf{y}}_{u,i}^{(RL)} \in \mathbb{R}^{K}=\overrightarrow{\mathbf{p}}_{u}^{(RL)} \odot \overrightarrow{\mathbf{q}}_{i}^{(RL)}$ : 표현 학습 부문 예측 벡터로서, 사용자-잠재요인과 아이템-잠재요인의 잠재요인 각 차원 선형 상호작용 값

### Matching Function Learning

- $\overrightarrow{\mathbf{p}}_{u}^{(ML)} \in \mathbb{R}^{K}$ : 사용자-잠재요인 벡터
- $\overrightarrow{\mathbf{q}}_{i}^{(ML)} \in \mathbb{R}^{K}$ : 아이템-잠재요인 벡터
- $\overrightarrow{\mathbf{v}}_{u,i}\in \mathbb{R}^{2K}=\overrightarrow{\mathbf{p}}_{u}^{(ML)} \oplus \overrightarrow{\mathbf{q}}_{i}^{(ML)}$
- $\overrightarrow{\mathbf{x}}_{u,i}\in \mathbb{R}^{2K}=\text{Self-ATTN}\left[\overrightarrow{\mathbf{v}}_{u,i}\right]$ : (사용자, 아이템) 쌍에 대하여, 잠재요인 간 주의 표현 벡터로서, 동일 차원뿐만 아니라 서로 다른 차원 간 상호작용을 고려한 값
- $\overrightarrow{\mathbf{y}}_{u,i}^{(ML)}\in \mathbb{R}^{K}=\text{MLP}_{\text{ReLU}}\left[\overrightarrow{\mathbf{x}}_{u,i} \oplus \overrightarrow{\mathbf{v}}_{u,i}\right]$ : 매칭 학습 부문 예측 벡터로서, 서로 다른 차원 간 관계를 고려한, 사용자-잠재요인, 아이템-잠재요인의 비선형 상호작용 값

### Fusion

- $\hat{r}_{u,i}=\sigma\left[\overrightarrow{\mathbf{y}}_{u,i}^{(RL)} \oplus \overrightarrow{\mathbf{y}}_{u,i}^{(ML)}\right]$