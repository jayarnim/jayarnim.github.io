# Index

1. Prerequisite
2. PCA
3. LDA
4. `sklearn.decomposition.PCA`

<hr></br>

## Prerequisite

### ğŸ’¡ Projection

<p align="center"><img alt="projection" src="https://github.com/jayarnim/jayarnim/assets/116495744/5573d87c-f94c-4634-a11b-7af4edd89640" width=80%></p>

- ë²¡í„° $\overrightarrow{a}$ ë¥¼ ë²¡í„° $\overrightarrow{b}$ ì— ì •ì‚¬ì˜í–ˆì„ ë•Œ, ì •ì‚¬ì˜ ë²¡í„° $\text{proj}_{\overrightarrow{b}}(\overrightarrow{a})$ ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ

    $$\begin{aligned}
    \cos{90\degree}
    &= \frac{(\overrightarrow{a}-p\overrightarrow{b})^{T}\overrightarrow{b}}{||\overrightarrow{a}||\cdot||\overrightarrow{b}||}\\
    &= 0\\
    \therefore \text{proj}_{\overrightarrow{b}}(\overrightarrow{a})
    &= p\overrightarrow{b}\\
    &= \left(\frac{\overrightarrow{a}^{T}\overrightarrow{b}}{||\overrightarrow{b}||^{2}}\right)\overrightarrow{b}
    \end{aligned}$$

    - $p=\displaystyle\frac{\overrightarrow{a}^{T}\overrightarrow{b}}{||\overrightarrow{b}||^{2}}$ : ì •ì‚¬ì˜ ë²¡í„°ì˜ í¬ê¸°
    - $\overrightarrow{b}$ : ì •ì‚¬ì˜ ë²¡í„°ì˜ ë°©í–¥

### ğŸ’¡ Covariance Matrix

- **ê³µë¶„ì‚°(Covariance)** : ë‘ í™•ë¥ ë³€ìˆ˜ì˜ ì„ í˜•ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œë¡œì„œ, ë‘ í™•ë¥ ë³€ìˆ˜ì˜ í¸ì°¨(ê´€ì¸¡ì¹˜ì™€ í‰ê·  ì‚¬ì´ ê±°ë¦¬)ë¥¼ ê³±í•œ ê°’ì˜ í‰ê· 

    $$
    \sigma_{XY} = \frac{1}{N}\sum_{i=1}^{N}(X_{i}-\mu_X)(Y_{i}-\mu_Y)
    $$

- **ê³µë¶„ì‚°í–‰ë ¬(Covariance Matrix)** : $n$ ê°œ ë³€ìˆ˜ë“¤ ê°„ ê³µë¶„ì‚°ì„ ë‚˜ì—´í•œ $n \times n$ ì •ë°©í–‰ë ¬

    $$
    \Sigma=
    \begin{matrix}
    & \overrightarrow{A} & \overrightarrow{B} & \overrightarrow{C} \\
    \overrightarrow{A} & \sigma_{A}^2 & \sigma_{AB} & \sigma_{AC} \\
    \overrightarrow{B} & \sigma_{BA} & \sigma_{B}^2 & \sigma_{BC} \\
    \overrightarrow{C} & \sigma_{CA} & \sigma_{CB} & \sigma_{C}^2
    \end{matrix}
    $$

### ğŸ’¡ Linear Transformation

<p align="center"><img alt="ì„ í˜•ë³€í™˜" src="https://github.com/jayarnim/jayarnim/assets/116495744/4e5847af-09b2-4050-be5b-68f0277a2a34" width=80%></p>

- í–‰ë ¬ $\mathbf{X}$ ì„ í†µí•œ ì„ í˜•ë³€í™˜ì€ ì–´ë–¤ ì¢Œí‘œë¥¼ $\begin{pmatrix}1\\0\end{pmatrix},\begin{pmatrix}0\\1\end{pmatrix}$ ë¥¼ ê¸°ì €ë¡œ ì‚¬ìš©í•˜ëŠ” 2ì°¨ì› ì¢Œí‘œê³„ì—ì„œ $\overrightarrow{x}_{1},\overrightarrow{x}_{2}$ ë¥¼ ê¸°ì €ë¡œ ì‚¬ìš©í•˜ëŠ” 2ì°¨ì› ì¢Œí‘œê³„ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•¨

    $$\begin{aligned}
    \mathbf{X}
    &= \begin{pmatrix} 1&3\\-2&0 \end{pmatrix}\\
    &= \begin{pmatrix} \overrightarrow{x}_{1}&\overrightarrow{x}_{2} \end{pmatrix}
    \end{aligned}$$

- ë²¡í„° $\overrightarrow{v}$ ëŠ” $\begin{pmatrix}1\\0\end{pmatrix},\begin{pmatrix}0\\1\end{pmatrix}$ ë¥¼ ê¸°ì €ë¡œ ì‚¬ìš©í•˜ëŠ” 2ì°¨ì› ì¢Œí‘œê³„ì˜ ì¢Œí‘œ $(-1,2)$ ë¥¼ ë‚˜íƒ€ëƒ„

    $$\begin{aligned}
    \overrightarrow{v}
    &= \begin{pmatrix} 1\\-2 \end{pmatrix}\\
    &= -1\begin{pmatrix}1\\0\end{pmatrix} + 2\begin{pmatrix}0\\1\end{pmatrix}\\
    \end{aligned}$$

- $\mathbf{X}$ ë¥¼ í†µí•œ ì„ í˜• ë³€í™˜ ê²°ê³¼ $\overrightarrow{v}$ ëŠ” $\overrightarrow{x}_{1},\overrightarrow{x}_{2}$ ë¥¼ ê¸°ì €ë¡œ ì‚¬ìš©í•˜ëŠ” 2ì°¨ì› ì¢Œí‘œê³„ì˜ ì¢Œí‘œ $(-1,2)$ ë¡œ ë³€í™˜ë˜ì—ˆìŒ

    $$\begin{aligned}
    \mathbf{X}\cdot\overrightarrow{v}
    &= \begin{pmatrix} 1&3\\-2&0 \end{pmatrix} \cdot \begin{pmatrix} 1\\-2 \end{pmatrix}\\
    &= \begin{pmatrix}-5\\2\end{pmatrix}\\
    &= -1\overrightarrow{x}_{1} + 2\overrightarrow{x}_{2}
    \end{aligned}$$

### ğŸ’¡ Eigen-Vector

<p align="center"><img alt="ê³ ìœ ë²¡í„°" src="https://github.com/jayarnim/jayarnim/assets/116495744/34f3ec69-c070-4538-843c-dfb3a6922d6f" width=80%></p>

- **ê³ ìœ ë²¡í„°(Eigen-Vector; $\overrightarrow{v}$)** : ì •ë°©í–‰ë ¬ $A_n$ ìœ¼ë¡œ ì„ í˜•ë³€í™˜í–ˆì„ ë•Œ, ê·¸ ë°©í–¥ì€ ë³€í•˜ì§€ ì•Šê³  ë‹¨ì§€ í¬ê¸°ë§Œ ë³€í•˜ëŠ” $\overrightarrow{0}$ ì´ ì•„ë‹Œ ë²¡í„°

    $$\begin{aligned}
    \begin{pmatrix}
    a_{11}&a_{12}&\cdots&a_{1n}\\
    a_{21}&a_{22}&\cdots&a_{1n}\\
    \vdots&\vdots&\ddots&\vdots\\
    a_{n1}&a_{n2}&\cdots&a_{nn}
    \end{pmatrix}
    \begin{pmatrix}
    v_{1} \\ v_{2} \\ \vdots \\ v_{n}
    \end{pmatrix}
    =
    \lambda
    \begin{pmatrix}
    v_{1} \\ v_{2} \\ \vdots \\ v_{n}
    \end{pmatrix}
    \Leftrightarrow
    A_{n \times n} \overrightarrow{v} 
    = \lambda \overrightarrow{v}
    \end{aligned}$$

- **ê³ ìœ ê°’(Eigen-Value; $\lambda$)** : ê³ ìœ ë²¡í„°ì˜ ì„ í˜•ë³€í™˜ ì „ í¬ê¸° ëŒ€ë¹„ ì„ í˜•ë³€í™˜ í›„ í¬ê¸°ì˜ ë¹„ìœ¨

</br>

## PCA

### ğŸ’¡ Principal Component Analysis

- **ì •ì˜** : ê³ ì°¨ì› ë°ì´í„°ì— ëŒ€í•˜ì—¬, Xì˜ ë°©í–¥ì  ë¶„í¬ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” **ìƒˆë¡œìš´ ì €ì°¨ì› ì§êµ ì¢Œí‘œ**ë¥¼ í•™ìŠµí•˜ëŠ” ê¸°ë²•

    <p align="center"><img alt="ì£¼ì„±ë¶„ë¶„ì„" src="https://github.com/jayarnim/jayarnim/assets/116495744/1db1d27b-58c2-4e3b-aab9-847e42c87166" width=80%></p>

    - **ì£¼ì„±ë¶„(Principal Component; PC)** : ìƒˆë¡œìš´ ì €ì°¨ì› ì§êµ ì¢Œí‘œ

- **ë°©ë²•** : **ê´€ì¸¡ì¹˜ ê°„ ìƒëŒ€ì  íŠ¹ì„±**ì„ ì˜ ë³´ì¡´í•˜ëŠ” ì„±ë¶„ë“¤ì„ ì¶”ì¶œí•¨

    <p align="center"><img alt="SS" src="https://github.com/jayarnim/jayarnim/assets/116495744/79450a8b-aa57-4087-9007-96104ac155b1" width=80%></p>

    - $\text{component}$ : ì£¼ì„±ë¶„ ë²¡í„° $\overrightarrow{w}$
    - $\text{datapoint}$ : ê´€ì¸¡ì¹˜ ë²¡í„° $\overrightarrow{x}\in \mathbf{X}$
    - $\text{projected data}$ : ì£¼ì„±ë¶„ ë²¡í„°ì— ëŒ€í•œ ê´€ì¸¡ì¹˜ ë²¡í„°ì˜ ì •ì‚¬ì˜ ë²¡í„° $\text{proj}_{\overrightarrow{w}}(\overrightarrow{x})$
    - $D_{1}$ : ê´€ì¸¡ì¹˜ ë²¡í„°ì— ëŒ€í•˜ì—¬ ë³´ì¡´í•˜ëŠ” ì •ë³´ë¡œì„œ **ë¶„ì‚°**
    - $D_{2}$ : ê´€ì¸¡ì¹˜ ë²¡í„°ì— ëŒ€í•˜ì—¬ ìœ ì‹¤í•˜ëŠ” ì •ë³´
    - $D_{3}$ : ê´€ì¸¡ì¹˜ ë²¡í„°ì˜ ë³¸ë˜ ì •ë³´

### ğŸ’¡ ì£¼ì„±ë¶„ ë„ì¶œ ê³¼ì •ì˜ ì´í•´

<p align="center"><img alt="ì£¼ì„±ë¶„" src="https://github.com/jayarnim/jayarnim/assets/116495744/c0d5f8e9-7cf8-4f80-9f10-b68d4b167ed8" width=80%></p>

- ê´€ì¸¡ì¹˜ í–‰ë ¬ $X_{N \times P}$ ë¥¼ ë‹¨ìœ„ë²¡í„° $\overrightarrow{w}$ ì— ì •ì‚¬ì˜í•œë‹¤ê³  í•˜ì

    $$\begin{aligned}
    proj_{\overrightarrow{w}}(\mathbf{X})
    &= \frac{<\mathbf{X},\overrightarrow{w}>}{||w||^2}\cdot\overrightarrow{w}\\
    &= (\overrightarrow{w}^{T}\mathbf{X})\cdot\overrightarrow{w}(\because ||w||=1)
    \end{aligned}$$

    - $\overrightarrow{w}$ : ì •ì‚¬ì˜ ë²¡í„°ì˜ ë°©í–¥
    - $\overrightarrow{w}^{T}\mathbf{X}$ : ì •ì‚¬ì˜ ë²¡í„°ì˜ í¬ê¸°

- $\overrightarrow{w}$ ì— ì •ì‚¬ì˜ëœ ê´€ì¸¡ì¹˜ë“¤ì˜ ë¶„ì‚° $\mathbf{V}$ ì€ ë‹¤ìŒê³¼ ê°™ìŒ

    $$\begin{aligned}
    \mathbf{V}
    &= \frac{1}{n}(\overrightarrow{w}^{T}\mathbf{X})(\overrightarrow{w}^{T}\mathbf{X})^{T}\\
    &= \frac{1}{n}(\overrightarrow{w}^{T}\mathbf{X}\mathbf{X}^{T}\overrightarrow{w})\\
    &= \overrightarrow{w}^{T}\Sigma\overrightarrow{w}
    \end{aligned}$$

    - $\Sigma=\displaystyle\frac{1}{n}\mathbf{X}\mathbf{X}^{T}$ : ê´€ì¸¡ì¹˜ í–‰ë ¬ $X$ ì˜ ê³µë¶„ì‚° í–‰ë ¬

- $\mathbf{V}$ ì„ ìµœëŒ€í™”í•˜ëŠ” $\overrightarrow{w}$ ë¥¼ ì±„íƒí•œë‹¤ê³  í•˜ì

    $$
    \hat{\overrightarrow{w}}
    = \argmax_{\overrightarrow{w}}{\overrightarrow{w}^{T}\Sigma\overrightarrow{w}}\\
    \begin{aligned}
    \\\text{s.t.} \quad
    & \overrightarrow{w}^{T}\overrightarrow{w}=1
    \end{aligned}
    $$

- ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•ì— ê¸°ì´ˆí•˜ì—¬ $\hat{\overrightarrow{w}}$ ë„ì¶œ

    $$\begin{aligned}
    L(\overrightarrow{w},\lambda)
    &= \overrightarrow{w}^{T}\Sigma\overrightarrow{w}-\lambda(\overrightarrow{w}^{T}\overrightarrow{w}-1)\\\\
    \frac{\partial L(\overrightarrow{w},\lambda)}{\overrightarrow{w}}
    &= \Sigma\overrightarrow{w}-\lambda\overrightarrow{w}\\
    &= 0\\\\
    \therefore (\Sigma-\lambda\mathbf{I})\hat{\overrightarrow{w}}
    &=0
    \end{aligned}$$

- $\mathbf{V}$ ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ì£¼ì„±ë¶„ $\overrightarrow{w}$ ì€ $\mathbf{X}$ ì˜ ê³µë¶„ì‚° í–‰ë ¬ $\Sigma$ ì˜ ê³ ìœ ë²¡í„°ì„

    $$\begin{aligned}
    \Sigma
    &= \mathbb{V}\mathbb{\Lambda}\mathbb{V}^{-1},\\
    \mathbb{V}
    &= \begin{pmatrix}\overrightarrow{w}_{1}&\overrightarrow{w}_{2}&\cdots&\overrightarrow{w}_{p}\end{pmatrix}\\
    \mathbb{\Lambda}
    &= \text{diag}(\lambda_{1},\lambda_{2},\cdots,\lambda_{p})
    \end{aligned}$$

### ğŸ’¡ ì£¼ì„±ë¶„ ë²¡í„°ì˜ ì„¤ëª…ë ¥ ì´í•´

- **ì£¼ì„±ë¶„ ë²¡í„°ì˜ ê³ ìœ ê°’** : ê´€ì¸¡ì¹˜ í–‰ë ¬ $\mathbf{X}$ ì— ëŒ€í•˜ì—¬ **ì£¼ì„±ë¶„ ë²¡í„°ì— ëŒ€í•œ ì •ì‚¬ì˜ ë²¡í„° ê°„ ë¶„ì‚°**

    $$\begin{aligned}
    \mathbf{V}
    &= \frac{1}{n}(\overrightarrow{w}^{T}\mathbf{X})(\overrightarrow{w}^{T}\mathbf{X})^{T}\\
    &= \frac{1}{n}\overrightarrow{w}^{T}\mathbf{X}\mathbf{X}^{T}\overrightarrow{w}\\
    &= \overrightarrow{w}^{T}\Sigma\overrightarrow{w}\\
    &= \hat{\overrightarrow{w}}^{T}\lambda\hat{\overrightarrow{w}}(\because \Sigma\hat{\overrightarrow{w}}-\lambda\hat{\overrightarrow{w}}=0)\\
    &= \lambda(\because \overrightarrow{w}^{T}\overrightarrow{w}=1)
    \end{aligned}$$

- **ì£¼ì„±ë¶„ ë²¡í„°ì˜ ì„¤ëª…ë ¥** : ê´€ì¸¡ì¹˜ í–‰ë ¬ $\mathbf{X}_{N \times P}$ ì— ëŒ€í•˜ì—¬ ìƒì„± ê°€ëŠ¥í•œ $P$ ê°œì˜ ì£¼ì„±ë¶„ ë²¡í„° ê³ ìœ ê°’ í•©ê³„ ëŒ€ë¹„ í•´ë‹¹ ì£¼ì„±ë¶„ ë²¡í„° ê³ ìœ ê°’ ë¹„ìœ¨

    $$
    \frac{\lambda_{k}}{\sum_{i=1}^{p}{\lambda_{i}}}
    $$

</br>

## LDA

### ğŸ’¡ Linear Discriminant Analysis

- **ì •ì˜** : ê³ ì°¨ì› ë°ì´í„°ì— ëŒ€í•˜ì—¬, **ì£¼ì–´ì§„ í´ë˜ìŠ¤ë¥¼ ê°€ì¥ ì˜ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ”** ìƒˆë¡œìš´ ì €ì°¨ì› ì§êµ ì¢Œí‘œë¥¼ ì°¾ëŠ” ê¸°ë²•

    <p align="center"><img alt="lda" src="https://github.com/jayarnim/jayarnim/assets/116495744/081f75ba-505a-4e94-be61-2568697232e7" width=80%></p>

- **ë°©ë²•** : **í´ë˜ìŠ¤ ê°„ ë¶„ì‚°ì€ ìµœëŒ€í™”**í•˜ëŠ” ë™ì‹œì— **í´ë˜ìŠ¤ ë‚´ ê´€ì¸¡ì¹˜ ê°„ ë¶„ì‚°ì€ ìµœì†Œí™”**í•˜ëŠ” ì„±ë¶„ë“¤ì„ ì¶”ì¶œí•¨

    $$
    \hat{\overrightarrow{w}}
    =\argmax_{\overrightarrow{w}}{\frac{\Sigma^{2}}{\sigma_{1}^{2}+\sigma_{2}^{2}}}\\
    \begin{aligned}
    \\\text{s.t.} \quad
    & \overrightarrow{w}^{T}\overrightarrow{w}=1
    \end{aligned}
    $$

    - $\Sigma^{2}$ : ì •ì‚¬ì˜ í›„ í´ë˜ìŠ¤ ê°„ ë¶„ì‚°
    - $\sigma_{i}^{2}$ : ì •ì‚¬ì˜ í›„ $i$ ë²ˆì§¸ í´ë˜ìŠ¤ ë‚´ ê´€ì¸¡ì¹˜ ê°„ ë¶„ì‚°

### ğŸ’¡ ì„ í˜• íŒë³„ í•¨ìˆ˜ ë„ì¶œ ê³¼ì •ì˜ ì´í•´

- **ì •ì‚¬ì˜ í›„ ë²”ì£¼ ê°„ ë¶„ì‚° $\Sigma^{2}$**

    $$\begin{aligned}
    \Sigma^{2}
    &= (\overrightarrow{\mu}_{1}-\overrightarrow{\mu}_{2})(\overrightarrow{\mu}_{1}-\overrightarrow{\mu}_{2})^{T}\\
    &= (\overrightarrow{w}^{T}\overrightarrow{m}_{1}-\overrightarrow{w}^{T}\overrightarrow{m}_{2})(\overrightarrow{w}^{T}\overrightarrow{m}_{1}-\overrightarrow{w}^{T}\overrightarrow{m}_{2})^{T}\quad(\because \overrightarrow{\mu}_{i}=\overrightarrow{w}^{T}\overrightarrow{m}_{i})\\
    &= \overrightarrow{w}^{T}(\overrightarrow{m}_{1}-\overrightarrow{m}_{2})(\overrightarrow{m}_{1}-\overrightarrow{m}_{2})^{T}\overrightarrow{w}\\
    &= \overrightarrow{w}^{T}\mathbf{S}_{B}\overrightarrow{w}
    \end{aligned}$$

    - $\overrightarrow{m}_{i}$ : $i$ ë²ˆì§¸ ë²”ì£¼ $C_{i}$ ì˜ ì¤‘ì‹¬ì  ë²¡í„°
    - $\overrightarrow{\mu}_{i}=\text{proj}_{\overrightarrow{w}}(\overrightarrow{m}_{i})$ : $\overrightarrow{m}_{i}$ ì˜ ì •ì‚¬ì˜ ë²¡í„°
    - $\mathbf{S}_{B}$ : ë²”ì£¼ $C_{i},C_{j}$ ê°„ í¸ì°¨
    - $\Sigma$ : ì •ì‚¬ì˜ í›„ ë²”ì£¼ $C_{i},C_{j}$ ê°„ í¸ì°¨

- **ì •ì‚¬ì˜ í›„ ë²”ì£¼ ë‚´ ë¶„ì‚° $\sigma_{i}^{2}$**

    $$\begin{aligned}
    \sigma_{i}^{2}
    &= \sum_{j=1}^{|C_{i}|}{(\overrightarrow{y}_{j}-\overrightarrow{\mu}_{i})(\overrightarrow{y}_{j}-\overrightarrow{\mu}_{i})^{T}}\quad(\overrightarrow{x}_{j} \in C_{i})\\
    &= \sum_{j=1}^{|C_{i}|}{(\overrightarrow{w}^{T}\overrightarrow{x}_{j}-\overrightarrow{w}^{T}\overrightarrow{m}_{i})(\overrightarrow{w}^{T}\overrightarrow{x}_{j}-\overrightarrow{w}^{T}\overrightarrow{m}_{i})^{T}}\quad(\because \overrightarrow{y}_{j}=\overrightarrow{w}^{T}\overrightarrow{x}_{j})\\
    &= \overrightarrow{w}^{T}\left[\sum_{j=1}^{|C_{i}|}{(\overrightarrow{x}_{j}-\overrightarrow{m}_{i})(\overrightarrow{x}_{j}-\overrightarrow{m}_{i})^{T}}\right]\overrightarrow{w}\\
    &= \overrightarrow{w}^{T}\mathbf{S}_{i}\overrightarrow{w}
    \end{aligned}$$

    - $\overrightarrow{x}_{j} \in C_{i}$ : $i$ ë²ˆì§¸ ë²”ì£¼ $C_{i}$ ì˜ $j$ ë²ˆì§¸ ê´€ì¸¡ì¹˜ ë²¡í„°
    - $\overrightarrow{y}_{j}=\text{proj}_{\overrightarrow{w}}(\overrightarrow{x}_{j})$ : $\overrightarrow{x}_{j}$ ì˜ ì •ì‚¬ì˜ ë²¡í„°
    - $S_{i}$ : $i$ ë²ˆì§¸ ë²”ì£¼ $C_{i}$ ì˜ ë²”ì£¼ ë‚´ ê´€ì¸¡ì¹˜ ê°„ í¸ì°¨
    - $\sigma_{i}$ : ì •ì‚¬ì˜ í›„ $i$ ë²ˆì§¸ ë²”ì£¼ $C_{i}$ ì˜ ë²”ì£¼ ë‚´ ê´€ì¸¡ì¹˜ ê°„ í¸ì°¨

- **ëª©ì  í•¨ìˆ˜ ì¬ì •ì˜**

    $$
    \hat{\overrightarrow{w}}
    =\argmax_{\overrightarrow{w}}{\frac{\overrightarrow{w}^{T}\mathbf{S}_{B}\overrightarrow{w}}{\overrightarrow{w}^{T}(\mathbf{S}_{1}+\mathbf{S}_{2})\overrightarrow{w}}}\\
    \begin{aligned}
    \\\text{s.t.} \quad
    & \overrightarrow{w}^{T}\overrightarrow{w}=1
    \end{aligned}
    $$

- **ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•ì„ í†µí•œ ìµœì í™” ë¬¸ì œ í’€ì´**

    $$\begin{aligned}
    L(\overrightarrow{w},\lambda)
    &= \frac{\overrightarrow{w}^{T}\mathbf{S}_{B}\overrightarrow{w}}{\overrightarrow{w}^{T}(\mathbf{S}_{1}+\mathbf{S}_{2})\overrightarrow{w}}-\lambda(\overrightarrow{w}^{T}\overrightarrow{w}-1)\\\\

    \frac{\partial L(\overrightarrow{w},\lambda)}{\partial \overrightarrow{w}}
    &= 0\\\\

    \therefore \left[\mathbf{S}_{B}^{-1}(\mathbf{S}_{1}+\mathbf{S}_{2})-\lambda\mathbf{I}\right]\hat{\overrightarrow{w}}
    &=0
    \end{aligned}$$

</br>

## [`sklearn.decomposition.PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)

```python
from sklearn.decomposition import PCA
```

### ğŸ’¡ General HyperParameter

- `random_state = None`

### ğŸ’¡ Model HyperParameter

- `n_components(default : 5)` : ì¶•ì†Œí•  ì°¨ì›ì˜ ê°œìˆ˜
- `whiten(default : False)` : Standard Scaling ì—¬ë¶€

### ğŸ’¡ Attribute

- `n_features_` : ì¶•ì†Œ ì „ ì°¨ì›ì˜ ê°œìˆ˜
- `feature_names_in_` : ì¶•ì†Œ ì „ ì°¨ì›ëª…
- `mean_` : ì¶•ì†Œ ì „ ì°¨ì›ë³„ í‰ê· 
- `n_components_` : ì¶•ì†Œ í›„ ì°¨ì›ì˜ ê°œìˆ˜
- `components_` : ê³ ìœ ë²¡í„°
- `explained_variance_` : ê° ê³ ìœ ë²¡í„°ì˜ ê³ ìœ ê°’
- `explained_variance_ratio_` : ì „ì²´ ê³ ìœ ë²¡í„°ì˜ ê³ ìœ ê°’ ëŒ€ë¹„ ê° ê³ ìœ ë²¡í„°ì˜ ê³ ìœ ê°’

</br><hr>

#### ì´ë¯¸ì§€ ì¶œì²˜

- http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/

- https://github.com/lovit/python_ml_intro