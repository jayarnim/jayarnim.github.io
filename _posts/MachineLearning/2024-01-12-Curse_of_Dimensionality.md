# Index

1. Curse of Dimensionality
2. Feature Selection

<hr></br>

## Curse of Dimensionality

### ğŸ’¡ ì°¨ì›ì˜ ì €ì£¼

- **ì •ì˜** : ê³ ì°¨ì›ì¼ìˆ˜ë¡ ì•Œê³ ë¦¬ì¦˜ì´ ì œëŒ€ë¡œ í•™ìŠµí•˜ì§€ ëª»í•˜ëŠ” í˜„ìƒ

    <p align="center"><img alt="ì°¨ì›ì˜ì €ì£¼" src="https://github.com/jayarnim/jayarnim/assets/116495744/0ee13c18-824c-4676-a4bb-a9b874ba4baf" width=80%></p>

    - ê´€ì¸¡ì¹˜ ê°„ ê±°ë¦¬ê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ë©€ì–´ì§ì— ë”°ë¼ ì°¨ì›ë³„ í•™ìŠµ ê°€ëŠ¥í•œ ê´€ì¸¡ì¹˜ê°€ í¬ì†Œí•´ì§

- **ì°¨ì› ì¶•ì†Œì˜ ë‹¹ìœ„ì„±**

    <p align="center"><img alt="ë‹¤ì–‘ì²´í•™ìŠµ" src="https://user-images.githubusercontent.com/116495744/224497076-8a2e6100-88a5-444c-abb9-377e61e961ee.jpeg" width=80%></p>

    > #### Manifold hypothesis
    > Many high-dimensional data sets that occur in the real world actually lie along low-dimensional latent manifolds inside that high-dimensional space.

### ğŸ’¡ ì°¨ì› ì¶•ì†Œ ê¸°ë²•ì˜ ì¢…ë¥˜

- **ì°¨ì› ì„ íƒ(Feature Selection)** : ìœ íš¨í•œ ì°¨ì›ì„ ì„ ë³„í•˜ëŠ” ë°©ë²•
    - **Filter Approach**
        - Odds Ratio

    - **Wrapper Approach**
        - Forward Selection
        - Backward Elimination
        - Stepwise Selection

- **ì°¨ì› ì¶”ì¶œ(Feature Extraction)** : ì›ë³¸ì˜ íŠ¹ì§•ì„ ë³´ì¡´í•˜ëŠ” ìƒˆë¡œìš´ ì°¨ì›ì„ ì¶”ì¶œí•˜ëŠ” ë°©ë²•
    - $\argmax_{\overrightarrow{w}}{\sigma^{2}}$
        - ì£¼ì„±ë¶„ ë¶„ì„(`P`rinciple `C`omponent `A`nalysis; PCA)
        - ì„ í˜• íŒë³„ ë¶„ì„(`L`inear `D`iscriminant `A`nalysis; LDA)

    - $\argmax_{\overrightarrow{w}}{dist}$
        - ë‹¤ì°¨ì› ì²™ë„ë²•(`M`ulti-`D`imensional `S`caling; MDS)

    - **Reveal Non-Linear Structure**
        - t-SNE(`t`-distributed `S`tochastic `N`eighbor `E`mbedding)
        - LLE(`L`ocally `L`inear `E`mbedding)
        - ISOMAP(`ISO`metric feature `MAP`ping)

</br>

## Feature Selection

> #### Occam's Razor
> Entities should not be multiplied beyond necessity.

### ğŸ’¡ Filter Approach

- **ìŠ¹ì‚°(Odds)** : ë³€ìˆ˜ $Y$ ê°€ ë°˜ì‘í•  ê°€ëŠ¥ì„±ì´ ë°˜ì‘í•˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ë³´ë‹¤ ëª‡ ë°° ë†’ì€ê°€

    $$\begin{aligned}
    \text{odds}(Y)
    &= \frac{P(Y=1)}{1-P(Y=1)}
    \end{aligned}$$

- **ìŠ¹ì‚°ë¹„(Odds Ratio; OR)** : ë³€ìˆ˜ $X$ ê°€ ì°¸ì¼ ë•Œ $Y$ ê°€ ë°˜ì‘í•  ê°€ëŠ¥ì„±ì´, $X$ ê°€ ê±°ì§“ì¼ ë•Œ $Y$ ê°€ ë°˜ì‘í•  ê°€ëŠ¥ì„±ë³´ë‹¤ ëª‡ ë°° ë†’ì€ê°€

    $$\begin{aligned}
    \text{OR}(Y|X)
    &= \frac{\text{odds}(X=1)}{\text{odds}(X=0)}\\
    &= \frac{\frac{P(Y=1\big|X=1)}{1-P(Y=1\big|X=1)}}{\frac{P(Y=1\big|X=0)}{1-P(Y=1\big|X=0)}}
    \end{aligned}$$

    - $\text{OR}(Y|X) = 1$ : $X$ ì˜ ë³€ë™ì´ $Y$ ì˜ ë³€ë™ì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠìŒ
    - $\text{OR}(Y|X) < 1$ : $X$ ëŠ” $Y$ ì™€ ìŒì˜ ìƒê´€ê´€ê³„ì— ìˆìŒ
    - $\text{OR}(Y|X) > 1$ : $X$ ëŠ” $Y$ ì™€ ì–‘ì˜ ìƒê´€ê´€ê²Œì— ìˆìŒ

- **ë¡œì§€ìŠ¤í‹± íšŒê·€ì‹ ê°€ì¤‘ì¹˜ì™€ ìŠ¹ì‚°ë¹„ì˜ ìƒê´€ê´€ê³„ ì´í•´**

    - ë‹¨ìˆœ íšŒê·€ ë¶„ì„ í•˜ ë¡œì§€ìŠ¤í‹± íšŒê·€ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŒ

        $$
        \ln{\frac{P(Y=1)}{1-P(Y=1)}}
        =\beta_0 + \beta_1 X
        $$

    - $X=1$ ì¼ ë•Œì˜ ë¡œì§€ìŠ¤í‹± íšŒê·€ì‹

        $$\begin{aligned}
        \ln{\displaystyle\frac{P(Y=1 \Big| X=1)}{1-P(Y=1 \Big| X=1)}}
        &= \beta_0 + \beta_1 \times 1 \\
        &= \beta_0 + \beta_1
        \end{aligned}$$

    - $X=0$ ì¼ ë•Œì˜ ë¡œì§€ìŠ¤í‹± íšŒê·€ì‹

        $$\begin{aligned}
        \ln{\displaystyle\frac{P(Y=1 \Big| X=0)}{1-P(Y=1 \Big| X=0)}}
        &= \beta_0 + \beta_1 \times 0 \\
        &= \beta_0
        \end{aligned}$$

    - ë‘ íšŒê·€ì‹ì„ ë¹¼ë©´ ë‹¤ìŒê³¼ ê°™ìŒ

        $$\begin{aligned}
        &\ln{\displaystyle\frac{P(Y=1 \Big| X=1)}{1-P(Y=1 \Big| X=1)}} - \ln{\displaystyle\frac{P(Y=1 \Big| X=0)}{1-P(Y=1 \Big| X=0)}}\\
        &= \ln{\frac{\frac{P(Y=1 \big| X=1)}{1-P(Y=1 \big| X=1)}}{\frac{P(Y=1 \big| X=0)}{1-P(Y=1 \big| X=0)}}}\\
        &= (\beta_0 + \beta_1) - \beta_0\\
        &= \beta_1
        \end{aligned}$$

    - ë”°ë¼ì„œ ì„¤ëª…ë³€ìˆ˜ $X$ ì˜ ê°€ì¤‘ì¹˜ $\beta_1$ ê³¼ ìŠ¹ì‚°ë¹„ $\text{OR}(Y|X)$ ê°„ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê´€ê³„ê°€ ì„±ë¦½í•¨

        $$\begin{aligned}
        \therefore \text{OR}(Y|X)
        &= \frac{\frac{P(Y=1 \big| X=1)}{1-P(Y=1 \big| X=1)}}{\frac{P(Y=1 \big| X=0)}{1-P(Y=1 \big| X=0)}} \\
        &= \exp[\beta_1]
        \end{aligned}$$

### ğŸ’¡ Wrapper Approach

- **Forward Selection** : ì–´ë–¤ ë³€ìˆ˜ë„ ì„ íƒë˜ì§€ ì•Šì€ ìƒíƒœì—ì„œ ê°€ì¥ ì„¤ëª…ë ¥ì´ ì¢‹ì€ ë³€ìˆ˜ë¥¼ í•˜ë‚˜ì”© ì¶”ê°€í•˜ëŠ” ë°©ë²•

    $$\begin{aligned}
    \hat{x}_{i}&=\argmax_{x_{i}}R^2[y,f(x_{i})]\\
    \hat{x}_{j}&=\argmax_{x_{j}}R^2[y,f(x_{j \ne i};\hat{x}_{i})]\\
    \hat{x}_{k}&=\argmax_{x_{k}}R^2[y,f(x_{k \ne i,j};\hat{x}_{i},\hat{x}_{j})]
    \end{aligned}\\
    \vdots
    $$

- **Backward Elimination** : ëª¨ë“  ë³€ìˆ˜ê°€ í¬í•¨ëœ ìƒíƒœì—ì„œ ì‹œì‘í•˜ì—¬ ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ë¥¼ í•˜ë‚˜ì”© ì œê±°í•˜ëŠ” ë°©ë²•

    $$\begin{aligned}
    \hat{x}_{i}&=\argmax_{x_{i}}R^2[y,f(\not{x_{i}})]\\
    \hat{x}_{j}&=\argmax_{x_{j}}R^2[y,f(\not{x_{j \ne i}};\not{\hat{x}_{i}})]\\
    \hat{x}_{k}&=\argmax_{x_{k}}R^2[y,f(\not{x_{k \ne i,j}};\not{\hat{x}_{i}},\not{\hat{x}_{j}})]
    \end{aligned}\\
    \vdots
    $$

- **Stepwise Selection** : ì–´ë–¤ ë³€ìˆ˜ë„ ì„ íƒë˜ì§€ ì•Šì€ ìƒíƒœì—ì„œ Forward Selection ê³¼ Backward Elimination ì„ ë²ˆê°ˆì•„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•

### ğŸ’¡ Metrics

- **Akaike Information Criteria(AIC)**

    $$\begin{aligned}
    \text{AIC}
    &= -2\ln{\hat{L}}+2k
    \end{aligned}$$

    - $\hat{L}$ : ëª¨ë¸ ì í•©ë„ë¡œì„œ $\overrightarrow{x}_{i}$ ê°€ ì£¼ì–´ì¡Œì„ ë•Œ $y_{i}$ ê°€ ë°œìƒí•  ê°€ëŠ¥ì„±

        $$\begin{aligned}
        \hat{L}
        &= \prod_{i=1}^{n}{P(Y=y_{i}|X=\overrightarrow{x}_{i};\hat{\overrightarrow{\theta}})}\\
        \hat{\overrightarrow{\theta}}
        &= \begin{pmatrix}\hat{\beta_{0}}&\hat{\beta_{1}}&\cdots&\hat{\beta_{d}}\end{pmatrix}
        \end{aligned}$$

    - $k$ : ëª¨ë¸ ë³µì¡ë„

- **Bayesian Information Criteria(BIC)**

    $$\begin{aligned}
    \text{BIC}
    &= -2\ln{\hat{L}}+k\ln{n}
    \end{aligned}$$

</br><hr>

#### ì´ë¯¸ì§€ ì¶œì²˜

- https://www.incodom.kr/%EC%B0%A8%EC%9B%90%EC%B6%95%EC%86%8C#h_85f3fb207a586b3f9b5702a3be7799e1

- http://matrix.skku.ac.kr/math4ai-intro/W12/