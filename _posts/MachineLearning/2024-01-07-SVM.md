---
order: 7
title: Support Vector Machine
date: 2024-01-07
categories: [Artificial Intelligence, Machine Learning]
tags: [writing]
math: true
image:
  path: /img/MachineLearning/Thumbnail.jpg
---

# ğŸ’¡ Support Vector Machine
-----

- **ì •ì˜** : **ë§ˆì§„(Margin)**ì„ ìµœëŒ€ë¡œ ê°€ì ¸ê°€ëŠ” **ì´ˆí‰ë©´(Hyper Plane)**ì„ ê·œì¹™ìœ¼ë¡œ í•˜ì—¬ ê´€ì¸¡ì¹˜ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜

    ![01](/img/MachineLearning/07-01.png){: width="100%"}

- **ìš©ì–´ì˜ ì´í•´**

    ![02](/img/MachineLearning/07-02.png){: width="100%"}

    - **ì´ˆí‰ë©´(Hyper Plane)** : ë²”ì£¼ë¥¼ êµ¬ë¶„í•˜ëŠ” ê²½ê³„
    - **ì„œí¬íŠ¸ ë²¡í„°(Support Vector)** : ì¸ì ‘í•œ ë²”ì£¼ì— ê°€ì¥ ê°€ê¹Œì´ ìœ„ì¹˜í•œ ë²¡í„°
    - **ë§ˆì§„(Margin)** : ì¸ì ‘í•œ ë‘ ë²”ì£¼ì˜ ì„œí¬íŠ¸ ë²¡í„°ë¥¼ ì§€ë‚˜ëŠ” í‰í–‰í•œ ë‘ ì§ì„  ì‚¬ì´ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬

# ğŸ’¡ ê²°ì • í•¨ìˆ˜ ë„ì¶œ
-----

### ì •ì˜

![03](/img/MachineLearning/07-03.png){: width="100%"}

- **ì´ˆí‰ë©´ ì •ì˜**

    $$
    \overrightarrow{w}^{T}\overrightarrow{x}+b=0
    $$

    - $\overrightarrow{x}$ : ì´ˆí‰ë©´ ìœ„ì— ìœ„ì¹˜í•œ ë²¡í„°
    - $\overrightarrow{w}$ : ì´ˆí‰ë©´ì˜ ë²•ì„  ë²¡í„°
    - $b$ : í¸í–¥ìœ¼ë¡œì„œ ì„¸ë¡œì¶• ì ˆí¸

- **ë²”ì£¼ ì •ì˜**

    $$
    y_{i} = \begin{cases} +1,\;if\;\overrightarrow{x}_{i} \in X^{+}\\ -1,\;if\;\overrightarrow{x}_{i} \in X^{-}\end{cases}
    $$

- **ì„œí¬íŠ¸ ë²¡í„° ì •ì˜**
    - í¸ì˜ìƒ ê´€ì¸¡ì¹˜ ë²¡í„° $\overrightarrow{x}_{\forall}$ ì™€ ì´ˆí‰ë©´ ì‚¬ì´ ê±°ë¦¬ ì ˆëŒ€ê°’ì€ ìµœì†Œ $1$ ì´ë¼ê³  í•˜ì

    - ì¢Œì¸¡ ì„œí¬íŠ¸ ë²¡í„° $\overrightarrow{x}^{+}$ : ë²”ì£¼ $X^{+}$ ì—ì„œ ì´ˆí‰ë©´ì— ê°€ì¥ ê°€ê¹Œì´ ìœ„ì¹˜í•œ ë²¡í„°

        $$
        \overrightarrow{w}^{T}\overrightarrow{x}^{+}+b=+1
        $$

    - ìš°ì¸¡ ì„œí¬íŠ¸ ë²¡í„° $\overrightarrow{x}^{-}$ : ë²”ì£¼ $X^{-}$ ì—ì„œ ì´ˆí‰ë©´ì— ê°€ì¥ ê°€ê¹Œì´ ìœ„ì¹˜í•œ ë²¡í„°

        $$
        \overrightarrow{w}^{T}\overrightarrow{x}^{-}+b=-1
        $$

### ë§ˆì§„ ë„ì¶œ

![04](/img/MachineLearning/07-04.png){: width="100%"}

- ìš°ì¸¡ ì„œí¬íŠ¸ ë²¡í„° $\overrightarrow{x}^{-}$ ì— ëŒ€í•˜ì—¬ ë°©í–¥ $\overrightarrow{w}$ ìœ¼ë¡œ í¬ê¸° $margin$ ë§Œí¼ ì´ë™í•˜ë©´ ì¢Œì¸¡ ì„œí¬íŠ¸ ë²¡í„° $\overrightarrow{x}^{+}$ ì— ì•ˆì°©í•œë‹¤ê³  í•˜ì

    $$
    \overrightarrow{x}^{+} = \overrightarrow{x}^{-} + margin \cdot \overrightarrow{w}
    $$

- $\overrightarrow{w}^{T}\overrightarrow{x}^{+}+b=1$ ì„ ë‹¤ìŒê³¼ ê°™ì´ ì¬ì •ì˜í•  ìˆ˜ ìˆìŒ

    $$\begin{aligned}
    \overrightarrow{w}^{T}\overrightarrow{x}^{+}+b=1\\
    \overrightarrow{w}^{T}(\overrightarrow{x}^{-} + margin \cdot \overrightarrow{w})+b=1\\
    \overrightarrow{w}^{T}\overrightarrow{x}^{-} + margin \cdot \overrightarrow{w}^{T}\overrightarrow{w} + b = 1\\
    (\overrightarrow{w}^{T}\overrightarrow{x}^{-} + b) + margin \cdot \overrightarrow{w}^{T}\overrightarrow{w} = 1\\
    -1 + margin \cdot ||w||^2 = 1
    \end{aligned}$$

- ë”°ë¼ì„œ ë§ˆì§„ì„ ë‹¤ìŒê³¼ ê°™ì´ ë„ì¶œí•  ìˆ˜ ìˆìŒ

    $$
    margin = \frac{2}{||w||^2}
    $$

### ë§ˆì§„ ìµœëŒ€í™”

- **ìµœì í™” ë¬¸ì œ ì •ì˜**
    - ëª©ì  í•¨ìˆ˜

        $$
        \max{\frac{2}{||w||^2}}
        \Rightarrow \min{\frac{1}{2}||w||^2}
        $$

    - ì œì•½ ì¡°ê±´

        $$
        y_{i}(\overrightarrow{w}^{T}\overrightarrow{x}_{i}+b) \ge 1
        $$

- **ë¼ê·¸ë‘ì£¼ í•¨ìˆ˜ ë„ì¶œ**

    $$\begin{aligned}
    L(w,b,\lambda)&=\frac{1}{2}||w||^2 - \sum_{i=1}^{n}{\lambda_{i}\cdot\{y_{i}(\overrightarrow{w}^{T}\overrightarrow{x}_{i}+b)-1\}}
    \end{aligned}$$

    - $\lambda_{i}\ge0$ : ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜

- **KKT ì¡°ê±´ í•˜ ë¼ê·¸ë‘ì£¼ ë“€ì–¼ í•¨ìˆ˜ë¡œ ë³€í™˜**
    - ëª©ì  í•¨ìˆ˜

        $$\begin{aligned}
        g(\lambda) &= \inf_{w,b}{L(w,b,\lambda)}\\
        &= \max_{\lambda}{\min_{w,b}{L(w,b,\lambda)}}
        \end{aligned}$$

    - ì œì•½ ì¡°ê±´ ($\because$ Complementary Slackness, KKT)

        $$
        \lambda_{i}\cdot\{y_{i}(\overrightarrow{w}^{T}\overrightarrow{x}_{i}+b)-1\}=0
        $$

        - ì„œí¬íŠ¸ ë²¡í„° : $$y_{i \in SV}(\overrightarrow{w}^{T}\overrightarrow{x}_{i \in SV}+b)-1=0$$ ($$\because \overrightarrow{w}^{T}\overrightarrow{x}_{i \in SV}+b = 1$$)
        - ê·¸ ì™¸ ë²¡í„° : $$\lambda_{i \notin SV}=0$$ ($$\because \overrightarrow{w}^{T}\overrightarrow{x}_{i \notin SV}+b > 1$$)

- **ë¼ê·¸ë‘ì£¼ ë“€ì–¼ í•¨ìˆ˜ í’€ì´**

    - $\overrightarrow{w}$, $b$ ì— ëŒ€í•˜ì—¬ í¸ë¯¸ë¶„

        $$\begin{aligned}
        \frac{\partial L(w,b,\lambda)}{\partial w}
        &= \overrightarrow{w} - \sum_{i=1}^{n}{\lambda_{i}y_{i}\overrightarrow{x}_{i}}\\
        &= 0\\
        \therefore \overrightarrow{w}^{*}
        &= \sum_{i=1}^{n}{\lambda_{i}y_{i}\overrightarrow{x}_{i}}\\\\

        \frac{\partial L(w,b,\lambda)}{\partial b}
        &= 0 - \sum_{i=1}^{n}{\lambda_{i}y_{i}}\\
        &= 0\\
        \therefore \sum_{i=1}^{n}{\lambda_{i}y_{i}} 
        &= 0
        \end{aligned}$$

    - í¸ë¯¸ë¶„í•œ ê²°ê³¼ë¥¼ ë¼ê·¸ë‘ì£¼ ë“€ì–¼ í•¨ìˆ˜ì— ëŒ€ì…

        $$\begin{aligned}
        \frac{1}{2}||w^{*}||^2
        &= \frac{1}{2}\overrightarrow{w}^{*}\cdot\overrightarrow{w}^{*}\\
        &= \frac{1}{2}\sum_{i=1}^{n}{\lambda_{i}y_{i}\overrightarrow{x}_{i}} \cdot \sum_{j=1}^{n}{\lambda_{j}y_{j}\overrightarrow{x}_{j}}\\
        &= \frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}{\lambda_{i}\lambda_{j}y_{i}y_{j}\overrightarrow{x}_{i}^{T}\overrightarrow{x}_{j}}\\\\

        \sum_{i=1}^{n}{\lambda_{i}\cdot\{y_{i}(\overrightarrow{w}^{T}\overrightarrow{x}_{i}+b)-1\}}
        &= \sum_{i=1}^{n}{\lambda_{i}y_{i}\overrightarrow{w}^{T}\overrightarrow{x}_{i}} + b\sum_{i=1}^{n}{\lambda_{i}y_{i}} - \sum_{i=1}^{n}{\lambda_{i}}\\
        &= \sum_{i=1}^{n}{\lambda_{i}y_{i}(\sum_{j=1}^{n}\lambda_{j}y_{j}\overrightarrow{x}_{j})\overrightarrow{x}_{i}} + b \cdot 0 - \sum_{i=1}^{n}{\lambda_{i}}\\
        &= \sum_{i=1}^{n}\sum_{j=1}^{n}{\lambda_{i}\lambda_{j}y_{i}y_{j}\overrightarrow{x}_{i}\overrightarrow{x}_{j}} - \sum_{i=1}^{n}{\lambda_{i}}\\\\

        \therefore g(\lambda)
        &= \inf_{w,b}{L(w,b,\lambda)}\\
        &= \max_{\lambda}\min_{w,b}{L(w,b,\lambda)}\\
        &= \max_{\lambda}{L(\lambda)}\\
        &= \max_{\lambda}{\left[\frac{1}{2}||w^{*}||^2 - \sum_{i=1}^{n}{\lambda_{i}\cdot\{y_{i}(\overrightarrow{w}^{T}\overrightarrow{x}_{i}+b)-1\}}\right]}\\
        &= \max_{\lambda}{\left[\sum_{i=1}^{n}{\lambda_{i}} - \frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}{\lambda_{i}\lambda_{j}y_{i}y_{j}\overrightarrow{x}_{i}^{T}\overrightarrow{x}_{j}}\right]}
        \end{aligned}$$

### ì„œí¬íŠ¸ ë²¡í„° ë„ì¶œ

- **ë§ˆì§„ì„ ìµœëŒ€í™”í•˜ëŠ” ë²•ì„  ë²¡í„° $\overrightarrow{w}^{*}$**

    $$
    \overrightarrow{w}^{*}
    = \sum_{i \in SV}{\lambda_{i}y_{i}\overrightarrow{x}_{i}}
    $$

- **ë§ˆì§„ì„ ìµœëŒ€í™”í•˜ëŠ” í¸í–¥ $b^{*}$**
    - ë¼ê·¸ë‘ì£¼ ë“€ì–¼ í•¨ìˆ˜ì˜ ì œì•½ ì¡°ê±´ ($\because$ Complementary Slackness, KKT)
        - ì„œí¬íŠ¸ ë²¡í„° : $$y_{i \in SV}(\overrightarrow{w}^{T}\overrightarrow{x}_{i \in SV}+b)-1=0$$ ($$\because \overrightarrow{w}^{T}\overrightarrow{x}_{i \in SV}+b = 1$$)
        - ê·¸ ì™¸ ë²¡í„° : $\lambda_{i \notin SV}=0$ ($\because \overrightarrow{w}^{T}\overrightarrow{x}_{i \notin SV}+b > 1$)

    - ë§ˆì§„ì„ êµ¬í•¨ì— ìˆì–´ ê·¸ ì™¸ ë²¡í„°ëŠ” í•„ìš”í•˜ì§€ ì•ŠìŒ

        $$
        y_{i \in SV}(\overrightarrow{w}^{T}\overrightarrow{x}_{i \in SV}+b)-1=0
        $$

    - ë”°ë¼ì„œ ë§ˆì§„ì„ ìµœëŒ€í™”í•˜ëŠ” ì„œí¬íŠ¸ ë²¡í„°ì˜ í¸í–¥ $b^{*}_{SV}$ ì„ ë‹¤ìŒê³¼ ê°™ì´ ë„ì¶œí•  ìˆ˜ ìˆìŒ

        $$\begin{aligned}
        b^{*}
        &= y_{i \in SV} - \overrightarrow{w}^{T}\overrightarrow{x}_{i \in SV}\\
        &= \frac{1}{|SV|}\sum_{i \in SV}{\left[y_{i} - \overrightarrow{w}^{T}\overrightarrow{x}_{i}\right]}\\
        &= \frac{1}{|SV|}\sum_{i \in SV}{\left[y_{i} - \left(\sum_{j \in SV}{\lambda_{j}y_{j}\overrightarrow{x}_{j}}\right)\overrightarrow{x}_{i}\right]}\;(\because \overrightarrow{w}^{*}=\sum_{i}{\lambda_{i}y_{i}\overrightarrow{x}_{i}})\\
        &= \frac{1}{|SV|}\sum_{i \in SV}\sum_{j \in SV}{\left[y_{i} - \lambda_{j}y_{j}\overrightarrow{x}_{j}\overrightarrow{x}_{i}\right]}
        \end{aligned}$$

- **ë§ˆì§„ì„ ìµœëŒ€í™”í•˜ëŠ” ì„œí¬íŠ¸ ë²¡í„° $\overrightarrow{x}_{SV} \in SV$**

    $$\begin{aligned}
    SV 
    &= \{\overrightarrow{x}_{SV}|\overrightarrow{w}^{*} \cdot \overrightarrow{x}_{SV} + b^{*}=|1|\}\\
    &= \left\{ \overrightarrow{x}_{SV}|\left(\sum_{i=1}^{n}{\lambda_{i}^{*}y_{i}\overrightarrow{x}_{i}}\right) \cdot \overrightarrow{x}_{SV} + \frac{1}{|SV|}\sum_{i \in SV}\sum_{j \in SV}{\left[y_{i} - \lambda_{j}y_{j}\overrightarrow{x}_{j}\overrightarrow{x}_{i}\right]}=|1| \right\}
    \end{aligned}$$

### ìš”ì•½

$$\begin{aligned}
y_{q}
&= \begin{cases}
+1,\;if\;f(\overrightarrow{q}) > 0 \\
-1,\;if\;f(\overrightarrow{q}) < 0 \\
\end{cases}\\\\

f(\overrightarrow{q})
&= \overrightarrow{w}^{*} \cdot \overrightarrow{q} + b^{*}\\
&= \left(\sum_{i \in SV}{\lambda_{i}y_{i}\overrightarrow{x}_{i}}\right) \cdot \overrightarrow{q} + \frac{1}{|SV|}\sum_{i \in SV}\sum_{j \in SV}{\left[y_{i} - \lambda_{j}y_{j}\overrightarrow{x}_{j}\overrightarrow{x}_{i}\right]}
\end{aligned}$$

- $\overrightarrow{q}$ : ì‹ ê·œ ê´€ì¸¡ì¹˜ ë²¡í„°
- $y_{q}$ : $\overrightarrow{q}$ ì˜ ë²”ì£¼
- $f(\overrightarrow{q})$ : ê²°ì • í•¨ìˆ˜ë¡œì„œ ì´ˆí‰ë©´ $\overrightarrow{w}^{T}\overrightarrow{x}+b=0$ ê³¼ ë²¡í„°ì˜ ì‚¬ì˜ ê±°ë¦¬

# ğŸ’¡ ì†Œí”„íŠ¸ ë§ˆì§„
-----

- **í•˜ë“œ ë§ˆì§„(Hard Margin)ì˜ ë¬¸ì œì ** : ì´ìƒ ê´€ì¸¡ì¹˜ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš° ë§ˆì§„ì„ ìµœëŒ€í™”í•˜ëŠ” ì´ˆí‰ë©´ì„ íƒìƒ‰í•˜ê¸° ì–´ë ¤ì›€

    ![05](/img/MachineLearning/07-05.png){: width="100%"}

- **ì†Œí”„íŠ¸ ë§ˆì§„(Soft Margin)ì˜ í•´ê²°ì±…** : ë§ˆì§„ ìœ„ë°˜ $\xi$ ë¥¼ í—ˆìš©í•˜ì—¬ ì¼ë¶€ ì´ìƒ ê´€ì¸¡ì¹˜ë¥¼ ë°°ì œí–ˆì„ ë•Œ ë§ˆì§„ì„ ìµœëŒ€í™”í•˜ëŠ” ì´ˆí‰ë©´ì„ íƒìƒ‰í•¨

    ![06](/img/MachineLearning/07-06.png){: width="100%"}

    - **ë§ˆì§„ ìœ„ë°˜(Margin Violation; $\xi$)** : ì´ˆí‰ë©´ ê·¼ë°©ì—ì„œ ë°œìƒ ê°€ëŠ¥í•œ ì†Œìˆ˜ì˜ ì´ìƒ ê´€ì¸¡ì¹˜ì— ëŒ€í•œ ì˜¤ë¥˜ë¡œì„œ, í•´ë‹¹ ê´€ì¸¡ì¹˜ë¡œë¶€í„° ì„œí¬íŠ¸ ë²¡í„°ë¥¼ ì§€ë‚˜ê³  ì´ˆí‰ë©´ê³¼ í‰í–‰í•œ ì§ì„ ê¹Œì§€ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬

### ë§ˆì§„ ìœ„ë°˜ $\xi$ ë¥¼ ê³ ë ¤í•˜ëŠ” ìµœì í™” ë¬¸ì œ ì •ì˜

- **í•˜ë“œ ë§ˆì§„ì˜ ìµœì í™” ë¬¸ì œ**

    $$\begin{aligned}
    &\min{\frac{1}{2}||w||^2}\\\\
    \quad \text{s.t.} \quad &y_{i}(\overrightarrow{w}^{T}\overrightarrow{x}_{i}+b) \ge 1
    \end{aligned}$$

- **ì†Œí”„íŠ¸ ë§ˆì§„ì˜ ìµœì í™” ë¬¸ì œ**

    $$\begin{aligned}
    &\min{\left[\frac{1}{2}{||w||^2}+C\sum_{i=1}^{n}{\xi_i}\right]}\\\\
    \quad \text{s.t.} \quad &y_{i}(\overrightarrow{w}^{T}\overrightarrow{x}_{i}+b) \ge 1-\xi_i,\\
    &\xi_i \ge 0
    \end{aligned}$$

    - $\xi_{i}$ : ê´€ì¸¡ì¹˜ ë²¡í„° $\overrightarrow{x}_{i}$ ì— ëŒ€í•œ ë§ˆì§„ ìœ„ë°˜

    - $C$ : ë§ˆì§„ ìœ„ë°˜ì— ëŒ€í•œ ê·œì œ ê°•ë„

        ![07](/img/MachineLearning/07-07.png){: width="100%"}

### ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•ì— ê¸°ì´ˆí•œ í’€ì´

- **ë¼ê·¸ë‘ì£¼ í•¨ìˆ˜ ë„ì¶œ**

    $$\begin{aligned}
    L(w,b,\lambda,\xi,\mu)
    = &\left[\frac{1}{2}||w||^2 - \sum_{i=1}^{n}{\lambda_{i}\{y_{i}(\overrightarrow{w}^{T}+b)-(1-\xi_{i})\}}\right]\\
    &+ \left[C\sum_{i=1}^{n}{\xi_{i}}-\sum_{i=1}^{b}{\mu_{i}\xi_{i}}\right]
    \end{aligned}$$

    - $\lambda \ge 0$ : ì œì•½ ì¡°ê±´ $y_{i}(\overrightarrow{w}^{T}\overrightarrow{x}+b) \ge 1-\xi_{i}$ ì— ëŒ€í•œ ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜
    - $\mu \ge 0$ : ì œì•½ ì¡°ê±´ $\xi_{i} \ge 0$ ì— ëŒ€í•œ ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜

- **KKT ì¡°ê±´ í•˜ ë¼ê·¸ë‘ì£¼ ë“€ì–¼ í•¨ìˆ˜ ë„ì¶œ**

    $$\begin{aligned}
    g(\lambda,\mu)
    &= \inf_{w,b,\xi}{L(w,b,\lambda,\xi,\mu)}\\
    &= \max_{\lambda,\mu}{\min_{w,b,\xi}{L(w,b,\lambda,\xi,\mu)}}\\\\

    \quad \text{s.t.} \quad &\lambda_{i}\{y_{i}(\overrightarrow{w}^{T}\overrightarrow{x}+b)-(1-\xi_{i})\}=0,\\
    &\mu_{i}\xi_{i}=0
    \end{aligned}$$

- **$\overrightarrow{w}$,$b$,$\xi$ ì— ëŒ€í•˜ì—¬ í¸ë¯¸ë¶„**

    $$\begin{aligned}
    \frac{\partial L(w,b,\lambda,\xi,\mu)}{\partial w}
    &= \overrightarrow{w} - \sum_{i=1}^{n}{\lambda_{i}y_{i}\overrightarrow{x}_{i}}\\
    &= 0\\\\

    \frac{\partial L(w,b,\lambda,\xi,\mu)}{\partial b}
    &= \sum_{i=1}^{n}{\lambda_{i}y_{i}}\\
    &= 0\\\\

    \frac{\partial L(w,b,\lambda,\xi,\mu)}{\partial \xi}
    &= C-\lambda_{i}-\mu_{i}\\
    &= 0\\\\

    \therefore \overrightarrow{w}^{*}
    &= \sum_{i=1}^{n}{\lambda_{i}y_{i}\overrightarrow{x}_{i}},\\
    \sum_{i=1}^{n}{\lambda_{i}y_{i}}
    &= 0,\\
    \mu_{i}
    &= C - \lambda_{i}
    \end{aligned}$$

- **ìœ„ ê²°ê³¼ë¥¼ ë¼ê·¸ë‘ì£¼ ë“€ì–¼ í•¨ìˆ˜ì— ëŒ€ì…í•˜ì—¬ $\overrightarrow{w}^{*}$,$b^{*}$ ë„ì¶œ**

    $$\begin{aligned}
    \overrightarrow{w}^{*}
    &= \sum_{i \in SV}{\lambda_{i}y_{i}\overrightarrow{x}_{i}}\\
    b^{*}
    &= \sum_{i \in SV}{\sum_{j \in SV}{\left[y_{i}-\lambda_{j}y_{j}\overrightarrow{x}_{j}\overrightarrow{x}_{i}\right]}}
    \end{aligned}$$

# ğŸ’¡ Kernel Trick
-----

- **ì •ì˜** : ì„ í˜•ìœ¼ë¡œëŠ” êµ¬ë¶„í•˜ê¸° ì–´ë ¤ìš´ ì €ì°¨ì› ê³µê°„ìƒì˜ ë°ì´í„° ì„¸íŠ¸ë¥¼, ì ì ˆí•œ ê²°ì • ê²½ê³„ë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” ê³ ì°¨ì› ê³µê°„ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ê¸°ë²•

    ![08](/img/MachineLearning/07-08.jpeg){: width="100%"}

### ë¨¸ì„œì˜ ì •ë¦¬(Mercer's Theorem)

> ì €ì°¨ì› ê³µê°„ $L$ ì—ì„œ ê³ ì°¨ì› ê³µê°„ $H$ ë¡œ ê´€ì¸¡ì¹˜ë“¤ì„ ë§¤í•‘í•˜ëŠ” ì»¤ë„í•¨ìˆ˜ $K$ ëŠ” $L$ ì—ì„œ í‘œí˜„ëœ ê´€ì¸¡ì¹˜ë“¤ ê°„ ìœ í´ë¦¬ë“œ ê±°ë¦¬ì™€ $H$ ì—ì„œ í‘œí˜„ëœ ê´€ì¸¡ì¹˜ë“¤ ê°„ ìœ í´ë¦¬ë“œ ê±°ë¦¬ë¥¼ ë³´ì¡´í•¨

- ì„ì˜ì˜ ê´€ì¸¡ì¹˜ $X_a, X_b$ ì— ëŒ€í•˜ì—¬, $2$ ì°¨ì› ê³µê°„ì—ì„œ í•´ë‹¹ ê´€ì¸¡ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„° $\overrightarrow{a}, \overrightarrow{b}$ ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•˜ì

    $$\begin{aligned}
    \overrightarrow{a}
    &= \begin{pmatrix}a_1\\a_2\end{pmatrix}\\
    \overrightarrow{b}
    &= \begin{pmatrix}b_1\\b_2\end{pmatrix}
    \end{aligned}$$

- $\overrightarrow{a}, \overrightarrow{b}$ ì„ $3$ ì°¨ì›ìƒì˜ ë²¡í„° $\Phi(\overrightarrow{a}),\Phi(\overrightarrow{b})$ ë¡œ ë§¤í•‘í•˜ëŠ” ì»¤ë„í•¨ìˆ˜ $K(\overrightarrow{a}, \overrightarrow{b})$ ëŠ” ë‹¤ìŒì˜ ì¡°ê±´ì„ ë§Œì¡±í•¨

    $$\begin{aligned}
    K(\overrightarrow{a}, \overrightarrow{b})
    &= (\overrightarrow{a}^T \overrightarrow{b})^2 \\
    &= a_1^2b_1^2 + 2(a_1b_1a_2b_2) + a_2^2b_2^2 \\
    &= (a_1^2, \sqrt{2}a_1a_2, a_2^2) \cdot (b_1^2, \sqrt{2}b_1b_2, b_2^2) \\
    &= \Phi(\overrightarrow{a}) \cdot \Phi(\overrightarrow{b})
    \end{aligned}$$

### ì»¤ë„í•¨ìˆ˜ì˜ ì¢…ë¥˜

- **Linear**

    $$K(\overrightarrow{a}, \overrightarrow{b}) = \overrightarrow{a}^T \overrightarrow{b}
    $$

- **Polynomial**

    $$
    K(\overrightarrow{a}, \overrightarrow{b}) = (\gamma \overrightarrow{a}^T \overrightarrow{b} + r)^d
    $$

- **Radial Basis Function(RBF)**

    $$
    K(\overrightarrow{a}, \overrightarrow{b}) = \exp(-\gamma ||\overrightarrow{a}- \overrightarrow{b}||^2)
    $$

- **Hyperbolic Tangent**

    $$
    K(\overrightarrow{a}, \overrightarrow{b}) = \tanh(\gamma \overrightarrow{a}^T \overrightarrow{b} + r)
    $$

# ğŸ’¡ SVR
-----

![09](/img/MachineLearning/07-09.png){: width="100%"}

- **ì´ˆí‰ë©´**

    $$\begin{aligned}
    f(\overrightarrow{x}_{i})
    &= \overrightarrow{w}^{T}\overrightarrow{x}_{i}+b
    \end{aligned}$$

- **ì°¨ì´ì ** : ì œì•½ ì¡°ê±´

    ![10](/img/MachineLearning/07-10.png){: width="100%"}

    - **íŒë³„ ë¶„ì„** : ë§ˆì§„ ë²”ìœ„ ì´ë‚´ì— ê´€ì¸¡ì¹˜ ë²¡í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ

        $$\begin{aligned}
        \text{s.t.} \quad
        &y_{i}(\overrightarrow{w}^{T}\overrightarrow{x}_{i}+b) \ge 1 + \xi_{i},\\
        &\xi_{i} \ge 0
        \end{aligned}$$

    - **íšŒê·€ ë¶„ì„** : ë§ˆì§„ ë²”ìœ„ ì´ë‚´ì— ëª¨ë“  ê´€ì¸¡ì¹˜ ë²¡í„°ê°€ ì¡´ì¬í•¨

        $$\begin{aligned}
        \text{s.t.} \quad
        &-(\varepsilon + \xi_{i}) \le f(\overrightarrow{x}_{i}) - y_{i} \le \varepsilon + \eta_{i},\\
        &\xi_{i},\eta_{i} \ge 0
        \end{aligned}$$

- **SVR ìµœì í™” ë¬¸ì œ**

    $$
    \overrightarrow{\hat{w}},\hat{b},\hat{\xi}_{i},\hat{\eta}_{i}
    =\argmin_{\overrightarrow{w},b,\xi,\eta}{\left[\frac{1}{2}||w||^{2}+C\sum_{i=1}^{n}{(\xi_{i}+\eta_{i})}\right]}\\
    \begin{aligned}
    \\ \text{s.t.} \quad
    & \varepsilon + \xi_{i} + f(\overrightarrow{x}) - y_{i} \ge 0,\\
    & \varepsilon + \eta_{i} - f(\overrightarrow{x}) + y_{i} \ge 0,\\
    & \xi_{i}, \eta_{i} \ge 0
    \end{aligned}$$

# ğŸ’¡ [sklearn.svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn-svm-svc)
-----

```python
from sklearn.svm import SVC
```

### General HyperParameter

- `random_state(default : None)`

### Model HyperParameter

- `decision_function_shape(default : 'ovr')` : ë‹¤í•­ ë¶„ë¥˜ ì‹œ ê²°ì • ê²½ê³„ íƒìƒ‰ ë°©ë²•
    - `'ovo'` : One VS One(ì¼ëŒ€ì¼ êµ¬ë¶„ ê²°ì • ê²½ê³„ íƒìƒ‰)
    - `'ovr'` : One VS Rest(ì¼ëŒ€ë‹¤ êµ¬ë¶„ ê²°ì • ê²½ê³„ íƒìƒ‰)

### Soft Margin

- `C(default : 1.0)` : ë§ˆì§„ ìœ„ë°˜ì— ëŒ€í•œ ê·œì œ ê°•ë„

- `gamma(default : 'scale')` : ê²°ì • ê²½ê³„ë¥¼ ì–¼ë§ˆë‚˜ ìœ ì—°í•˜ê²Œ ê·¸ì„ ê²ƒì¸ê°€
    - `'auto'` : $\displaystyle\frac{1}{n(features)}$
    - `'scale'` : $\displaystyle\frac{1}{n(features) \times X.var()}$
    - `float type`

- `tol(default : 0.001)` : í—ˆìš© ì˜¤ì°¨

### Kernel Trick

- `kernel(default : 'rbf')` : ì»¤ë„í•¨ìˆ˜ ì„¤ì •
    - `'linear'` : $\overrightarrow{a}^T \overrightarrow{b}$
    - `'poly'` : $(\gamma \overrightarrow{a}^T \overrightarrow{b} + r)^d$
    - `'rbf'` : $\tanh(\gamma \overrightarrow{a}^T \overrightarrow{b} + r)$

- `degree(default : 3)` : ë‹¤í•­ì‹ ì»¤ë„í•¨ìˆ˜ì˜ ì°¨ìˆ˜ $d$ ì„¤ì •

- `coef0(default : 0.0)` : ë‹¤í•­ì‹ ì»¤ë„í•¨ìˆ˜ì˜ ìƒìˆ˜í•­ $r$ ì„¤ì •

### To Prevent Overfitting

- `max_iter(default : -1)` : ê²°ì • ê²½ê³„ íƒìƒ‰ ìµœëŒ€ íšŸìˆ˜

### To Prevent Underfitting

- **Imbalance in the number of observations between categories**
    - `class_weight(default : None)` : ê°€ì¤‘í•  ë²”ì£¼ì™€ ê·¸ ê°’
        - `'balanced'`
        - `dictionary type`

-----

### ì´ë¯¸ì§€ ì¶œì²˜

- https://velog.io/@shlee0125
- https://medium.com/@niousha.rf/support-vector-regressor-theory-and-coding-exercise-in-python-ca6a7dfda927