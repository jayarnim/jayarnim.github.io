---
order: 9
title: tSNE
date: 2024-01-09
categories: [Machine Learning Techs, Machine Learning]
tags: [Machine Learning, Unsupervised Learning, Feature Engineering]
math: true
description: >-
  Based on the lecture “Intro. to Machine Learning (2023-2)” by Prof. Je Hyuk Lee, Dept. of Data Science, The Grad. School, Kookmin Univ.
image:
  path: /_post_refer_img/MachineLearning/Thumbnail.jpg
---

## Curse of Dimensionality
-----

### Curse of Dimensionality

- **정의** : 고차원일수록 알고리즘이 제대로 학습하지 못하는 현상

    ![01](/_post_refer_img/MachineLearning/09-01.png){: width="100%"}

    - 관측치 간 거리가 기하급수적으로 멀어짐에 따라 차원별 학습 가능한 관측치가 희소해짐

- **차원 축소의 당위성**

    ![02](/_post_refer_img/MachineLearning/09-02.jpeg){: width="100%"}

    > #### Manifold hypothesis
    > Many high-dimensional data sets that occur in the real world actually lie along low-dimensional latent manifolds inside that high-dimensional space.

### Dimensionality Reduction Methods

- **차원 선택(Feature Selection)** : 유효한 차원을 선별하는 방법
    - **Filter Approach**

    - **Wrapper Approach**
        - Forward Selection
        - Backward Elimination
        - Stepwise Selection

- **차원 추출(Feature Extraction)** : 원본의 특징을 보존하는 새로운 차원을 추출하는 방법
    - $\text{arg} \max_{\overrightarrow{w}}{\sigma^{2}}$
        - 주성분 분석(`P`rinciple `C`omponent `A`nalysis; PCA)
        - 선형 판별 분석(`L`inear `D`iscriminant `A`nalysis; LDA)

    - $\text{arg} \max_{\overrightarrow{w}}{dist}$
        - 다차원 척도법(`M`ulti-`D`imensional `S`caling; MDS)

    - **Reveal Non-Linear Structure**
        - t-SNE(`t`-distributed `S`tochastic `N`eighbor `E`mbedding)
        - LLE(`L`ocally `L`inear `E`mbedding)
        - ISOMAP(`ISO`metric feature `MAP`ping)

## What? t-SNE
-----

### `S`tochastic `N`eighbor `E`mbedding

- **정의** : 관측치 간 고차원 공간 상 확률적 유사도를 보존하면서 저차원으로 매핑하는 비선형 차원 축소

    ![01](/_post_refer_img/MachineLearning/11-01.jpeg){: width="100%"}

### `t`-distributed `S`tochastic `N`eighbor `E`mbedding

- **SNE의 문제점** : Crowding Problem

    ![02](/_post_refer_img/MachineLearning/11-02.png){: width="100%"}

    - 가우시안 분포는 양쪽 꼬리 부분이 충분히 두텁지 않은 형태를 보임
    - 즉, 확률변수 $$\Vert \overrightarrow{x}_{i}-\overrightarrow{x}_{j} \Vert$$ 가 일정한 값 이상부터는 유사도에 큰 차이가 없음

- **t-SNE의 해결책** : Student t-Dristribution

    ![03](/_post_refer_img/MachineLearning/11-03.jpg){: width="100%"}

    $$\begin{aligned}
    T
    &= \frac{Z}{\sqrt{\displaystyle\frac{V}{\nu}}}\\
    V
    &= \sum_{i=1}^{k}{Z_{i}^{2}}
    \end{aligned}$$

    - $Z$ : 표준 가우시안 분포
    - $V$ : 자유도가 $\nu$ 인 카이제곱 분포

## How to Extract
-----

### 두 관측치의 고차원 공간 상 유사도 도출

- 확률변수 $X$ 가 가우시안 분포 $N(\mu, \sigma^{2})$ 을 따른다고 했을 때, $X$ 의 확률밀도함수는 다음과 같음

    $$\begin{aligned}
    f(x)
    &= \frac{1}{\sqrt{2\pi\sigma^{2}}}\exp{\left[-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right]}
    \end{aligned}$$

- **관측치 $$\overrightarrow{x}_{i}$$ 로부터의 거리에 대한 $$\overrightarrow{x}_{i}$$ 와의 유사도**의 가우시안 분포에 기초했을 때, $$\overrightarrow{x}_{j}$$ 가 $$\overrightarrow{x}_{i}$$ 와 유사할 가능성을 다음과 같이 정의하자

    $$\begin{aligned}
    p_{j|i} 
    = \frac{\exp{\left[-\frac{||\overrightarrow{x}_{i}-\overrightarrow{x}_{j}||^{2}}{2\sigma^{2}}\right]}}{\sum_{k \ne i}{\exp{\left[-\frac{||\overrightarrow{x}_{i}-\overrightarrow{x}_{k}||^{2}}{2\sigma^{2}}\right]}}}\\\\

    ||\overrightarrow{x}_{i}-\overrightarrow{x}_{j}|| \sim N_{i}(0,\sigma^{2})
    \end{aligned}$$

- **또한 관측치 $$\overrightarrow{x}_{j}$$ 로부터의 거리에 대한 $$\overrightarrow{x}_{j}$$ 와의 유사도**의 가우시안 분포에 기초했을 때, $$\overrightarrow{x}_{i}$$ 가 $$\overrightarrow{x}_{j}$$ 와 유사할 가능성을 다음과 같이 정의하자

    $$\begin{aligned}
    p_{i|j}
    = \frac{\exp{\left[-\frac{|\overrightarrow{x}_{i}-\overrightarrow{x}_{j}|^{2}}{2\sigma^{2}}\right]}}{\sum_{k \ne j}{\exp{\left[-\frac{|\overrightarrow{x}_{j}-\overrightarrow{x}_{k}|^{2}}{2\sigma^{2}}\right]}}}\\\\

    ||\overrightarrow{x}_{i}-\overrightarrow{x}_{j}|| \sim N_{j}(0,\sigma^{2})
    \end{aligned}$$

- 두 가능성은 **서로 다른 확률 분포에 기초하고 있으므로** 그 값이 반드시 일치한다고 볼 수 없음

    $$
    p_{j|i} \ne p_{i|j}
    $$

- 따라서 $$\overrightarrow{x}_{i}$$ 와 $$\overrightarrow{x}_{j}$$ 의 유사도를 다음과 같이 정의함

    $$\begin{aligned}
    p_{i,j}
    &= \frac{p_{j|i} + p_{i|j}}{2n}
    \end{aligned}$$

### 두 관측치의 2차원 공간 상 유사도 도출

- **관측치 $$\overrightarrow{y}_{i}$$ 로부터의 거리에 대한 $$\overrightarrow{y}_{i}$$ 와의 유사도**의 가우시안 분포에 기초했을 때, $$\overrightarrow{y}_{j}$$ 가 $$\overrightarrow{y}_{i}$$ 와 유사할 가능성을 다음과 같이 정의하자

    $$\begin{aligned}
    q_{j|i} 
    = \frac{\exp{\left[-||\overrightarrow{y}_{i}-\overrightarrow{y}_{j}||^{2}\right]}}{\sum_{k \ne i}{\exp{\left[-||\overrightarrow{y}_{i}-\overrightarrow{y}_{k}||^{2}\right]}}}\\\\

    ||\overrightarrow{y}_{i}-\overrightarrow{y}_{j}|| \sim N_{i}(0,\sigma^{2})
    \end{aligned}$$

    - $$\overrightarrow{y}$$ : 고차원 공간 상의 관측치 벡터 $$\overrightarrow{x}$$ 를 2차원 공간 상에 매핑한 벡터

- $$\overrightarrow{y}_{i}$$ 와 $$\overrightarrow{y}_{j}$$ 의 유사도를 다음과 같이 정의함

    $$\begin{aligned}
    q_{i,j}
    &= \frac{q_{j|i} + q_{i|j}}{2n}
    \end{aligned}$$

### 비용함수를 최소화하는 아규먼트 도출

- **KL(Kullback-Leibler Divergence)** : 확률변수 $X$ 의 동일한 아규먼트 $x_{i}$ 에 대하여 두 확률 분포 $P,Q$ 의 차이를 측정하는 지표

    $$
    \text{KL}(P|Q)
    = \sum_{i=1}^{n}{P(X=x_{i})\cdot\log{\frac{P(X=x_{i})}{Q(X=x_{i})}}}
    $$

- KL에 기초한 비용함수 정의

    $$\begin{aligned}
    Cost
    &= \sum_{i}{\text{KL}(P_{i}|Q_{i})}\\
    &= \sum_{i}{\sum_{j}{p_{i,j}\cdot\log{\frac{p_{i,j}}{q_{i,j}}}}}
    \end{aligned}$$

- $\hat{\mathbf{Y}}$ 도출

    $$\begin{aligned}
    \hat{\mathbf{Y}}
    &= \{\hat{\overrightarrow{y}}_{i}|\text{arg} \min_{\overrightarrow{y}_{i}}{Cost}\}
    \end{aligned}$$

## [sklearn.manifold.TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn-manifold-tsne)
-----

```python
from sklearn.manifold import TSNE
```

### General HyperParameter

- `random_state = None`
- `n_jobs(default : None)` : 병렬로 작업할 코어 갯수

### Model HyperParameter

- `n_components(default : 2)` : 축소할 차원의 갯수
- `metric(default : 'euclidean')` : 관측치 간 기하 거리 측정 방법
- `perplexity(default : 30.0)` : 참조할 Nearest Neighbors 갯수로서 5~50 사이의 값을 권장함
- `early_exaggeration(default :12.0)` : `perplexity` 에 기초하여 형성된 군집 간 거리

### Learning Task HyperParameter

- `learning_rate(default : 'auto')` : 학습률
- `n_iter(default : 1000)` : 다양체 탐색 횟수로서 250 이상의 값을 권장함
- `n_iter_without_progress(default : 300)` : 손실 함수가 몇 번 이상 개선되지 않을 경우 학습을 중단할 것인가