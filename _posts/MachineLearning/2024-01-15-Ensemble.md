---
order: 15
title: Ensemble
date: 2024-01-15
categories: [Machine Learning Techs, Machine Learning]
tags: [Machine Learning, Unsupervised Learning, Feature Engineering]
math: true
description: >-
  Based on the lecture â€œIntro. to Machine Learning (2023-2)â€ by Prof. Je Hyuk Lee, Dept. of Data Science, The Grad. School, Kookmin Univ.
image:
  path: /_post_refer_img/MachineLearning/Thumbnail.jpg
---

$$\begin{aligned}
\text{Error}
&= \mathbb{E}\left[\left(Y-\hat{f}(X)\right)^{2}\right]\\
&= \mathbb{E}\left[\left(f(X) + \varepsilon - \hat{f}(X)\right)^{2}\right]\\
&= \mathbb{E}\left[\left(f(X)-\hat{f}(X)\right)^{2} + \varepsilon^{2} - 2 \cdot \varepsilon \cdot \left(f(X)-\hat{f}(X)\right) \right]\\
&= \mathbb{E}\left[\left(f(X)-\hat{f}(X)\right)^{2}\right] + \mathbb{E}\left[\varepsilon^{2}\right] - 2 \cdot \mathbb{E}\left[\varepsilon\right] \cdot \mathbb{E}\left[f(X)-\hat{f}(X) \right]\\
&= \mathbb{E}\left[\left(f(X)-\hat{f}(X)\right)^{2}\right] + \sigma^{2}\\
\\
\mathbb{E}\left[\left(f(X)-\hat{f}(X)\right)^{2}\right]
&= \mathbb{E}\left[\left(f(X)-\overline{f}(X)+\overline{f}(X)-\hat{f}(X)\right)^{2}\right]\\
&= \mathbb{E}\left[\left(f(X)-\overline{f}(X)\right)^{2}\right] + \mathbb{E}\left[\left(\overline{f}(X)-\hat{f}(X)\right)^{2}\right] + 2 \cdot \mathbb{E}\left[f(X)-\overline{f}(X)\right] \cdot \mathbb{E}\left[\overline{f}(X)-\hat{f}(X)\right]\\
&= \mathbb{E}\left[\left(f(X)-\overline{f}(X)\right)^{2}\right] + \mathbb{E}\left[\left(\overline{f}(X)-\hat{f}(X)\right)^{2}\right]\\
\\
\mathbb{Bias}\left[\hat{f}(X)\right]
&= \mathbb{E}\left[\hat{f}(X)\right] - f(X)\\
\mathbb{Var}\left[\hat{f}(X)\right]
&= \mathbb{E}\left[\left(\overline{f}(X)-\hat{f}(X)\right)^{2}\right]\\
\\
\therefore \text{Error}
&= \mathbb{Bias}^{2}\left[\hat{f}(X)\right] + \mathbb{Var}\left[\hat{f}(X)\right] + \sigma^{2}
\end{aligned}$$

- $Y$ : ì‹¤ì œ ê´€ì¸¡ì¹˜
- $\varepsilon \sim N(0, \sigma^2)$ : ë…¸ì´ì¦ˆ
- $f(X)$ : ì‹¤ì œ í•¨ìˆ˜
- $\hat{f}(X)$ : $f(X)$ ì— ëŒ€í•œ ì˜ˆì¸¡ê°’
- $\overline{f}(X)$ : $\hat{f}(X)$ ì˜ í‰ê· 


## ì˜¤ì°¨ì™€ í¸í–¥, ë¶„ì‚° ê°„ ìƒê´€ê´€ê³„ ì´í•´

- **ì¡ìŒì˜ ì´í•´**

    $$\begin{aligned}
    y = f(x) + \varepsilon
    \end{aligned}$$

    - $y$ : ê´€ì¸¡ì¹˜ì˜ ì‹¤ì œê°’ìœ¼ë¡œì„œ ëª¨ìˆ˜

    - $f(x)$ : êµ¬í˜„í•˜ê³ ì í•˜ëŠ” ê´€ì¸¡ì¹˜ ìƒì„± í•¨ìˆ˜ë¡œì„œ ëª¨ìˆ˜ì˜ ì¶”ì •ëŸ‰

    - $\varepsilon$ : ëª¨ìˆ˜ë¥¼ ì •í™•í•˜ê²Œ ì¶”ì •í•˜ì§€ ëª»í•˜ëŠ” ì›ì¸ìœ¼ë¡œì„œ ì¡ìŒ(Noise)
        - $\varepsilon \sim N(0,\sigma^2)$ ê°€ì •

- ì˜¤ì°¨ì˜ êµ¬ì„±

    $$\begin{aligned}
    Err
    &= E[\varepsilon^2]
    &= E
    \end{aligned}$$


$$\begin{aligned}

Err
&= E[(y-\hat{y})^2]\\
&= E[(f(x)+\varepsilon-\hat{y})^2]\\
&= E[(f(x)-\hat{y})^2+\varepsilon^2+2\varepsilon(f(x)-\hat{y})]\\
&= E[(f(x)-\hat{y})^2]+E[\varepsilon^2]+2 \cdot E[\varepsilon] \cdot E[f(x)-\hat{y}]\\
&= E[(f(x)-\hat{y})^2] + \sigma^2 \quad (\because \varepsilon \sim N(0,\sigma^2))\\
\\
E[(f(x)-\hat{y})^2]
&= E[(f(x)-\overline{y}+\overline{y}-\hat{y})^2]\\
&= E[(f(x)-\overline{y})^2+(\overline{y}-\hat{y})^2+2 \cdot (f(x)-\overline{y}) \cdot (\overline{y}-\hat{y})]\\
&= E[(f(x)-\overline{y})^2]+E[(\overline{y}-\hat{y})^2]+2 \cdot E[f(x)-\overline{y}] \cdot E[\overline{y}-\hat{y}]\\
&= E[(f(x)-\overline{y})^2]+E[(\overline{y}-\hat{y})^2] \quad (\because E[f(x)-\overline{y}] = 0)\\
\\
Bias[\hat{y}]
&= E[\hat{y}-f(x)]\\
Var[\hat{y}]
&= E[(\hat{y}-E[\hat{y}])^2]\\
&= E[(\overline{y}-\hat{y})^2]\\
\\
\therefore Err
&= Bias[\hat{y}]^2 + Var[\hat{y}] + \sigma^2
\end{aligned}$$



## ëª¨ë¸ ë³µì¡ë„ì™€ ê³¼ì í•©ì˜ ìƒê´€ê´€ê³„

<p align="center"><a href="#"><img alt="ê³¼ì í•©" src="https://github.com/jayarnim/jayarnim/assets/116495744/8acf85ff-c937-41e5-9633-1613c3b4c0f7" width=80%></a></p>

- ëª¨ë¸ì„ ë‹¨ìˆœí•˜ê²Œ ì„¤ê³„í• ìˆ˜ë¡ ëª¨ë¸ì´ ê´€ì¸¡ì¹˜ì˜ ì „ë°˜ì ì¸ íŠ¹ì§•ì„ ì œëŒ€ë¡œ í¬ì°©í•˜ì§€ ëª»í•´ **ê³¼ì†Œì í•©(UnderFitting)** ë¨
- ëª¨ë¸ì„ ë³µì¡í•˜ê²Œ ì„¤ê³„í• ìˆ˜ë¡ ëª¨ë¸ì´ í‘œë³¸ì—ë§Œ êµ­í•œë˜ì–´ ë‚˜íƒ€ë‚˜ëŠ” ìƒì„¸í•œ íŠ¹ì§•ê¹Œì§€ í¬ì°©í•˜ê²Œ ë˜ì–´ í‘œë³¸ì— **ê³¼ëŒ€ì í•©(OverFitting)** ë¨

</br>

## ë¶„ì‚°, í¸í–¥ê³¼ ëª¨ë¸ ë³µì¡ë„ì˜ ìƒê´€ê´€ê³„

- **ì†ì‹¤í•¨ìˆ˜ì˜ ë¶„í•´**
    - ì„ í˜•íšŒê·€ì˜ ëª©ì í•¨ìˆ˜ì¸ ì”ì°¨ ììŠ¹ì˜ ê¸°ëŒ€ê°’ì„ $MSE$ ë¡œ ì •ì˜í•¨

        $$\begin{aligned}
        \min_{\beta_0, \beta_j^{\forall}} {MSE}
        \end{aligned}$$

    - $MSE$ ë¥¼ ì¶”ì •ëŸ‰ $\hat{Y}$ ì˜ ë¶„ì‚° $Var(\hat{Y})$ ì™€ í¸í–¥ $Bias(\hat{Y})$ ìœ¼ë¡œ ì–‘ë¶„í•  ìˆ˜ ìˆìŒ

        $$\begin{aligned}
        MSE
        &= E[(Y-\hat{Y})^2] \\
        &= Var(\hat{Y}) + (Bias(\hat{Y}))^2 \\
        &= Var(\hat{Y}) + \{(E(\hat{Y})-Y)^2\}
        \end{aligned}$$

- **ë¶„ì‚°ê³¼ í¸í–¥ì˜ ì´í•´**

    - $Var(\hat{Y})$ : ì¶”ì •ëŸ‰ì˜ ê¸°ëŒ€ê°’ $E(\hat{Y})$ ê³¼ ì¶”ì •ì¹˜ $\hat{y_i^{\forall}}$ ì˜ ì°¨ì´
    - $Bias(\hat{Y})$ : ëª¨ìˆ˜ $Y$ ì™€ ê·¸ ì¶”ì •ëŸ‰ì˜ ê¸°ëŒ€ê°’ $E(\hat{Y})$ ì˜ ì°¨ì´

    <p align="center"><a href="#"><img alt="ë¶„ì‚°ê³¼í¸í–¥ì˜ì´í•´" src="https://github.com/jayarnim/jayarnim/assets/116495744/cb542dc8-172e-4b66-9c09-93abdcbfec1a" width=80%></a></p>

- **ë¶„ì‚°, í¸í–¥ê³¼ ëª¨ë¸ ë³µì¡ë„ì˜ ê´€ê³„**

    <p align="center"><a href="#"><img alt="ë³µì¡ë„" src="https://github.com/jayarnim/jayarnim/assets/116495744/05e615db-1162-44ea-854b-11bebfc1c7a9" width=80%></a></p>

    - ëª¨ë¸ì„ ë³µì¡í•˜ê²Œ ì„¤ê³„í• ìˆ˜ë¡ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ëŠ” ê°ì†Œí•˜ëŠ” ë°˜ë©´, ì˜ˆì¸¡ê°’ê³¼ ê·¸ í‰ê· ì˜ ì°¨ì´ëŠ” ì¦ê°€í•¨
    - ëª¨ë¸ì„ ë‹¨ìˆœí•˜ê²Œ ì„¤ê³„í• ìˆ˜ë¡ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ëŠ” ì¦ê°€í•˜ëŠ” ë°˜ë©´, ì˜ˆì¸¡ê°’ê³¼ ê·¸ í‰ê· ì˜ ì°¨ì´ëŠ” ê°ì†Œí•¨
    - ì¦‰, ëª¨ë¸ ë³µì¡ë„ëŠ” í¸í–¥ê³¼ ìŒì˜ ìƒê´€ê´€ê³„, ë¶„ì‚°ê³¼ ì–‘ì˜ ìƒê´€ê´€ê³„ì— ìˆìŒ

</br>

## ì •í˜•í™”ì™€ ê·œì œ

<p align="center"><a href="#"><img alt="ì •í˜•í™”" src="https://github.com/jayarnim/jayarnim/assets/116495744/84cf465f-57ee-4b7e-8b04-04c1d80eff81" width=80%></a></p>

- **ì •í˜•í™”(Regularization)** : ëª¨ë¸ì´ í‘œë³¸ì— ê³¼ì í•©ë˜ì§€ ì•„ë‹ˆí•˜ê³  ì¼ë°˜í™”ë  ìˆ˜ ìˆë„ë¡ ê·œì œí•˜ëŠ” ì‘ì—…

    $$\begin{aligned}
    \min_{\beta_0, \beta_j^{\forall}}(MSE + Penalty)
    &= \min_{\beta_0, \beta_j^{\forall}}(E[(Y-\hat{Y})^2] + Penalty) \\
    &= \min_{\beta_0, \beta_j^{\forall}}(Var(\hat{Y}) + (Bias(\hat{Y}))^2 + Penalty)
    \end{aligned}$$

    - ìœ„ ë…¼ì˜ë¥¼ ì¢…í•©í•˜ë©´ í¸í–¥ì´ í´ìˆ˜ë¡ ëª¨ë¸ì´ ë‹¨ìˆœí•˜ê²Œ ì„¤ê³„ë˜ì–´ ê³¼ì†Œì í•©ë  ê°€ëŠ¥ì„±ì´ ë†’ìŒ
    - ë°˜ëŒ€ë¡œ ë¶„ì‚°ì´ í´ìˆ˜ë¡ ëª¨ë¸ì´ ë³µì¡í•˜ê²Œ ì„¤ê³„ë˜ì–´ í‘œë³¸ì— ê³¼ëŒ€ì í•©ë  ê°€ëŠ¥ì„±ì´ ë†’ìŒ
    - ëª¨ë¸ì„ ì¼ë°˜í™”í•˜ê¸° ìœ„í•´ í¸í–¥ê³¼ ë¶„ì‚°ì„ í¬ê´„í•˜ëŠ” $MSE$ ì— ê°€ì¤‘ì¹˜ì— ê´€í•œ ê·œì œ $Penalty$ ë¥¼ ë”í•˜ì—¬ ì˜ˆì¸¡ ììœ ë„ë¥¼ ì œí•œí•¨

- **ê·œì œ(Penalty)ì˜ ì¢…ë¥˜**

    <p align="center"><a href="#"><img alt="ê·œì œë¹„êµ" src="https://github.com/jayarnim/jayarnim/assets/116495744/5766884b-64dc-4099-a69f-6b295482458a" width=80%></a></p>

    - **L-1(Least Absolute Shrinkage and Selection Operator; LASSO)**

        $$\begin{aligned}
        Penalty
        &= \alpha ||\overrightarrow{w}||_{L1} \\
        &= \alpha \times (|\beta_1| + |\beta_2| + \cdots + |\beta_n|) \\
        &= \alpha \displaystyle\sum_{i=1}^{n}{|\beta_i|}
        \end{aligned}$$

    - **L-2(Ridge)**

        $$\begin{aligned}
        Penalty
        &= \alpha ||\overrightarrow{w}||_{L2} \\
        &= \alpha \times \{(\beta_1)^2 + (\beta_2)^2 + \cdots + (\beta_n)^2\} \\
        &= \alpha \displaystyle\sum_{i=1}^{n}{(\beta_i)^2}
        \end{aligned}$$











## Index

1. Ensemble
2. Voting
3. `sklearn.ensemble.VotingClassifier`

<hr></br>

## Ensemble

- **Ensemble** : ë‹¨ì¼ ëª¨ë¸ë“¤ì„ ê²°í•©í•˜ëŠ” ê¸°ë²•

- **êµ¬ë¶„**
    - **Voting** : ë‹¨ì¼ ëª¨ë¸ë“¤ì„ ìˆ˜ë™ìœ¼ë¡œ ê²°í•©í•˜ëŠ” ë°©ì‹

        <p align="center"><a href="#"><img alt="ë³´íŒ…" src="https://github.com/jayarnim/jayarnim/assets/116495744/505a65f5-cebd-4a62-a02d-3a6a7d19b066" width=80%></a></p>

    - **Bagging(Bootstrap Aggregating)** : ë‹¨ì¼ ëª¨ë¸ë“¤ì„ ë³‘ë ¬ë¡œ ì‘ì—…í•˜ë„ë¡ ì„¤ê³„í•˜ëŠ” ë°©ì‹

        <p align="center"><a href="#"><img alt="bagging" src="https://github.com/jayarnim/jayarnim/assets/116495744/901c311e-f4bf-4df5-901a-88873b1a3e66" width=80%></a></p>

    - **Boosting** : ë‹¨ì¼ ëª¨ë¸ë“¤ì„ ì§ë ¬ë¡œ ì‹¬í™” ì‘ì—…í•˜ë„ë¡ ì„¤ê³„í•˜ëŠ” ë°©ì‹

        <p align="center"><a href="#"><img alt="boosting" src="https://github.com/jayarnim/jayarnim/assets/116495744/020824f7-a8e4-4c67-825d-5c0cd3142f30" width=80%></a></p>

</br>

## Voting

- **Hard Voting** : ìµœë¹ˆê°’ìœ¼ë¡œ ê´€ì¸¡ì¹˜ì˜ ë²”ì£¼ë¥¼ ì„ ì •í•˜ëŠ” ë°©ì‹

    <p align="center"><img alt="í•˜ë“œë³´íŒ…" src="https://github.com/jayarnim/jayarnim/assets/116495744/115cdb86-1d0d-4cd9-adc7-136798eed8d1" width=80%></p>

- **Soft Voting** : ê° ë²”ì£¼ì— ì†í•  í™•ë¥ ì— ëŒ€í•œ í‰ê· ì´ ê°€ì¥ í° ë²”ì£¼ë¡œ ê´€ì¸¡ì¹˜ì˜ ë²”ì£¼ë¥¼ ì„ ì •í•˜ëŠ” ë°©ì‹

    <p align="center"><img alt="ì†Œí”„íŠ¸ë³´íŒ…" src="https://github.com/jayarnim/jayarnim/assets/116495744/fd6eaad0-54fa-403e-9957-345999429e6e" width=80%></p>

</br>

## [`sklearn.ensemble.VotingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn-ensemble-votingclassifier)

```python
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import VotingClassifier

estimator_list = [
    ("Support Vector Machine", SVC()),
    ("k-NN", KNeighborsClassifier()),
    ("Logistic Regression", LogisticRegression())
    ]

voting_clf = VotingClassifier(
    n_jobs = -1,
    estimators = estimator_list,
    voting = 'soft'
    )
```
### ğŸ’¡ General HyperParameter

- `n_jobs(default : None)` : ë³‘ë ¬ë¡œ ì‘ì—…í•  ì½”ì–´ ê°¯ìˆ˜

### ğŸ’¡ Model HyperParameter

- `estimators` : ë™ì›í•  ì•Œê³ ë¦¬ì¦˜ ëª©ë¡

- `weights(default : None)` : ê° ì•Œê³ ë¦¬ì¦˜ë³„ ê°€ì¤‘ì¹˜

- `voting(default : 'hard')` : ê²°í•© ë°©ì‹

    - `'hard'` : Hard Voting
    - `'soft'` : Soft Voting

</br><hr>

#### ì´ë¯¸ì§€ ì¶œì²˜
- https://velog.io/@jochedda