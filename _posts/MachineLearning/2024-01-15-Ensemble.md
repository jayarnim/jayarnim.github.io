---
order: 15
title: Ensemble
date: 2024-01-15
categories: [Machine Learning Techs, Machine Learning]
tags: [Machine Learning, Unsupervised Learning, Feature Engineering]
math: true
description: >-
    Based on the lecture “Intro. to Machine Learning (2023-2)” by Prof. Je Hyuk Lee, Dept. of Data Science, The Grad. School, Kookmin Univ.
image:
    path: /_post_refer_img/MachineLearning/Thumbnail.jpg
---

## Generalization Problem
-----

### Components of Error

![01](/_post_refer_img/MachineLearning/15-01.png){: width="100%"}

$$\begin{aligned}
\text{Error}
&= \mathbb{E}\left[\left(Y-\hat{f}(X)\right)^{2}\right]\\
&= \mathbb{E}\left[\left(f(X) + \varepsilon - \hat{f}(X)\right)^{2}\right]\\
&= \mathbb{E}\left[\left(f(X)-\hat{f}(X)\right)^{2} + \varepsilon^{2} - 2 \cdot \varepsilon \cdot \left(f(X)-\hat{f}(X)\right) \right]\\
&= \mathbb{E}\left[\left(f(X)-\hat{f}(X)\right)^{2}\right] + \mathbb{E}\left[\varepsilon^{2}\right] - 2 \cdot \mathbb{E}\left[\varepsilon\right] \cdot \mathbb{E}\left[f(X)-\hat{f}(X) \right]\\
&= \mathbb{E}\left[\left(f(X)-\hat{f}(X)\right)^{2}\right] + \sigma^{2}\\
\\
\mathbb{E}\left[\left(f(X)-\hat{f}(X)\right)^{2}\right]
&= \mathbb{E}\left[\left(f(X)-\overline{f}(X)+\overline{f}(X)-\hat{f}(X)\right)^{2}\right]\\
&= \mathbb{E}\left[\left(f(X)-\overline{f}(X)\right)^{2}\right] + \mathbb{E}\left[\left(\overline{f}(X)-\hat{f}(X)\right)^{2}\right] + 2 \cdot \mathbb{E}\left[f(X)-\overline{f}(X)\right] \cdot \mathbb{E}\left[\overline{f}(X)-\hat{f}(X)\right]\\
&= \mathbb{E}\left[\left(f(X)-\overline{f}(X)\right)^{2}\right] + \mathbb{E}\left[\left(\overline{f}(X)-\hat{f}(X)\right)^{2}\right]\\
\\
\mathbb{Bias}\left[\hat{f}(X)\right]
&= \mathbb{E}\left[\hat{f}(X)\right] - f(X)\\
\mathbb{Var}\left[\hat{f}(X)\right]
&= \mathbb{E}\left[\left(\overline{f}(X)-\hat{f}(X)\right)^{2}\right]\\
\\
\therefore \text{Error}
&= \mathbb{Bias}^{2}\left[\hat{f}(X)\right] + \mathbb{Var}\left[\hat{f}(X)\right] + \sigma^{2}
\end{aligned}$$

- $Y$ : 실제 관측치
- $\varepsilon \sim N(0, \sigma^2)$ : 노이즈
- $f(X)$ : 실제 함수
- $\hat{f}(X)$ : $f(X)$ 에 대한 예측값
- $\overline{f}(X)$ : $\hat{f}(X)$ 의 평균

### Bias-Variance Trade-off

![02](/_post_refer_img/MachineLearning/15-02.png){: width="100%"}

- **편향(Bias)** : 모형이 학습 데이터의 패턴을 충분히 학습하지 못해 발생하는 **과소적합(Underfitting)** 문제로서, 학습 데이터에 내재된 패턴의 복잡도에 비해 모형이 간소화되어 설계된 경우 발생함

- **분산(Variance)** : 모형이 학습 데이터에 너무 과도하게 적응하여 발생하는 **과대적합(Overfitting)** 문제로서, 모형 복잡도에 비해 학습 데이터가 희소한 경우 발생함

- **편향-분산 트레이드오프(Bias-Variance Trade-off)** : 모형 복잡도에 따른 편향과 분산의 상충 관계

## Ensemble
-----

- **앙상블 기법(Ensemble)** : 예측 오차를 줄이기 위하여 다양한 모형들을 결합하는 기법
    - 모형 간 다양성을 어떻게 확보할 것인가?
        - provide different random subset of the training data to each other
        - using some measurement ensuring it is substantially different from the other memebers

    - 모형별 결과물을 어떻게 결합할 것인가?

- **구분**

    - **Bagging(Bootstrap Aggregating)** : By Reducing Variance, Prevent Overfitting

        ![03](/_post_refer_img/MachineLearning/15-03.png){: width="100%"}

    - **Boosting** : By Reducing Bias, Prevent Underfitting

        ![04](/_post_refer_img/MachineLearning/15-04.png){: width="100%"}

### Necessity

- $m$ 번째 모형의 예측값을 다음과 같이 정의하자

    $$\begin{aligned}
    \underbrace{y_{m}(x)}_{\begin{array}{c} \text{predict value} \\ \text{of m-th model} \end{array}}
    &= \underbrace{f(x)}_{\text{real value}} + \underbrace{\epsilon_{m}(x)}_{\begin{array}{c} \text{error} \\ \text{of m-th model} \end{array}}
    \end{aligned}$$

- $m$ 번째 모형의 예측 오차의 자승을 다음과 같이 나타낼 수 있음

    $$\begin{aligned}
    \therefore \mathbb{E}\left[\epsilon_{m}^{2}(x)\right]
    &= \mathbb{E}\left[\left(y_{m} - f(x)\right)^{2}\right]
    \end{aligned}$$

- 개별 모형의 예측 오차 자승 평균 $\xi_{\text{avg}}$

    $$\begin{aligned}
    \xi_{\text{avg}}
    &= \frac{1}{M}\sum_{m=1}^{M}{\mathbb{E}\left[\epsilon_{m}^{2}(x)\right]}
    \end{aligned}$$

- 앙상블 모형의 예측 오차 자승 $\xi_{\text{ensemble}}$

    $$\begin{aligned}
    \xi_{\text{ensemble}}
    &= \mathbb{E}\left[\left(\frac{1}{M}\sum_{m=1}^{M}{y_{m}(x)}-f(x)\right)^{2}\right]\\
    &= \mathbb{E}\left[\left(\frac{1}{M}\sum_{m=1}^{M}{y_{m}(x)}-\frac{1}{M}\sum_{m=1}^{M}{f(x)}\right)^{2}\right]\\
    &= \mathbb{E}\left[\left(\frac{1}{M}\sum_{m=1}^{M}{\epsilon_{m}(x)}\right)^{2}\right]
    \end{aligned}$$

- Cauchy-Schwartz Inequation

    $$\begin{aligned}
    \left(\alpha_{1} \cdot \beta_{1} + \cdots + \alpha_{n} \cdot \beta_{n}\right)^{2} \le \left(\alpha_{1}^{2}+\cdots+\alpha_{n}^{2}\right)\left(\beta_{1}^{2}+\cdots+\beta_{n}^{2}\right)
    \end{aligned}$$

- $\text{if} \quad \alpha_{m}=1, \beta_{m}=\epsilon_{m}(x)$

    $$\begin{aligned}
    \left(\sum_{m=1}^{M}{1 \cdot \epsilon_{m}(x)}\right)^{2} \le M \cdot \sum_{m=1}^{M}{\epsilon_{m}^{2}(x)}
    \end{aligned}$$

- $\times \displaystyle\frac{1}{M^{2}}$

    $$\begin{aligned}
    \underbrace{\left(\frac{1}{M}\sum_{m=1}^{M}{\epsilon_{m}(x)}\right)^{2}}_{\xi_{\text{ensemble}}} \le \underbrace{\frac{1}{M} \sum_{m=1}^{M}{\epsilon_{m}^{2}(x)}}_{\xi_{\text{avg}}}
    \end{aligned}$$