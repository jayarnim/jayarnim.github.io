## Index

1. GBM
2. `sklearn.ensemble.GradientBoostingClassifier`

<hr></br>

## GBM

- [**Ensemble**](https://velog.io/@jayarnim/Ensemble-and-Voting)

- **Boosting** : ë‹¨ì¼ ëª¨ë¸ë“¤ì„ ì§ë ¬ë¡œ ì‹¬í™” ì‘ì—…í•˜ë„ë¡ ì„¤ê³„í•˜ëŠ” ë°©ì‹

    <p align="center"><a href="#"><img alt="boosting" src="https://github.com/jayarnim/jayarnim/assets/116495744/020824f7-a8e4-4c67-825d-5c0cd3142f30" width=80%></a></p>

    - ì´ì „ ëª¨ë¸ì— ëŒ€í•˜ì—¬ ê·¸ ì†ì‹¤ì´ í° ê´€ì¸¡ì¹˜ë“¤ì„ **í•™ìŠµë¥ (Learning Rate)**ë§Œí¼ ê°€ì¤‘í•˜ì—¬ ì‹¬í™”í•™ìŠµí•¨

        - [**ê²½ì‚¬í•˜ê°•ë²•ê³¼ í•™ìŠµë¥ ì˜ ì´í•´**](https://velog.io/@jayarnim/%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95%EA%B3%BC-SGDRegression)

- **Gradient Boosting Machine(GBM)** : **Decision Tree** ì— ê¸°ë°˜í•œ **Boosting Ensemble Algorithm**

    <p align="center"><a href="#"><img alt="ê·¸ë¼ë””ì–¸íŠ¸ë¶€ìŠ¤íŒ…ë¨¸ì‹ " src="https://github.com/jayarnim/jayarnim/assets/116495744/353d194d-74db-4a05-b0e9-88248d3925e9" width=80%></a></p>

    - [**Decision Tree**](https://velog.io/@jayarnim/%EA%B7%A0%EC%9D%BC%EB%8F%84%EC%99%80-Decision-Tree)

</br>

## [`sklearn.ensemble.GradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn-ensemble-gradientboostingclassifier)

```python
from sklearn.ensemble import GradientBoostingClassifier
```

### ğŸ’¡ General HyperParameter

- `random_state(default : None)`
- `warm_start(default : False)` : ë™ì‘ ë©”ì‹œì§€ ì¶œë ¥ ì—¬ë¶€ ì„¤ì •

### ğŸ’¡ Model HyperParameter

- **`criterion(default : 'friedman_mse')` : ì˜¤ì°¨($\approx$ ë¶ˆìˆœë„) ì¸¡ì • ë°©ë²•**
    - `'friedman_mse'`
    - `'squared_error'`

- `max_features(default : None)` : ê·œì¹™ ì„¤ê³„ ì‹œ ê³ ë ¤í•  ì„¤ëª…ë³€ìˆ˜ ê°¯ìˆ˜

    - `'sqrt'` : $\sqrt{n}$
    - `'log2'` : $\log_2{n}$
    - `None` : $n$

- **`n_estimators(default : 100)` : ë™ì›í•  Decision Tree ì˜ ê°œìˆ˜**

### ğŸ’¡ Gradient Descent HyperParameter

- `learning_rate(default : 0.1)` : í•™ìŠµë¥ 

- `loss(default : 'log_loss')` : ì†ì‹¤ í•¨ìˆ˜
    - `'log_loss'`
    - `'deviance'`
    - `'exponential'`

- `tol(default : 0.0001)` : í—ˆìš© ì†ì‹¤

- `n_iter_no_change(default : None)` : ì†ì‹¤ì´ ëª‡ íšŒ ì´ìƒ ìœ íš¨í•˜ê²Œ ê°ì†Œí•˜ì§€ ì•Šì„ ê²½ìš° í•™ìŠµì„ ì¤‘ë‹¨í•  ê²ƒì¸ê°€

### ğŸ’¡ Pruning

- **Pre-Pruning**
    - `max_depth(default : 3)` : íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´
    - `max_leaf_nodes(default : None)` : ë¦¬í”„ ë…¸ë“œì˜ ìµœëŒ€ ê°¯ìˆ˜
    - `min_samples_split(default : 2)` : í•˜ìœ„ ë…¸ë“œë¡œ ê°€ì§€ë¥¼ ë»—ê¸° ìœ„í•´ í•„ìš”í•œ ìµœì†Œí•œì˜ ìƒ˜í”Œ ê°œìˆ˜
    - `min_impurity_decrease(default : 0)` : í•˜ìœ„ ë…¸ë“œë¡œ ê°€ì§€ë¥¼ ë»—ê¸° ìœ„í•´ í•„ìš”í•œ ìµœì†Œí•œì˜ ë¶ˆìˆœë„ ê°œì„  ì •ë„
    - `min_samples_leaf(default : 1)` : ë¦¬í”„ ë…¸ë“œê°€ ë˜ê¸° ìœ„í•´ í•„ìš”í•œ ìµœì†Œí•œì˜ ìƒ˜í”Œ ê°œìˆ˜

- **Cost-Complexity Pruning**

    - `ccp_alpha(default : 0)`









## Index

1. XGBoost
2. `xgboost.XGBClassifier`

<hr></br>

## XGBoost

- [**Ensemble**](https://velog.io/@jayarnim/Ensemble-and-Voting)

- **Boosting** : ë‹¨ì¼ ëª¨ë¸ë“¤ì„ ì§ë ¬ë¡œ ì‹¬í™” ì‘ì—…í•˜ë„ë¡ ì„¤ê³„í•˜ëŠ” ë°©ì‹

    <p align="center"><a href="#"><img alt="boosting" src="https://github.com/jayarnim/jayarnim/assets/116495744/020824f7-a8e4-4c67-825d-5c0cd3142f30" width=80%></a></p>

    - ì´ì „ ëª¨ë¸ì— ëŒ€í•˜ì—¬ ê·¸ ì†ì‹¤ì´ í° ê´€ì¸¡ì¹˜ë“¤ì„ **í•™ìŠµë¥ (Learning Rate)**ë§Œí¼ ê°€ì¤‘í•˜ì—¬ ì‹¬í™”í•™ìŠµí•¨

        - [**ê²½ì‚¬í•˜ê°•ë²•ê³¼ í•™ìŠµë¥ ì˜ ì´í•´**](https://velog.io/@jayarnim/%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95%EA%B3%BC-SGDRegression)

- **eXtreme Gradient Boosting(XGBoost)** : GBMì„ ê· í˜• íŠ¸ë¦¬ ë¶„í•  ë°©ì‹ì„ ë³´ì¡´í•˜ë©´ì„œ ë³´ì™„í•œ ì•Œê³ ë¦¬ì¦˜

    - [**Decision Tree**](https://velog.io/@jayarnim/%EA%B7%A0%EC%9D%BC%EB%8F%84%EC%99%80-Decision-Tree)

    - [**Gradient Boosting Machine**](https://velog.io/@jayarnim/GBM)

    - **ê· í˜• íŠ¸ë¦¬ ë¶„í•  ë°©ì‹(Level-Wise Tree Growth)** : ëª¨ë“  ë¦¬í”„ ë…¸ë“œì—ì„œ ê· ë“±í•˜ê²Œ ê°€ì§€ì¹˜ê¸°í•˜ëŠ” ë°©ì‹

        <p align="center"><a href="#"><img alt="level" src="https://github.com/jayarnim/jayarnim/assets/116495744/f99789f7-ee66-4750-8bac-6c5caa9eb8a7" width=80%></a></p>

        - **ì¥ì ** : íŠ¸ë¦¬ í˜•íƒœê°€ ë¹„ëŒ€ì¹­ì ìœ¼ë¡œ í˜•ì„±ë˜ê±°ë‚˜ ê¹Šì´ê°€ ì‹¬í™”ë˜ì§€ ì•Šì•„ ê³¼ëŒ€ì í•©ì„ ë°©ì§€í•  ìˆ˜ ìˆìŒ
        - **ë‹¨ì ** : ì†ì‹¤ì„ ìµœì†Œí™”í•˜ì§€ ëª»í•  ê°€ëŠ¥ì„±ì´ ë†’ìŒ

</br>

## `xgboost.XGBClassifier`

```python
from xgboost import XGBClassifier
```

### ğŸ’¡ General HyperParameter

- `random_state(default : None)`
- `silent(default : 1)` : ë™ì‘ ë©”ì‹œì§€ ì¶œë ¥ ì—¬ë¶€ ì„¤ì •
- `n_jobs(default : None)` : ë³‘ë ¬ë¡œ ì‘ì—…í•  ì½”ì–´ ê°¯ìˆ˜

### ğŸ’¡ Model HyperParameter

- `objective(default : 'binary:logistic')` : ëª©ì  í•¨ìˆ˜ë¡œì„œ GBMì˜ `criterion` ê³¼ ìœ ì‚¬í•¨
    - `'binary:logistic'` : ì´í•­ ë¶„ë¥˜ ì‹œ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ì¸¡ì¹˜ê°€ ê° ë²”ì£¼ì— ì†í•  í™•ë¥ ì„ ë°˜í™˜í•¨
    - `'multi:softprob'` : ë‹¤í•­ ë¶„ë¥˜ ì‹œ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ì¸¡ì¹˜ê°€ ê° ë²”ì£¼ì— ì†í•  í™•ë¥ ì„ ë°˜í™˜í•¨
    - `'multi:softmax'` : ë‹¤í•­ ë¶„ë¥˜ ì‹œ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ì¸¡ì¹˜ê°€ ì†í•  í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ë²”ì£¼ë¥¼ ë°˜í™˜í•¨

- `n_estimators(default : 100)` : ë™ì›í•  Decision Tree ì˜ ê°œìˆ˜

### ğŸ’¡ Gradient Descent HyperParameter

- `learning_rate(default : None)` : í•™ìŠµë¥ ë¡œì„œ í†µìƒ $0.1 \sim 0.2$ ì˜ ê°’ì„ ì„¤ì •í•¨

- `early_stopping_rounds(default : None)` : ì†ì‹¤ì´ ëª‡ íšŒ ì´ìƒ ìœ íš¨í•˜ê²Œ ê°ì†Œí•˜ì§€ ì•Šì„ ê²½ìš° í•™ìŠµì„ ì¤‘ë‹¨í•  ê²ƒì¸ê°€

### ğŸ’¡ Pruning

- **Pre-Pruning**

    - `max_depth(default : None)` : íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´ë¡œì„œ í†µìƒ $3 \sim 10$ ì˜ ê°’ì„ ì„¤ì •í•¨
    - `min_child_weight(default : None)` : ì–´ë–¤ ë…¸ë“œì— ìœ„ì¹˜í•œ ëª¨ë“  ê´€ì¸¡ì¹˜ì˜ ê°€ì¤‘ì¹˜ í•©ì— ëŒ€í•˜ì—¬, í•´ë‹¹ ë…¸ë“œê°€ í•˜ìœ„ ë…¸ë“œë¡œ ê°€ì§€ì¹˜ê¸°í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ìµœì†Œí•©
    - `max_leaves(default : None)` : ë¦¬í”„ ë…¸ë“œì˜ ìµœëŒ€ ê°¯ìˆ˜
    - `gamma(default : None)` : í•˜ìœ„ ë…¸ë“œë¡œ ê°€ì§€ì¹˜ê¸° í•˜ê¸° ìœ„í•œ ìµœì†Œ ì†ì‹¤ ê°ì†Œ ê°’

### ğŸ’¡ Validation Hyperperameter

```python
XGBClassifier.fit()
```

- `eval_set` : êµì°¨ê²€ì¦ ì‹œ ì‚¬ìš©í•  ê²€ì¦ìš© ë°ì´í„° ì„¸íŠ¸

- `eval_metric` : êµì°¨ê²€ì¦ ì‹œ ì‚¬ìš©í•  í‰ê°€ ì§€í‘œ
    - `error` : Binary classification error rate(ì„ê³„ê°’ 0.5)
    - `logloss` : Negative log-likelihood
    - `merror` : multiclass classification error rate
    - `mlogloss` : multiclass logloss
    - `auc` : area under curve








## Index

1. LightGBM
2. `lightgbm.LGBMClassifier`

<hr></br>

## LightGBM

- [**Ensemble**](https://velog.io/@jayarnim/Ensemble-and-Voting)

- **Boosting** : ë‹¨ì¼ ëª¨ë¸ë“¤ì„ ì§ë ¬ë¡œ ì‹¬í™” ì‘ì—…í•˜ë„ë¡ ì„¤ê³„í•˜ëŠ” ë°©ì‹

    <p align="center"><a href="#"><img alt="boosting" src="https://github.com/jayarnim/jayarnim/assets/116495744/020824f7-a8e4-4c67-825d-5c0cd3142f30" width=80%></a></p>

    - ì´ì „ ëª¨ë¸ì— ëŒ€í•˜ì—¬ ê·¸ ì†ì‹¤ì´ í° ê´€ì¸¡ì¹˜ë“¤ì„ **í•™ìŠµë¥ (Learning Rate)**ë§Œí¼ ê°€ì¤‘í•˜ì—¬ ì‹¬í™”í•™ìŠµí•¨

        - [**ê²½ì‚¬í•˜ê°•ë²•ê³¼ í•™ìŠµë¥ ì˜ ì´í•´**](https://velog.io/@jayarnim/%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95%EA%B3%BC-SGDRegression)

- **Light Gradient Boosting Machine(LightGBM)** : GBMì„ ë¦¬í”„ ì¤‘ì‹¬ íŠ¸ë¦¬ ë¶„í•  ë°©ì‹ìœ¼ë¡œ ë³´ì™„í•œ ì•Œê³ ë¦¬ì¦˜

    - [**Decision Tree**](https://velog.io/@jayarnim/%EA%B7%A0%EC%9D%BC%EB%8F%84%EC%99%80-Decision-Tree)

    - [**Gradient Boosting Machine**](https://velog.io/@jayarnim/GBM)

    - **ë¦¬í”„ ì¤‘ì‹¬ íŠ¸ë¦¬ ë¶„í•  ë°©ì‹(Leaf-Wise Tree Growth)** : ë¦¬í”„ ë…¸ë“œ ì¤‘ ì†ì‹¤ì´ ê°€ì¥ í° ë…¸ë“œë§Œ ì·¨ì‚¬ì„ íƒí•˜ì—¬ ë¶„í• í•˜ëŠ” ë°©ì‹

        <p align="center"><a href="#"><img alt="leaf" src="https://github.com/jayarnim/jayarnim/assets/116495744/f2e375a3-baa6-43e7-ba0f-4a8940581a7f" width=80%></a></p>

        - **ì¥ì ** : ë¦¬í”„ ë…¸ë“œ ì¤‘ ì†ì‹¤ì´ í° ë…¸ë“œë¥¼ ì‹¬í™” ì‘ì—…í•˜ë¯€ë¡œ ì†ì‹¤ ìµœì†Œí™”ì— ë›°ì–´ë‚¨
        - **ë‹¨ì ** : íŠ¸ë¦¬ í˜•íƒœê°€ ë¹„ëŒ€ì¹­ì ìœ¼ë¡œ í˜•ì„±ë˜ê±°ë‚˜ ê¹Šì´ê°€ ì‹¬í™”ë˜ì–´ ê³¼ëŒ€ì í•©ë  ê°€ëŠ¥ì„±ì´ ë†’ìŒ

</br>

## `lightgbm.LGBMClassifier`

```python
from lightgbm import LGBMClassifier
```

### ğŸ’¡ General HyperParameter

- `random_state(default : None)`
- `silent(default : 1)` : ë™ì‘ ë©”ì‹œì§€ ì¶œë ¥ ì—¬ë¶€ ì„¤ì •
- `n_jobs(default : -1)` : ë³‘ë ¬ë¡œ ì‘ì—…í•  ì½”ì–´ ê°¯ìˆ˜

### ğŸ’¡ Model HyperParameter

- `objective(default : None)` : ëª©ì  í•¨ìˆ˜ë¡œì„œ GBMì˜ `criterion` ê³¼ ìœ ì‚¬í•¨
    - `binary` : ì´í•­ ë¶„ë¥˜ ë¶„ì„
    - `multiclass` : ë‹¤í•­ ë¶„ë¥˜ ë¶„ì„

- `n_estimators(default : 100)` : ë™ì›í•  Decision Tree ì˜ ê°œìˆ˜

### ğŸ’¡ Gradient Descent HyperParameter

- `learning_rate(default : 0.1)` : í•™ìŠµë¥ ë¡œì„œ í†µìƒ $0.1 \sim 0.2$ ì˜ ê°’ì„ ì„¤ì •í•¨

- `early_stopping_rounds(default : None)` : ì†ì‹¤ì´ ëª‡ íšŒ ì´ìƒ ìœ íš¨í•˜ê²Œ ê°ì†Œí•˜ì§€ ì•Šì„ ê²½ìš° í•™ìŠµì„ ì¤‘ë‹¨í•  ê²ƒì¸ê°€

### ğŸ’¡ Pruning

- **Pre-Pruning**

    - `max_depth(default : -1)` : íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´
    - `num_leaves(default : 31)` : ë¦¬í”„ ë…¸ë“œì˜ ìµœëŒ€ ê°¯ìˆ˜
    - `min_child_samples(default : 20)` : ë¦¬í”„ ë…¸ë“œ ê´€ì¸¡ì¹˜ì˜ ìµœì†Œ ê°¯ìˆ˜
    - `min_child_weight(default : 0.001)` : ì–´ë–¤ ë…¸ë“œì— ìœ„ì¹˜í•œ ëª¨ë“  ê´€ì¸¡ì¹˜ì˜ ê°€ì¤‘ì¹˜ í•©ì— ëŒ€í•˜ì—¬, í•´ë‹¹ ë…¸ë“œê°€ í•˜ìœ„ ë…¸ë“œë¡œ ê°€ì§€ì¹˜ê¸°í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ìµœì†Œí•©
    - `min_split_gain(default : 0.0)` : í•˜ìœ„ ë…¸ë“œë¡œ ê°€ì§€ì¹˜ê¸° í•˜ê¸° ìœ„í•œ ìµœì†Œ ì†ì‹¤ ê°ì†Œ ê°’

### ğŸ’¡ To Prevent Underfitting

- **About Imbalance in the number of observations between categories**

    - `class_weight(default : None)` : ê°€ì¤‘í•  ë²”ì£¼ì™€ ê·¸ ê°’

        - `'balanced'`
        - `dictionary type`

### ğŸ’¡ Validation Hyperperameter

```python
LGBMClassifier.fit()
```

- `eval_set` : êµì°¨ê²€ì¦ ì‹œ ì‚¬ìš©í•  ê²€ì¦ìš© ë°ì´í„° ì„¸íŠ¸

- `eval_metric` : êµì°¨ê²€ì¦ ì‹œ ì‚¬ìš©í•  í‰ê°€ ì§€í‘œ
    - `error` : Binary classification error rate(ì„ê³„ê°’ 0.5)
    - `logloss` : Negative log-likelihood
    - `merror` : multiclass classification error rate
    - `mlogloss` : multiclass logloss
    - `auc` : area under curve