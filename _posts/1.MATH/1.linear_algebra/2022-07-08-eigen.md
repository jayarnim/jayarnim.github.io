---
order: 5
title: Eigen
date: 2022-07-08
categories: [1.MATHEMATICAL TECHS, 1.linear algebra]
tags: [mathematics, linear algebra]
math: true
description: >-
    Based on the lecture “Mathematics for Artificial Intelligence (2022-1)” by Prof. Yeo Jin Chung, Dept. of AI, Big Data & Management, College of Business Administration, Kookmin Univ.
image:
    path: /_post_refer_img/1.MATH/1.linear_algebra/Thumbnail.png
---

## Eigen
-----

- 정방행렬 $\mathbf{A} \in \mathbb{R}^{N \times N}$ 로 선형 변환하였을 때 그 방향은 변하지 않고 길이만 변하는 벡터를 $\mathbf{A}$ 의 고유벡터(Eigen Vector), 이때 길이의 변화량을 고유값(Eigen Value)이라고 정의함

    ![01](/_post_refer_img/1.MATH/1.linear_algebra/05-01.png){: width="100%"}

    $$\begin{gathered}
    \mathbf{A}\mathbf{v}
    = \lambda\mathbf{v},\quad \mathbf{v}\ne\mathbf{0}\\
    \Updownarrow \\
    \begin{bmatrix}
    a_{1,1} & a_{1,2} & \cdots & a_{1,N}\\
    a_{2,1} & a_{2,2} & \cdots & a_{2,N}\\
    \vdots & \vdots & \ddots & \vdots\\
    a_{N,1} & a_{N,2} & \cdots & a_{N,N}\\
    \end{bmatrix}
    \begin{bmatrix}
    v_{1}\\
    v_{2}\\
    \vdots\\
    v_{N}\\
    \end{bmatrix}
    = \lambda \begin{bmatrix}
    v_{1}\\
    v_{2}\\
    \vdots\\
    v_{N}\\
    \end{bmatrix}
    \end{gathered}$$

- 연산 규칙:
    - $\mathrm{tr}(\mathbf{A})=\sum_{i=1}^{N}{\lambda_{i}}$
    - $\mathrm{det}(\mathbf{A})=\prod_{i=1}^{N}{\lambda_{i}}$
    - $\mathbf{A}\mathbf{v}=\lambda\mathbf{v} \quad \text{and} \quad \mathbf{A}^{T}\mathbf{w}=\lambda\mathbf{w}$
    - $\lambda_{i}=0 \Rightarrow \nexists \mathbf{A}^{-1}$
    - $$\lambda_{i} \ne \lambda_{j} \Rightarrow \alpha_{i}\mathbf{v}_{i} + \alpha_{j}\mathbf{v}_{j} \ne 0,\forall \alpha \ne 0$$ $$\quad$$

- 특성 방정식(Characteristic Equation):

    $$\begin{aligned}
    (\mathbf{A} - \lambda \mathbf{I})\mathbf{v}
    &=\mathbf{0},\quad \mathbf{v}\ne\mathbf{0}
    \end{aligned}$$

    - 정해: $$\nexists (\mathbf{A}-\lambda \mathbf{I})^{-1} \Leftrightarrow \exists \mathbf{v},\lambda$$
    - 불능: $$\exists (\mathbf{A}-\lambda \mathbf{I})^{-1} \Leftrightarrow \nexists \mathbf{v},\lambda$$

## Diagonalization
-----

- **행렬의 대각화(Diagonalization)**: 정방행렬 $$\mathbf{A} \in \mathbb{R}^{N \times N}$$ 에 대하여 $$\mathbf{P}^{-1}\mathbf{A}\mathbf{P}$$ 가 대각행렬 $$\Lambda=\mathrm{diag}(\lambda_{1},\cdots,\lambda_{N})$$ 이 되도록 만드는 정방행렬 $$\mathbf{P}(\ne \mathbf{0})$$ 이 이 존재하는 경우

    $$\begin{aligned}
    \mathbf{P}^{-1}\mathbf{A}\mathbf{P}
    &= \Lambda \\
    &= \mathrm{diag}(\lambda_{1},\cdots,\lambda_{N})
    \end{aligned}$$

    - $$\mathbf{P}$$: 고유벡터 행렬 혹은 대각화 행렬
    - $$\Lambda$$: 고유값 대각 행렬

- **대칭행렬의 직교 대각화**

    - 대칭행렬 $$\mathbf{A} \in \mathbb{R}^{N \times N}$$ 에 대하여 그 고유값이 $$\lambda_1,\lambda_2,\cdots,\lambda_n$$ 이고, 고유벡터가 $$\mathbf{v}_{1},\mathbf{v}_{2},\cdots,\mathbf{v}_{N}$$ 이라고 하자

    $$\begin{aligned}
    \mathbf{A}\mathbf{v}_{i}
    &= \lambda_{i}\mathbf{v}_{i}, \quad i=1,2,\cdots,N
    \end{aligned}$$

    - $$\mathbf{A}$$ 의 고유벡터 $$\mathbf{v}_{1},\mathbf{v}_{2},\cdots, \mathbf{v}_{N}$$ 는 항상 직교함

        $$\begin{aligned}
        \mathbf{v}_{1} \perp \mathbf{v}_{2} \perp \cdots \perp \mathbf{v}_{N}
        \end{aligned}$$

    - $$\mathbf{A}$$ 의 고유벡터들로 구성된 대각화 행렬 $$\mathbf{P}=\begin{bmatrix}\mathbf{v}_{1}&\mathbf{v}_{2}&\cdots&\mathbf{v}_{N}\end{bmatrix}$$ 은 항상 직교행렬임

        $$\begin{aligned}
        \mathbf{P}^{T}
        &=\mathbf{P}^{-1}
        \end{aligned}$$

    - 직교행렬 $$\mathbf{P}=\begin{bmatrix}\mathbf{v}_{1}&\mathbf{v}_{2}&\cdots&\mathbf{v}_{N}\end{bmatrix}$$ 은 $$\mathbf{A}$$ 를 그 고유값 $$\lambda_{1},\lambda_{2},\cdots,\lambda_{N}$$ 으로 구성된 대각행렬 $$\Lambda$$ 로 대각화시킴

        $$\begin{aligned}
        \mathbf{P}^{-1}\mathbf{A}\mathbf{P}
        &= \Lambda \\
        &= \mathrm{diag}(\lambda_1,\lambda_2,\cdots,\lambda_n)
        \end{aligned}$$