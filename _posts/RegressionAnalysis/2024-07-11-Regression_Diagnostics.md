---
order: 4
title: Regression Diagnostics
date: 2024-07-11
categories: [Statistical Techs, Regression Analysis]
tags: [Statistics, Regression]
math: true
description: >-
  Based on the lecture "Statistical Models and Application (2024-1)" by Prof. Yeo Jin Chung, Dept. of Data Science, The Grad. School, Kookmin Univ.
image:
  path: /_post_refer_img/RegressionAnalysis/Thumbnail.jpg
---

## Gauss-Markov Assumptions
-----

- `A.1` **ë°˜ì‘ë³€ìˆ˜ì™€ ì„¤ëª…ë³€ìˆ˜ì˜ ì„ í˜•ì„± ê°€ì •**

    >The Linear Model is Correctly Specified.

- `A.2` **ê´€ì¸¡ì¹˜ ê°„ ì˜¤ì°¨í•­ì˜ ìê¸°ìƒê´€ ì—†ìŒ(No Autocorrelation) ê°€ì •**

    >$\mathbb{Cov}\left[\varepsilon_i, \varepsilon_j\right]=0$ If $i \ne j$

- `A.3` **ì˜¤ì°¨í•­ì˜ ì •ê·œì„± ê°€ì •(Normality Assumption)**

    >$\varepsilon_i$ is Normally Distributed For Every $i=1,\cdots,n$

    - `A.3.1` **ì˜¤ì°¨í•­ì˜ ê¸°ëŒ€ê°’ì— ê´€í•œ ê°€ì •**

        >$\mathbb{E}\left[\varepsilon_i\right]=0$ For Every $i=1,\cdots,n$

    - `A.3.2` **ì˜¤ì°¨í•­ì˜ ë“±ë¶„ì‚°ì„±(Homoskedasticity) ê°€ì •**

        >$\mathbb{Var}\left[\varepsilon_i\right]=\sigma^2$ For Every $i=1,\cdots,n$

- `A.4` **ì„¤ëª…ë³€ìˆ˜ì˜ ë¹„í™•ë¥ ì„± ê°€ì • í˜¹ì€ ì„¤ëª…ë³€ìˆ˜ ê°„ í†µê³„ì  ë…ë¦½ì„± ê°€ì •**

    >$X_i$ is Non-Random For Every $i=1,\cdots,n$

## $\hat{\beta}_{1}$ subject to Gauss-Markov Assumptions
-----

- **ê°€ì¤‘ì¹˜ $\beta_1$ ì˜ ìµœì†ŒììŠ¹ì¶”ì •ëŸ‰ $\hat{\beta}_1$ ì€ $Y$ ì˜ ì„ í˜• ì¡°í•©**

    $$\begin{aligned}
    \hat{\beta}_1
    &= \frac{\mathbb{Cov}(X,Y)}{\mathbb{Var}(X)}\\
    &= \frac{\sum_{i=1}^{n}(x_i-\overline{x})(y_i-\overline{y})}{\sum_{i=1}^{n}{(x_i-\overline{x})^2}}\\
    \\
    \sum_{i=1}^{n}{(x_i-\overline{x})(y_i-\overline{y})}
    &= \sum_{i=1}^{n}{(x_i-\overline{x}) \cdot y_i} - \sum_{i=1}^{n}{(x_i-\overline{x}) \cdot \overline{y}}\\
    &= \sum_{i=1}^{n}{(x_i-\overline{x}) \cdot y_i} - \overline{y} \cdot \sum_{i=1}^{n}{(x_i-\overline{x})}\\
    &= \sum_{i=1}^{n}{(x_i-\overline{x}) \cdot y_i}\\
    \\
    \therefore \hat{\beta}_{1}
    &= \frac{\sum_{i=1}^{n}{(x_i-\overline{x}) \cdot y_i}}{\sum_{i=1}^{n}{(x_i-\overline{x})^2}}\\
    &= w_1 \cdot y_1 + w_2 \cdot y_2 + \cdots + w_n \cdot y_n\\
    w_{i}
    &=\frac{(x_i-\overline{x})}{\sum_{j=1}^{n}{(x_j-\overline{x})^2}}
    \end{aligned}$$

- **$\hat{\beta}_{1}$ ì— ëŒ€í•˜ì—¬ ë‹¤ìŒì´ ì„±ë¦½í•¨**

    - $\sum_{i=1}^{n}{w_i}=\displaystyle\frac{\sum_{i=1}^{n}{(x_i-\overline{x})}}{\sum_{j=1}^{n}{(x_j-\overline{x})^2}}=0$

    - $\sum_{i=1}^{n}{w_i \cdot x_i}=1$

        $$\begin{aligned}
        \sum_{i=1}^{n}{w_i \cdot x_i}
        &= \sum_{i=1}^{n}{\frac{(x_i-\overline{x})}{\sum_{j=1}^{n}{(x_j-\overline{x})^2}} \cdot x_i}\\
        &= \frac{\sum_{i=1}^{n}{(x_i-\overline{x})\cdot x_i}}{\sum_{j=1}^{n}{(x_j-\overline{x})^2}}\\
        \\
        \sum_{i=1}^{n}{(x_i-\overline{x})\cdot x_i}
        &= \sum_{i=1}^{n}{(x_{i}^{2}-\overline{x}\cdot x_{i})}\\
        &= \sum_{i=1}^{n}{x_{i}^{2}} - \overline{x}\cdot\sum_{i=1}^{n}{x_{i}}\\
        &= \sum_{i=1}^{n}{x_{i}^{2}} - n\cdot \overline{x}^{2}\\
        &= \sum_{i=1}^{n}{(x_{i}-\overline{x})^{2}}\\
        \\
        \therefore \sum_{i=1}^{n}{w_i \cdot x_i}
        &= \frac{\sum_{i=1}^{n}{(x_i-\overline{x})\cdot x_i}}{\sum_{j=1}^{n}{(x_j-\overline{x})^2}}\\
        &= \frac{\sum_{i=1}^{n}{(x_{i}-\overline{x})^{2}}}{\sum_{j=1}^{n}{(x_j-\overline{x})^2}}\\
        &= 1
        \end{aligned}$$

    - $\sum_{i=1}^{n}{w_i^2}=\displaystyle\frac{1}{\sum_{i=1}^{n}(x_i-\overline{x})^2}$

        $$\begin{aligned}
        \sum_{i=1}^{n}{w_i^2}
        &= \sum_{i=1}^{n}{\left(\frac{(x_{i}-\overline{x})}{\sum_{j=1}^{n}(x_{j}-\overline{x})^2}\right)^{2}}\\
        &= \frac{\sum_{i=1}^{n}{(x_{i}-\overline{x})^{2}}}{\left(\sum_{j=1}^{n}{(x_{j}-\overline{x})^2}\right)^{2}}\\
        &= \frac{\sum_{i=1}^{n}{(x_{i}-\overline{x})^{2}}}{\sum_{j=1}^{n}{(x_{j}-\overline{x})^2} \cdot \sum_{j=1}^{n}{(x_{j}-\overline{x})^2}}\\
        &= \frac{1}{\sum_{j=1}^{n}{(x_{j}-\overline{x})^2}}
        \end{aligned}$$

- **ë”°ë¼ì„œ ìµœì†ŒììŠ¹ì¶”ì •ëŸ‰ $\hat{\beta_1}$ ê³¼ ê·¸ ëª¨ìˆ˜ $\beta_1$ ì‚¬ì´ì—ëŠ” ë‹¤ìŒì´ ì„±ë¦½í•¨**

    $$\begin{aligned}
    \hat{\beta}_{1}
    &= \sum_{i=1}^{n}{w_i \cdot y_i}\\
    &= \sum_{i=1}^{n}{w_i \cdot (\beta_0+\beta_1 \cdot x_i+\varepsilon_i)}\\
    &= \beta_1 + \sum_{i=1}^{n}{w_i \cdot \varepsilon_i}
    \end{aligned}$$

## Non-linearity Problem
-----

- **$Y$ ì™€ $X$ ê°„ ì„ í˜• ê´€ê³„ê°€ ì„±ë¦½í•˜ì§€ ì•ŠëŠ”ë‹¤ê³  í•˜ì**

    $$\begin{aligned}
    y_i= f(x_i)+\varepsilon_i
    \end{aligned}$$

- **ìµœì†ŒììŠ¹ì¶”ì •ëŸ‰ $$\hat{\beta}_{1}$$ ì„ ë‹¤ìŒê³¼ ê°™ì´ ì´í•´í•  ìˆ˜ ìˆìŒ**

    $$\begin{aligned}
    \hat{\beta}_{1}
    &= \frac{\sum_{i=1}^{n}{(x_i - \overline{x})(y_i - \overline{y})}}{\sum_{i=1}^{n}{(x_i - \overline{x})^2}}\\
    &= \frac{\sum_{i=1}^{n}{(x_i - \overline{x})(f(x_i)+\varepsilon_i - \overline{f(x)} - \overline{\varepsilon})}}{\sum_{i=1}^{n}{(x_i - \overline{x})^2}}\\
    &= \frac{\sum_{i=1}^{n}{(x_i-\overline{x})(f(x_i)-\overline{f(x_i)})}}{\sum_{i=1}^{n}{(x_i - \overline{x})^2}} + \frac{\sum_{i=1}^{n}{(x_i-\overline{x})(\varepsilon_i-\overline{\varepsilon})}}{\sum_{i=1}^{n}{(x_i - \overline{x})^2}}\\
    &= \frac{\sum_{i=1}^{n}{(x_i-\overline{x})(f(x_i)-\overline{f(x_i)})}}{\sum_{i=1}^{n}{(x_i - \overline{x})^2}} + \frac{\sum_{i=1}^{n}{(x_i-\overline{x}) \cdot \varepsilon_i}}{\sum_{i=1}^{n}{(x_i - \overline{x})^2}} - \frac{\sum_{i=1}^{n}{(x_i-\overline{x}) \cdot \overline{\varepsilon}}}{\sum_{i=1}^{n}{(x_i - \overline{x})^2}}\\
    &= \frac{\sum_{i=1}^{n}{(x_i-\overline{x})(f(x_i)-\overline{f(x_i)})}}{\sum_{i=1}^{n}{(x_i - \overline{x})^2}} + \sum_{i=1}^{n}{w_i \cdot \varepsilon_i}
    \end{aligned}$$

- **ë”°ë¼ì„œ $$\hat{\beta}_{1}$$ ì€ $$\beta_{1}$$ ì˜ ë¶ˆí¸ì¶”ì •ëŸ‰ì´ ë  ìˆ˜ ì—†ìŒ**

    $$\begin{aligned}
    \mathbb{E}\left[\hat{\beta}_{1}\right]
    &= \mathbb{E}\left[\frac{\sum_{i=1}^{n}{(x_i-\overline{x})(f(x_i)-\overline{f(x_i)})}}{\sum_{i=1}^{n}{(x_i - \overline{x})^2}} + \sum_{i=1}^{n}{w_i \cdot \varepsilon_i}\right]\\
    &= \mathbb{E}\left[\frac{\sum_{i=1}^{n}{(x_i-\overline{x})(f(x_i)-\overline{f(x_i)})}}{\sum_{i=1}^{n}{(x_i - \overline{x})^2}}\right] + \mathbb{E}\left[\sum_{i=1}^{n}{w_i \cdot \varepsilon_i}\right]\\
    &= \mathbb{E}\left[\frac{\sum_{i=1}^{n}{(x_i-\overline{x})(f(x_i)-\overline{f(x_i)})}}{\sum_{i=1}^{n}{(x_i - \overline{x})^2}}\right] + \sum_{i=1}^{n}{\mathbb{E}\left[w_i\right] \cdot \mathbb{E}\left[\varepsilon_i\right]} \quad \text{s.t.} \quad X \perp \varepsilon\\
    &= \mathbb{E}\left[\frac{\sum_{i=1}^{n}{(x_i-\overline{x})(f(x_i)-\overline{f(x_i)})}}{\sum_{i=1}^{n}{(x_i - \overline{x})^2}}\right] + \sum_{i=1}^{n}{\mathbb{E}\left[w_i\right] \cdot 0} \quad (\because \varepsilon \sim N(0,\sigma^2))\\
    &= \mathbb{E}\left[\frac{\sum_{i=1}^{n}{(x_i-\overline{x})(f(x_i)-\overline{f(x_i)})}}{\sum_{i=1}^{n}{(x_i - \overline{x})^2}}\right]\\
    &\ne \beta_1
    \end{aligned}$$

## Auto-correlation Problem
-----

## Hetero-scedasticity Problem
-----

## Multi-col-linearity Problem
-----

### ğŸ’¡ ê³ ì „ì  ê°€ì • `A.2` ì˜ ìˆ˜í•™ì  ì´í•´

- **ì„ í˜• ê²°í•©(Linear Combination)** : ì°¨ì›ì´ $n$ ìœ¼ë¡œ ë™ì¼í•œ ì„ì˜ì˜ ë²¡í„°ì™€ ìŠ¤ì¹¼ë¼ì— ëŒ€í•˜ì—¬, ê° í•­ì— ìŠ¤ì¹¼ë¼ë¥¼ ê³±í•˜ê±°ë‚˜ ìƒí˜¸ ë”í•¨ìœ¼ë¡œì¨ ì¼ë ¨ì˜ í•­ìœ¼ë¡œ êµ¬ì„±í•˜ëŠ” ì‘ì—…

    $$
    \alpha_{1}\overrightarrow{a_{1}} + \alpha_{2}\overrightarrow{a_{2}} + \cdots + \alpha_{p}\overrightarrow{a_{p}}
    $$

- **ì„ í˜• ì¢…ì†(Linearly Dependent)** : ì–´ë–¤ ë²¡í„°ê°€ ë‹¤ë¥¸ ë²¡í„°ë“¤ì˜ ì„ í˜• ê²°í•©ìœ¼ë¡œ í‘œí˜„ ê°€ëŠ¥í•œ ê²½ìš°
        
    $$
    \alpha_{1}\overrightarrow{a_{1}} + \alpha_{2}\overrightarrow{a_{2}} + \cdots + \alpha_{p}\overrightarrow{a_{p}} = 0, \alpha^{\forall} \ne 0
    $$

- **ì„ í˜• ë…ë¦½(Linearly Independent)** : ì–´ë–¤ ë²¡í„°ê°€ ë‹¤ë¥¸ ë²¡í„°ë“¤ì˜ ì„ í˜• ê²°í•©ìœ¼ë¡œ í‘œí˜„ë  ìˆ˜ ì—†ëŠ” ê²½ìš°

    $$
    \alpha_{1}\overrightarrow{a_{1}} + \alpha_{2}\overrightarrow{a_{2}} + \cdots + \alpha_{p}\overrightarrow{a_{p}} = 0 \Rightarrow \alpha^{\forall} = 0
    $$

- **ê³ ì „ì  ê°€ì • `A.2` ì˜ ìˆ˜í•™ì  ì´í•´**

    $$\begin{aligned}
    X_i
    &\ne \lambda X_j \; (i \ne j, \lambda \ne 0)
    \end{aligned}$$

    - ì„¤ëª…ë³€ìˆ˜ë²¡í„° $\overrightarrow{X_i}$ ê°€ $\overrightarrow{X_j}(i \ne j)$ ì— ì„ í˜• ì¢…ì†ì´ì–´ì„œëŠ” ì•ˆ ë¨

### Multicollinearity Problem

#### ê°€ì¤‘ì¹˜ ìµœì†ŒììŠ¹ì¶”ì •ëŸ‰ì˜ ì´í•´

- **ê°€ì¤‘ì¹˜ $\beta_i$**

    $$\begin{aligned}
    \beta_{i}
    &= \displaystyle\frac{\partial Y}{\partial X_i}
    \end{aligned}$$

    - $i$ ë²ˆì§¸ ì„¤ëª…ë³€ìˆ˜ $X_i$ ì˜ ê°€ì¤‘ì¹˜ $\beta_i$ ëŠ” $X_i$ ê°€ ë°˜ì‘ë³€ìˆ˜ $Y$ ì— ìˆœìˆ˜í•˜ê²Œ ë¯¸ì¹˜ëŠ” ì˜í–¥ë ¥ìœ¼ë¡œì„œ, $X_{j^{\forall}} (j^{\forall} \ne i)$ ì˜ ê°’ì´ ì¼ì •í•  ë•Œ $X_i$ ë³€ë™ ì‹œ $Y$ ì˜ ë³€ë™ ì–‘ìƒì„ ë‚˜íƒ€ëƒ„

- **ê°€ì¤‘ì¹˜ $\beta_i$ ì˜ ìµœì†ŒììŠ¹ì¶”ì •ëŸ‰**

    $$\begin{aligned}
    \hat{\beta}_{i}
    &= \displaystyle\frac{\sigma_{X_i,Y}}{\sigma_{X_i}^2}
    \end{aligned}$$

    - ê³ ì „ì  ê°€ì • í•˜ $X_i$ ì˜ ê°€ì¤‘ì¹˜ $\beta_i$ ì˜ ìµœì†ŒììŠ¹ì¶”ì •ëŸ‰ì€ $X_i$ ì˜ ë¶„ì‚° ëŒ€ë¹„ $X_i$ ì™€ $Y$ ì˜ ê³µë¶„ì‚°ì˜ ë¹„ìœ¨ì„

#### ğŸ’¡ ë‹¤ì¤‘ê³µì„ ì„±

- **ì™„ì „ê³µì„ ì„±(Perfect Multi-Col-Linearity)** : ê³ ì „ì  ê°€ì • `A.2` ê°€ ìœ„ë°°ë˜ëŠ” ìƒí™©

    $$\begin{aligned}
    X_i
    &= \lambda X_j \; (i \ne j, \lambda \ne 0)
    \end{aligned}$$

    - $X_j(j^{\forall} \ne i)$ ì˜ ë³€ë™ì´ $X_i$ ì˜ ë³€ë™ì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤ê³  í•˜ì
    - $X_{i}$ ì™€ $Y$ ì˜ ê³µë¶„ì‚° $\sigma_{X_{i},Y}$ ëŠ” ì œ3ì˜ ìš”ì¸ì¸ $X_{j}$ ì˜ ë³€ë™ì— ì˜í–¥ì„ ë°›ì€ $X_{i}$ ì˜ ë³€ë™ì— ì˜í•œ $Y$ ì˜ ë³€ë™ì„ ë‚´í¬í•¨
    - ì¦‰, $X_i$ ì™€ $X_j$ ê°€ ì„ í˜• ë…ë¦½ì´ ì•„ë‹ ê²½ìš°, $X_i$ ì˜ ê°€ì¤‘ì¹˜ $\hat{\beta}_i$ ê°€ $Y$ ì— ëŒ€í•œ $X_i$ ì˜ ìˆœìˆ˜í•œ ì„¤ëª…ë ¥ì´ë¼ê³  ë³¼ ìˆ˜ ì—†ìŒ

- **ë‹¤ì¤‘ê³µì„ ì„±(Multicollinearity)** : ì„¤ëª…ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ê°€ ì„ í˜•ê´€ê³„ë¼ê³  í™•ì •í•  ìˆ˜ëŠ” ì—†ìœ¼ë‚˜, ì„ í˜•ê´€ê³„ì— ê°€ê¹Œìš´ í˜•íƒœë¥¼ ë³´ì´ëŠ” ê²½ìš°

    $$
    \rho (X_i, X_j) \approx \pm 1
    $$

#### ğŸ’¡ ë¶„ì‚°íŒ½ì°½ê³„ìˆ˜

- **$Y$ ë³€ë™ì˜ ì´í•´**

    - **ì´ ë³€ë™(Sum of Squared Total; $SST$)**

        $$\begin{aligned}
        SST
        &= \displaystyle\sum_{i=1}^{n}(Y_i-\overline{Y})^2\\
        &= SSR+SSE
        \end{aligned}$$

    - **íšŒê·€ë³€ë™(Sum of Squared Regression; $SSR$)** : $X_{\forall}$ ì˜ ì„ í˜•ì¡°í•©ìœ¼ë¡œ ì„¤ëª… ê°€ëŠ¥í•œ ë³€ë™

        $$
        SSR=\displaystyle\sum_{i=1}^{n}(\hat{Y_i} -\overline{Y})^2
        $$

    - **ì˜¤ì°¨ë³€ë™(Sum of Squared Regression; $SSE$)** : $X_i$ ì˜ ì„ í˜•ì¡°í•©ìœ¼ë¡œ ì„¤ëª…í•  ìˆ˜ ì—†ëŠ” ë³€ë™

        $$
        SSE=\displaystyle\sum_{i=1}^{n}e_i^2
        $$

- **ê²°ì •ê³„ìˆ˜(Coefficient of Determination; $R^2$)** : ì´ë³€ë™ ëŒ€ë¹„ íšŒê·€ë³€ë™

    $$\begin{aligned}
    R^2
    &= \displaystyle\frac{SSR}{SST} \\
    &= \displaystyle\frac{SSR}{SSR + SSE} \\
    &= 1 - \displaystyle\frac{SSE}{SST}
    \end{aligned}$$

- **ë¶„ì‚°íŒ½ì°½ê³„ìˆ˜(Variance Inflation Factor; $VIF$)** : ë³€ë™ì„ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ì¤‘ê³µì„ ì„±ì„ ê³„ì‚°í•˜ëŠ” ì§€í‘œ

    $$\begin{aligned}
    VIF_i
    &= \displaystyle\frac{1}{1-R_{i}^{2}}
    \end{aligned}$$