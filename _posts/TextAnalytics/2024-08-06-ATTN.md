---
order: 9
title: Attention Mechanism
date: 2024-08-06
categories: [Data Mining, Text Analytics]
tags: [Data Mining, NLP, Text Mining, Machine Translation, Attention Mechanism]
math: true
description: >-
    Based on the lecture “Text Analytics (2024-1)” by Prof. Je Hyuk Lee, Dept. of Data Science, The Grad. School, Kookmin Univ.
image:
    path: /_post_refer_img/TextAnalytics/Thumbnail.png
---

## Attention Mechanism
-----

- **어텐션 메커니즘(Attention Mechanism)** : 개별 질의에 특화된 정답 후보군별 중요도를 도출하는 메커니즘
    - **교차 어텐션(Cross Attention)** : 입력값과 반환할 값이 다른 경우
    - **셀프 어텐션(Self Attention)** : 입력값과 반환할 값이 같은 경우

- **Framework**

    ![04](/_post_refer_img/TextAnalytics/09-04.jpg){: width="100%"}

    $$\begin{aligned}
    \text{ATTN}\left(\mathcal{Q},\mathcal{K},\mathcal{V}\right)
    = \underbrace{\text{Softmax}\left[f(\mathcal{Q},\mathcal{K})\right]}_{\text{Attention Weight}} \cdot \mathcal{V}
    \end{aligned}$$

    - $\mathcal{Q}$ : 입력값에 대하여 정보를 얻고자 하는 기준점으로서 **질의(Query)**
    - $\mathcal{K}$ : 질의와 매칭하여 관련성을 평가할 기준으로서 **키(Key)**
    - $\mathcal{V}$ : 관련성을 기반으로 반환할 값으로서 **선택지(Value)**

- **`Example` Recommendation** (Latent Factor Model)

    > 사용자 개인화 영화 추천 서비스를 기획하고 있다. 이때 사용자의 과거 시청 이력 외에, 사용자 프로파일 정보와 아이템 프로파일 정보를 추가 활용하고자 한다. 어떻게 하면 개별 사용자에 대한 영화별 중요도를 산출할 수 있을까?

    | 표현 | 정의 | ATTN |
    |---|---|---|
    | $$\overrightarrow{\mathbf{u}}_{i} \in \mathbf{U}_{M \times D}$$ | 사용자 임베딩 벡터 | 입력값 |
    | $$\overrightarrow{\mathbf{q}}_{i} \in \mathbf{Q}_{M \times D}$$ | 사용자 프로파일 벡터 | 입력값에 대하여 정보를 얻고자 하는 기준점 |
    | $$\overrightarrow{\mathbf{v}}_{j} \in \mathbf{V}_{N \times D}$$ | 아이템 임베딩 벡터 | 반환할 값 |
    | $$\overrightarrow{\mathbf{k}}_{j} \in \mathbf{K}_{N \times D}$$ | 아이템 프로파일 벡터 | 반환할 값에 대하여 질의와 매칭하여 관련성을 평가할 기준 |

    $$\begin{aligned}
    \overrightarrow{\mathbf{w}}_{i}
    = \text{ATTN}\left(\overrightarrow{\mathbf{q}}_{i}, \mathbf{K}, \mathbf{V}\right)
    \in \mathbb{R}^{N}
    \end{aligned}$$

- **Attention Score Function**

    | Name | Function | Defined by |
    |---|---|---|
    | Dot Product | $$f(\overrightarrow{\mathbf{q}}, \mathbf{K}) = \overrightarrow{\mathbf{q}} \cdot \mathbf{K}$$ | Luong et al. (2015) |
    | Learnable Weighted Attention | $$f(\overrightarrow{\mathbf{q}}, \mathbf{K}) = \overrightarrow{\mathbf{q}}^{T} \cdot \mathbf{W} \cdot \mathbf{K}$$ | Luong et al. (2015) |
    | Additive Attention | $$f(\overrightarrow{\mathbf{q}}, \mathbf{K}) = \mathbf{W}^{T}_{A} \cdot \text{tanh}\left[\mathbf{W}_{B} \cdot (\overrightarrow{\mathbf{q}} \oplus \mathbf{K})\right]$$ | Bahdanau et al. (2015) |
    | Concatenation | $$f(\overrightarrow{\mathbf{q}}, \mathbf{K}) = \mathbf{W}^{T}_{A} \cdot \text{tanh}\left[\mathbf{W}_{B} \cdot \overrightarrow{\mathbf{q}} + \mathbf{W}_{C} \cdot \mathbf{K}\right]$$ | Bahdanau et al. (2015) |
    | Scaled Dot Product | $$f(\overrightarrow{\mathbf{q}}, \mathbf{K}) = \displaystyle\frac{\overrightarrow{\mathbf{q}}^{T} \cdot \mathbf{K}}{\sqrt{n}}$$ | Vaswani et al. (2017) |

## Multi-Head Attention
-----

- **가중 어텐션(Weighted Attention)** : 표현력을 강화하기 위하여 학습 가능한 가중치 행렬 $\mathcal{W}$ 을 활용하여 $\mathcal{Q}, \mathcal{K}, \mathcal{V}$ 를 선형 변환하고, 이를 기반으로 유사도를 계산하여 입력 간 상호작용 정보를 동적으로 학습하는 기법

    $$\begin{aligned}
    \mathcal{Q}
    &= \mathbf{Q}_{M \times D_{Q}} \cdot \mathcal{W}^{(Q)}_{D_{Q} \times D} &\in \mathbb{R}^{M \times D}\\
    \mathcal{K}
    &= \mathbf{K}_{N \times D_{K}} \cdot \mathcal{W}^{(K)}_{D_{K} \times D} &\in \mathbb{R}^{N \times D}\\
    \mathcal{V}
    &= \mathbf{V}_{N \times D_{V}} \cdot \mathcal{W}^{(V)}_{D_{V} \times D} &\in \mathbb{R}^{N \times D}
    \end{aligned}$$

    - $\mathcal{Q}, \mathcal{K}, \mathcal{V}$ 간 **차원 보정이** 필요한 경우
        - $\mathcal{Q}$ 와 $\mathcal{K}$ 가 서로 다른 특징 차원을 가지고 있는 경우
        - $\mathcal{Q}$ 와 $\mathcal{K}$ 가 서로 같은 특징 차원을 공유하고 있으나 상황에 따라 유사도가 중요도에 작용하는 방향(선호/비선호)이 다를 경우
    - 상호작용 정보를 포착함에 있어서 유사도 이상의 **복잡한 관계를 조명하고자 하는 경우**
        - 셀프 어텐션에서, 입력값과 반환할 값은 동일하나 역할($\mathcal{Q}, \mathcal{K}, \mathcal{V}$)에 따라 서로 다른 정보 처리를 수행해야 하는 경우
        - 멀티 헤드 어텐션에서, 헤드마다 상호작용 정보를 다각도로 포착하고자 하는 경우

- **멀티 헤드 어텐션(Multi-Head Attention)** : 입력 데이터를 여러 독립적인 어텐션 메커니즘(헤드)으로 병렬 처리하여, 데이터 간 관계와 패턴을 다각도로 학습하는 기법

    ![03](/_post_refer_img/TextAnalytics/09-03.png){: width="100%"}

    - 하나의 헤드는 독립적인 가중 어텐션으로 이루어짐

        $$\begin{aligned}
        \text{HEAD}^{(h)}
        &= \text{ATTN}^{(h)}\left(\mathbf{Q} \cdot \mathcal{W}_{Q}^{(h)}, \mathbf{K} \cdot \mathcal{W}_{K}^{(h)}, \mathbf{V} \cdot \mathcal{W}_{V}^{(h)}\right)
        \end{aligned}$$

    - 멀티 헤드 어텐션의 결과값은 헤드별 결과값의 벡터 결합을 선형 변환한 값임

        $$\begin{aligned}
        \text{Multi-Head}\left(\mathbf{Q}, \mathbf{K}, \mathbf{V}\right)
        &= \left[\cdots \oplus \text{HEAD}^{(h)} \oplus \cdots \right] \cdot \mathcal{W}_{\mathcal{O}}
        \end{aligned}$$

## Application to SEQ2SEQ
-----

- **SEQ2SEQ 적용 목적** : RNN 계열 레이어의 초기 순번 정보 유실 문제 및 기울기 소실 문제를 보완하기 위함으로서, 문맥 벡터를 최종 은닉 상태로 획일화하여 사용하지 않고, 인코더 각 순번 은닉 상태와 디코더 현재 시점 간 관련성을 고려하여, 디코더의 각 시점에 특화된 문맥 벡터를 생성함

- **종류**
    - **루옹 어텐션(Luong Attention)** : 디코더의 **현재 시점 은닉 상태를** 활용하여 문맥 벡터를 생성하고, 이를 **현재 시점 은닉 상태에** 적용하는 어텐션 기법
    - **바다나우 어텐션(Bahdanau Attention)** : 디코더의 **이전 시점 은닉 상태를** 활용하여 문맥 벡터를 생성하고, 이를 **현재 시점 입력값에** 적용하는 어텐션 기법

### Luong Attention

![01](/_post_refer_img/TextAnalytics/09-01.png){: width="100%"}

- Attention Mechanism

    $$
    \mathbf{W}^{(t)} = \text{Softmax}\left[\overrightarrow{\mathbf{q}}_{t} \cdot \mathbf{K}\right] \cdot \mathbf{V}
    $$

    - $$\overrightarrow{\mathbf{q}}_{t} = \eta_{t}$$ : 디코더의 $t$ 시점 은닉 상태
    - $$\mathbf{K} = \mathbf{V} = \mathbf{H}$$ : 인코더의 각 순번 은닉 상태 행렬

- 디코더의 $t$ 시점 특화 문맥 벡터(Context Vector) 도출

    $$\begin{aligned}
    \overrightarrow{\mathbf{z}}_{t}
    = \sum_{i}{\mathbf{W}^{(t)}_{i}}
    = \overrightarrow{\mathbf{w}}^{(t)}_{1} + \overrightarrow{\mathbf{w}}^{(t)}_{2} + \cdots + \overrightarrow{\mathbf{w}}^{(t)}_{T}
    \end{aligned}$$

- $t$ 시점 문맥 벡터와 $t$ 시점 은닉 상태 정보 종합

    $$
    \overrightarrow{\mathbf{s}}_{t}
    = \text{F}_{\text{tanh}}\left[\overrightarrow{\mathbf{z}}_{t} \oplus \eta_{t}\right]
    $$

### Bahdanau Attention

![02](/_post_refer_img/TextAnalytics/09-02.png){: width="100%"}

- Attention Mechanism

    $$
    \mathbf{W}^{(t)} = \text{Softmax}\left[\overrightarrow{\mathbf{q}}_{t} \cdot \mathbf{K}\right] \cdot \mathbf{V}
    $$

    - $$\overrightarrow{\mathbf{q}}_{t} = \eta_{t-1}$$ : 디코더의 $t-1$ 시점 은닉 상태
    - $$\mathbf{K} = \mathbf{V} = \mathbf{H}$$ : 인코더의 각 순번 은닉 상태 행렬

- 디코더의 $t$ 시점 특화 문맥 벡터(Context Vector) 도출

    $$\begin{aligned}
    \overrightarrow{\mathbf{z}}_{t}
    = \sum_{i}{\mathbf{W}^{(t)}_{i}}
    = \overrightarrow{\mathbf{w}}^{(t)}_{1} + \overrightarrow{\mathbf{w}}^{(t)}_{2} + \cdots + \overrightarrow{\mathbf{w}}^{(t)}_{T}
    \end{aligned}$$

- $t$ 시점 문맥 벡터와 $t$ 시점 입력 벡터 정보 종합

    $$
    \overrightarrow{\mathbf{s}}_{t}
    = \overrightarrow{\mathbf{z}}_{t} \oplus \hat{\mathbf{y}}_{t-1}
    $$

-----

### 이미지 출처

- https://www.linkedin.com/pulse/what-self-attention-impact-large-language-models-llm-nikhil-goel-srpbc
- https://newsletter.theaiedge.io/p/the-multi-head-attention-mechanism
- https://wikidocs.net/22893
- https://wikidocs.net/73161