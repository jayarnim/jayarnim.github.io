---
order: 11
title: BERT
date: 2024-08-08
categories: [Data Mining, Text Analytics]
tags: [Data Mining, NLP, Text Mining, Language Model, Attention Mechanism]
math: true
description: >-
    Based on the lecture “Text Analytics (2024-1)” by Prof. Je Hyuk Lee, Dept. of Data Science, The Grad. School, Kookmin Univ.
image:
    path: /_post_refer_img/TextAnalytics/Thumbnail.png
---

## BERT
-----

- **버트(`B`idirectional `E`ncoder `R`epresentations from `T`ransformers; BERT)** : 트랜스포머의 인코더 아키텍처를 기반으로 하여 위키피디아(25억 단어), BooksCorpus(8억 단어) 등 레이블 없는 텍스트 데이터로 사전 훈련된 대형 언어 모형(Pre-trained `L`arge `L`anguage `M`odel)

    ![01](/_post_refer_img/TextAnalytics/11-01.png){: width="100%"}

- **Version Comparison**

    | | BASE | LARGE |
    |---|---|---|
    | Embedding Dimension | 768	| 1024 |
    | Encoder Layers | 12 | 24 |
    | Attention Heads | 12 | 16 |
    | Total Parameter | 110M | 340M |

## Word Representation
-----

![02](/_post_refer_img/TextAnalytics/11-02.jpg){: width="100%"}

$$\begin{aligned}
\overrightarrow{\mathbf{x}}_{i}
&= \overrightarrow{\mathbf{x}}^{(i)}_{\text{TOKEN}} + \overrightarrow{\mathbf{x}}^{(i)}_{\text{POS}} + \overrightarrow{\mathbf{x}}^{(i)}_{\text{SEG}}
\end{aligned}$$

- **Unique(or Max) Number of Vector**

    | VECTOR | NUM |
    |---|---|
    | Tokens | 30,522 |
    | Max Seq | 512 |
    | Max Sentence | 2 |

- **Each observation can contain up to two sequences**

    > **`CLS` `my` `dog` `is` `cute` `SEP` `he` `likes` `play` `##ing` `SEP`**

    - `CLS` : 입력 정보를 종합하여 대표하는 벡터
    - `SEP` : 문장 구분자

- **Word-Piece Embedding** : 토큰 임베딩 벡터를 생성함
    - **OOV(`O`ut `o`f `V`ocabulary)** : 단어 사전에 존재하는 단어들로 쪼개어 임베딩 벡터를 생성하고, 이때 Sub-Word 는 앞에 `##` 을 표기하여 온전한 단어가 아님을 나타냄
    - Here is the sentence I want **embeddings** for. <br> → [`here`, `is`, `the`, `sentence`, `i`, `want`, `em`, `##bed`, `##ding`, `##s`, `for`, `.`]

- **Position Embedding** : 토큰의 위치를 표현하는 벡터를 생성함
    - 사전에 정의된 함수에 따라 위치를 표현하는 포지셔널 인코딩(Positional Encoding)과 달리, 학습 가능한 가중치를 사용하여 최적의 위치 정보를 학습함

- **Segment Embedding** : 토큰이 소속된 시퀀스 범주를 구분하는 벡터를 생성함

    | Token | Segment Embedding |
    |---|---|
    | `CLS` | First Sequence Vector |
    | Tokens @ First Sequence | First Sequence Vector |
    | `SEP` | First Sequence Vector |
    | Tokens @ Second Sequence | Second Sequence Vector |
    | `SEP` | Second Sequence Vector |

## Single Layer
-----

![05](/_post_refer_img/TextAnalytics/11-05.png){: width="100%"}

$$\begin{aligned}
\mathbf{X}^{(0)}
&=\text{Embeddings}\left(\text{Tokens}\right)\\
\mathbf{H}^{(k)}
&=\text{Layer-Norm}\Big[\text{Multi-Head}\left(\mathbf{X}^{(k)}\right) + \mathbf{X}^{(k)}\Big]\\
\mathbf{Y}^{(k)}
&=\text{Layer-Norm}\Big[\text{FFN}\left(\mathbf{H}^{(k)}\right) + \mathbf{H}^{(k)}\Big]
\end{aligned}$$

- $\mathbf{X}$ is Input Data of Single Layer, $\mathbf{Y}$ is Output Data of Single Layer
    - $$\mathbf{X}^{(k+1)}=\mathbf{Y}^{(k)}$$ : Input Data of $k+1$ Encoder Layer is Output Data of $k$
    - Input Data of Initial Layer $$\mathbf{X}^{(0)}$$ is Sum of Token Embedding & Position Embedding & Segment Embedding Vector
    - Output Data of Final Layer $$\mathbf{Z}=\mathbf{Y}^{(K)}$$ is Output of BERT Module

- $$\text{Multi-Head}\left(\mathbf{X}^{(k)}\right)$$ : Multi-Head Self Attention

- $$\text{FFN}\left(\mathbf{H}^{(k)}\right)$$ : `F`eed-`F`orward `N`etworks

    $$\begin{aligned}
    \text{FFN}\left(\mathbf{H}^{(k)}\right)
    &=\mathbf{W}^{(k)}_{2} \cdot \left(\text{ReLU}\left[\mathbf{W}^{(k)}_{1} \cdot \mathbf{H}^{(k)} + \overrightarrow{\mathbf{b}}^{(k)}_{1}\right]\right) + \overrightarrow{\mathbf{b}}^{(k)}_{2}
    \end{aligned}$$

    - $\mathbf{W}^{(k)}_{1} \in \mathbb{R}^{M \times 4d}$ : **Dimension Expansion** to four times the Dimension of the Embedding Vector
    - $\mathbf{W}^{(k)}_{2} \in \mathbb{R}^{M \times d}$ : **Dimension Reduction** to Embedding Vector Dimension

## Pre-Training for Transfer Learning
-----

![03](/_post_refer_img/TextAnalytics/11-03.jpg){: width="100%"}

- **MLM(`M`asked `L`anguage `M`odel)** : 입력 문장에서 일부는 `MASK` 로 가리고, 일부는 다른 단어로 무작위 변경한 후(10%), 원래 단어를 예측하도록 양방향 학습함으로써 일반화된 언어 패턴을 학습하고 문맥을 이해하는 능력을 강화함

    | 역할 | 처리 방법 | 비중 |
    |---|---|---|
    | CONTEXT | 변경하지 않음 | 85% |
    | PREDICTION | MASK | 12% |
    | PREDICTION |무작위 변경 | 1.5% |
    | PREDICTION | 변경하지 않음 | 1.5% |

    - **Bidirection** : 트랜스포머 디코더가 단방향 예측($t-1$ 시점까지 정보를 기반으로 $t$ 시점을 예측)을 수행했던 것과 달리, 버트는 양방향 예측(이전 시점과 이후 시점의 정보를 종합하여 $t$ 시점을 예측)을 수행하므로, 문맥 벡터 생성 시 미래 순번 정보를 모두 마스킹 처리하지 않음

- **NSP(`N`ext `S`entence `P`rediction)** : `CLS` 토큰을 기반으로 두 문장이 연속된 문장인지 아닌지 예측하도록 학습함으로써 문장 간 관계를 학습함

    ![06](/_post_refer_img/TextAnalytics/11-06.jpg){: width="100%"}

-----

### Reference

- https://pub.towardsai.net/demystifying-bert-efe9ac3c1c74
- https://wikidocs.net/115055