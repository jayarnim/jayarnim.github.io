---
order: 8
title: SEQ2SEQ
date: 2024-08-05
categories: [Data Mining, Text Analytics]
tags: [Data Mining, NLP, Text Mining, Machine Translation]
math: true
description: >-
    Based on the lecture “Text Analytics (2024-1)” by Prof. Je Hyuk Lee, Dept. of Data Science, The Grad. School, Kookmin Univ.
image:
    path: /_post_refer_img/TextAnalytics/Thumbnail.png
---

## Machine Translation
-----

- **기계 번역(Machine Translation)** : 특정 언어로 표현된 텍스트를 다른 언어로 된 텍스트로 변환하는 과정

    ![01](/_post_refer_img/TextAnalytics/08-01.jpg){: width="100%"}

    - **NLU(`N`atural `L`anguage `U`nderstanding)** : 특정 언어로 된 텍스트를 이해하는 과정으로서, 해당 텍스트를 벡터 공간에 사상(Projection)하는 절차(Many to One)
    - **NLG(`N`atural `L`anguage `G`eneration)** : 다른 언어로 된 텍스트를 생성하는 과정으로서, 사상된 벡터를 목표 언어로 변환하는 절차(Many to Many)

- **Brief History**
    - **RBMT(Rule-based Machine Translation)** : 규칙 기반 기계 번역
    - **SMT(Statistical Machine Translation)** : 통계적 기계 번역
    - **NMT(`N`eural Networks based `M`achine `T`ranslation)** : 신경망 기반 기계 번역
        - Word Embedding
        - End-to-End Model

## SEQ2SEQ
-----

- **시퀀스 투 시퀀스(SEQ2SEQ)** : RNN 계열 레이어 기반 기계 번역 알고리즘

    $$\begin{aligned}
    P(y_{1}, y_{2}, \cdots, y_{T^{\prime}} \mid x_{1}, x_{2}, \cdots, x_{T})
    = \prod_{t=1}^{T^{\prime}}{P(y_{t} \mid z, y_{1}, y_{2}, \cdots, y_{t-1})}
    \end{aligned}$$

- **Architecture**

    ![02](/_post_refer_img/TextAnalytics/08-02.jpg){: width="100%"}

    - **인코더(Encoder)** : 입력 문장의 모든 단어들을 순차로 입력 받아 하나의 문맥 벡터(Context Vector)로 변환하는 모듈(NLU Process)

        $$\begin{aligned}
        h_{t}, c_{t}
        &= \text{LSTM}\left[\overrightarrow{\mathbf{w}}_{t} \mid h_{t-1}, c_{t-1}\right]
        \end{aligned}$$

        - $$h_{t}$$ : $t$ 시점 은닉 상태
        - $$c_{t}$$ : $t$ 시점 셀 상태
        - $$\overrightarrow{\mathbf{w}}_{t}$$ : $t$ 번째 단어 임베딩 벡터
        - $$\overrightarrow{\mathbf{z}}=h_{T}$$ : 마지막 시점 은닉 상태로서 문맥 벡터(Context Vector)

    - **디코더(Decoder)** : 문맥 벡터를 참조하여 목표 언어의 단어들을 순차로 출력하는 모듈(NLG Process)

        $$\begin{aligned}
        \eta_{t}, \sigma_{t}
        &= \text{LSTM}\left[\hat{\mathbf{y}}_{t-1} \mid \eta_{t-1}, \sigma_{t-1}\right]
        \end{aligned}$$

        - $$\eta_{t}$$ : $t$ 시점 은닉 상태
            - $$\eta_{0}=\overrightarrow{\mathbf{z}}$$ : 초기 은닉 상태는 문맥 벡터를 할당함
        - $$\sigma_{t}$$ : $t$ 시점 셀 상태
        - $$\hat{\mathbf{y}}_{t}=\text{MLP}_{\text{Softmax}}\left[\eta_{t}\right]$$ : 목표 언어의 $t$ 번째 단어 확률 분포 벡터
            - $$\hat{\mathbf{y}}_{0}$$ : 초기 입력값은 `<SOS>` 토큰을 활용함
            - $$\hat{\mathbf{y}}_{T}$$ : 마지막 출력값은 `<EOS>` 토큰을 활용함

- **교사 강요(Teacher Forcing)** : 디코더 학습 과정에서, 다음 시점의 입력으로서 이전 시점에서 생성한 출력값 대신 정답 시퀀스를 사용하는 기법

    ![03](/_post_refer_img/TextAnalytics/08-03.jpg){: width="100%"}

    - **Training Phase** : 정답 시퀀스 $$\overrightarrow{\mathbf{y}}_{t-1}$$ 를 입력값으로 사용함

        $$\begin{aligned}
        \eta_{t}, \sigma_{t}
        &= \text{LSTM}\left[\overrightarrow{\mathbf{y}}_{t-1} \mid \eta_{t-1}, \sigma_{t-1}\right]
        \end{aligned}$$

    - **Inference Phase** : 이전 시점에서 생성한 출력값 $$\hat{\mathbf{y}}_{t-1}$$ 를 입력값으로 사용함

        $$\begin{aligned}
        \eta_{t}, \sigma_{t}
        &= \text{LSTM}\left[\hat{\mathbf{y}}_{t-1} \mid \eta_{t-1}, \sigma_{t-1}\right]
        \end{aligned}$$

## Attention Application
-----

- **어텐션 메커니즘(Attention Application)** : 개별 질의에 특화된 정답 후보군별 중요도를 도출하는 메커니즘

- **SEQ2SEQ 적용 목적** : RNN 계열 레이어의 기울기 소실 문제를 보완하기 위함으로서, 문맥 벡터를 최종 은닉 상태로 활용하지 않고, 인코더 각 순번 은닉 상태와 디코더 현재 시점 간 관련성을 고려하여, 디코더의 각 시점에 특화된 문맥 벡터를 생성함

- **종류**
    - **루옹 어텐션(Luong Attention)** : 디코더의 현재 시점 은닉 상태를 활용하여 문맥 벡터를 생성하고, 이를 현재 시점 은닉 상태에 적용하는 어텐션 기법
    - **바다나우 어텐션(Bahdanau Attention)** : 디코더의 이전 시점 은닉 상태를 활용하여 문맥 벡터를 생성하고, 이를 현재 시점 입력값에 적용하는 어텐션 기법

### How to Attention

- **Attention Mechanism**

    $$\begin{aligned}
    \text{ATTN}\left(Q,K,V\right)
    = \underbrace{\text{Softmax}\left[f(Q,K)\right]}_{\text{Attention Weight}} \cdot V
    \end{aligned}$$

    - $Q$ : 특화할 대상으로서 질의(Query)
    - $K$ : 질의와의 관련성을 도출하기 위한 참조 정보로서 키(Key)
    - $V$ : 최종적으로 사용할 정보로서 정답(Value)

- **주의 가중치(Attention Wieght) 도출 함수 정의**

    $$\begin{aligned}
    \mathbf{A}^{(t)} = \text{Softmax}\left[f(\overrightarrow{\mathbf{q}}_{t}, \mathbf{K})\right]
    \end{aligned}$$

    | Name | Function | Defined by |
    |---|---|---|
    | Dot Product | $$f(\overrightarrow{\mathbf{q}}, \mathbf{K}) = \overrightarrow{\mathbf{q}} \cdot \mathbf{K}$$ | Luong et al. (2015) |
    | Learnable Weighted Attention | $$f(\overrightarrow{\mathbf{q}}, \mathbf{K}) = \overrightarrow{\mathbf{q}}^{T} \cdot \mathbf{W} \cdot \mathbf{K}$$ | Luong et al. (2015) |
    | Additive Attention | $$f(\overrightarrow{\mathbf{q}}, \mathbf{K}) = \mathbf{W}^{T}_{A} \cdot \text{tanh}\left[\mathbf{W}_{B} \cdot (\overrightarrow{\mathbf{q}} \oplus \mathbf{K})\right]$$ | Bahdanau et al. (2015) |
    | Concatenation | $$f(\overrightarrow{\mathbf{q}}, \mathbf{K}) = \mathbf{W}^{T}_{A} \cdot \text{tanh}\left[\mathbf{W}_{B} \cdot \overrightarrow{\mathbf{q}} + \mathbf{W}_{C} \cdot \mathbf{K}\right]$$ | Bahdanau et al. (2015) |
    | Scaled Dot Product | $$f(\overrightarrow{\mathbf{q}}, \mathbf{K}) = \displaystyle\frac{\overrightarrow{\mathbf{q}}^{T} \cdot \mathbf{K}}{\sqrt{n}}$$ | Vaswani et al. (2017) |


### Luong Attention

![04](/_post_refer_img/TextAnalytics/08-04.png){: width="100%"}

- Attention Mechanism

    $$
    \mathbf{A}^{(t)} = \text{Softmax}\left[\overrightarrow{\mathbf{q}}_{t} \cdot \mathbf{K}\right] \cdot \mathbf{V}
    $$

    - $$\overrightarrow{\mathbf{q}}_{t} = \eta_{t}$$ : 디코더의 $t$ 시점 은닉 상태
    - $$\overrightarrow{\mathbf{k}}_{i} = h_{i}$$ : 인코더의 $i$ 번째 순번 은닉 상태
    - $$\overrightarrow{\mathbf{v}}_{i} = h_{i}$$ : 인코더의 $i$ 번째 순번 은닉 상태

- 디코더의 $t$ 시점 특화 문맥 벡터(Context Vector) 도출

    $$\begin{aligned}
    \overrightarrow{\mathbf{z}}_{t}
    = \sum_{i}{\mathbf{A}^{(t)}_{i}}
    \end{aligned}$$

- $t$ 시점 문맥 벡터와 $t$ 시점 은닉 상태 정보 종합

    $$
    \overrightarrow{\mathbf{s}}_{t}
    = \text{MLP}_{\text{tanh}}\left[\overrightarrow{\mathbf{z}}_{t} \oplus \eta_{t}\right]
    $$

- 목표 언어의 $t$ 번째 단어 확률 분포 벡터 도출

    $$
    \hat{\mathbf{y}}_{t}
    = \text{MLP}_{\text{Softmax}}\left[\overrightarrow{\mathbf{s}}_{t}\right]
    $$

### Bahdanau Attention

![05](/_post_refer_img/TextAnalytics/08-05.png){: width="100%"}

- Attention Mechanism

    $$
    \mathbf{A}^{(t)} = \text{Softmax}\left[\overrightarrow{\mathbf{q}}_{t} \cdot \mathbf{K}\right] \cdot \mathbf{V}
    $$

    - $$\overrightarrow{\mathbf{q}}_{t} = \eta_{t-1}$$ : 디코더의 $t-1$ 시점 은닉 상태
    - $$\overrightarrow{\mathbf{k}}_{i} = h_{i}$$ : 인코더의 $i$ 번째 순번 은닉 상태
    - $$\overrightarrow{\mathbf{v}}_{i} = h_{i}$$ : 인코더의 $i$ 번째 순번 은닉 상태

- 디코더의 $t$ 시점 특화 문맥 벡터(Context Vector) 도출

    $$\begin{aligned}
    \overrightarrow{\mathbf{z}}_{t}
    = \sum_{i}{\mathbf{A}^{(t)}_{i}}
    \end{aligned}$$

- $t$ 시점 문맥 벡터와 $t$ 시점 입력 벡터 종합

    $$
    \overrightarrow{\mathbf{s}}_{t}
    = \overrightarrow{\mathbf{z}}_{t} \oplus \hat{\mathbf{y}}_{t-1}
    $$

- 목표 언어의 $t$ 번째 단어 확률 분포 벡터 도출

    $$\begin{aligned}
    \eta_{t}, \sigma_{t}
    &= \text{LSTM}\left[\overrightarrow{\mathbf{s}}_{t} \mid \eta_{t-1}, \sigma_{t-1}\right]\\
    \hat{\mathbf{y}}_{t}
    &= \text{MLP}_{\text{Softmax}}\left[\eta_{t}\right]
    \end{aligned}$$

## Metric
-----

- **BLEU Score(`B`i-`L`ingual `E`valuation `U`nderstudy Score)**

-----

### 이미지 출처

- https://yjjo.tistory.com/35