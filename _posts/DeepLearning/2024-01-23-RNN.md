---
order: 5
title: RNN
date: 2024-01-23
categories: [Machine Learning Techs, Deep Learning]
tags: [Deep Learning, NLP]
math: true
description: >-
  Based on the lecture “Intro. to Deep Learning (2023-2)” by Prof. Seong Man An, Dept. of Data Science, The Grad. School, Kookmin Univ.
image:
  path: /_post_refer_img/DeepLearning/Thumbnail.jpg
---

## RNN
-----

### Recurrent Neural Network

- **완전 연결 계층의 문제점** : 시계열 데이터 처리 불가능

    ![01](/_post_refer_img/DeepLearning/05-01.png){: width="100%"}

    - 동시성의 문제
    - 과거 상태를 보관할 메모리 장치의 부재

- **순환 신경망** : 이전 단계의 출력을 현재 단계에서 **반복적으로 고려하며** 입력 정보를 **순차적으로 처리하는** 신경망 알고리즘

    ![02](/_post_refer_img/DeepLearning/05-02.png){: width="100%"}

- **용례**

    ![03](/_post_refer_img/DeepLearning/05-03.png){: width="100%"}

    - **one to one**
    - **one to many** : Image Captioning
    - **many to one** : 감성 분석(Sentiment Classification)
    - **many to many** : 기계 번역(Machine Translation)

### 작동 방식

![04](/_post_refer_img/DeepLearning/05-04.jpeg){: width="100%"}

- **은닉 상태($\overrightarrow{s}_{t}$)의 갱신**

    $$\begin{aligned}
    \overrightarrow{s}_{t}
    &= \text{tanh}(\mathbf{U}\cdot\overrightarrow{x}_{t}+\mathbf{W}\cdot\overrightarrow{s}_{t-1}+\overrightarrow{\beta}_{s})
    \end{aligned}$$

    - $\text{tanh}$ : 활성화 함수
    - $\overrightarrow{x}_{t}$ : $t$ 시점에서의 입력 벡터
    - $\mathbf{U}$ : 입력값에 대한 가중치 행렬
    - $\overrightarrow{s}_{t-1}$ : $t-1$ 시점에서의 은닉 상태 벡터
    - $\mathbf{W}$ : 이전 은닉 상태에 대한 가중치 행렬
    - $\overrightarrow{\beta}_{s}$ : 편향 벡터

- **출력값($\overrightarrow{o}_{t}$)의 도출**

    $$\begin{aligned}
    \overrightarrow{o}_{t}
    &= \text{softmax}(\mathbf{V}\cdot\overrightarrow{s}_{t}+\overrightarrow{\beta}_{o})
    \end{aligned}$$

    - $\text{softmax}$ : 활성화 함수
    - $\overrightarrow{s}_{t}$ : $t$ 시점에서의 은닉 상태 벡터
    - $\mathbf{V}$ : 은닉값에 대한 가중치 행렬
    - $\overrightarrow{\beta}_{o}$ : 편향 벡터

## LSTM
-----

### Long Short-Term Memory

- **RNN의 한계점** : 장기 의존성 문제

    ![05](/_post_refer_img/DeepLearning/05-05.png){: width="100%"}

    - 역전파 과정에서 기울기가 소실되어 시퀀스가 길수록 초기 순번 정보에 대하여 학습되지 않음

- **장-단기 메모리 기법** : **여러 개의 게이트(Gate)**를 설치하고 **셀 상태와 은닉 상태를 구분함**으로써 RNN의 한계점을 보완하는 기법

    ![06](/_post_refer_img/DeepLearning/05-06.png){: width="100%"}

### 작동 방식

![07](/_post_refer_img/DeepLearning/05-07.jpeg){: width="100%"}

- **얼마나 잊을 것인가**

    ![08](/_post_refer_img/DeepLearning/05-08.png){: width="100%"}

    - **망각 게이트($f_{t}(x_{t},h_{t-1})$)** : 얼마나 잊을지 결정하는 관문

- **얼마나 기억할 것인가**

    ![09](/_post_refer_img/DeepLearning/05-09.png){: width="100%"}

    - **입력 게이트($i_{t}(x_{t},h_{t-1})$)** : 얼마나 기억할지 결정하는 관문
    - **셀 상태 업데이트($$\widetilde{C}_{t}(x_{t},h_{t-1})$$)** : 입력 게이트에서 판단할 정보를 도출하는 관문

- **현재 상태를 어떻게 정의할 것인가**

    ![10](/_post_refer_img/DeepLearning/05-10.png){: width="100%"}

    - **셀 상태($C_{t}$)** : 이전 셀 상태($C_{t-1}$)를 적당히 망각하고($f_{t}$), 현재 상태에서 갱신된 정보($$\widetilde{C}_{t}$$)를 적당히 기억하는($$i_{t}$$) 상태

- **얼마나 출력할 것인가**

    ![11](/_post_refer_img/DeepLearning/05-11.png){: width="100%"}

    - **출력 게이트($o_{t}(x_{t},h_{t-1})$)** : 얼마나 출력할지 결정하는 관문
    - **은닉 상태($h_{t}$)** : 셀 상태($C_{t}$)에서 적당히 출력된($o_{t}$) 상태

-----

### 이미지 출처

- https://dgkim5360.tistory.com/entry/understanding-long-short-term-memory-lstm-kr