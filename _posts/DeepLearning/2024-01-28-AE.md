---
order: 7
title: AutoEncoder
date: 2024-01-28
categories: [Machine Learning Techs, Deep Learning]
tags: [Deep Learning, Autoencoder]
math: true
description: >-
  Based on the lecture “Intro. to Deep Learning (2023-2)” by Prof. Seong Man An, Dept. of Data Science, The Grad. School, Kookmin Univ.
image:
  path: /_post_refer_img/DeepLearning/Thumbnail.jpg
---

## AutoEncoder
-----

- **오토인코더(`A`uto`E`ncoder; AE)** : 입력 데이터를 압축시켜 저차원 특징 공간으로 축소한 후, 이를 다시 확장하여 원본으로 복원하는 인공신경망 아키텍처

### How to Modeling

- **Encoder:**

    $$\begin{aligned}
    \mathbf{z} = F\left(\mathbf{x} \mid \Theta \right)
    \end{aligned}$$

    - $$F_{\Theta}$$ : Encoder Layer
    - $$\mathbf{x}$$ : Input Data
    - $$\mathbf{z}$$ : Latent Space

- **Decoder:** 

    $$\begin{aligned}
    \hat{\mathbf{x}} = G\left(\mathbf{z} \mid \Phi \right)
    \end{aligned}$$

    - $$G_{\Phi}$$ : Encoder Layer
    - $$\mathbf{z}$$ : Latent Space
    - $$\hat{\mathbf{x}}$$ : Output Data

- **Optimization:**

    $$\begin{aligned}
    \hat{\mathbf{z}}, \hat{\Theta}, \hat{\Phi} &= \text{arg}\min{\mathcal{L}\left(\mathbf{x}, \hat{\mathbf{x}}\right)}
    \end{aligned}$$

### Types

- **Basic AutoEncoder** : 입력층, 잠재공간, 출력층으로 구성

    ![01](/_post_refer_img/DeepLearning/06-01.png){: width="100%"}

- **적층 오토인코더(Stacked AutoEncoder or Deep AutoEncoder)** : 입력층과 잠재공간, 잠재공간과 출력층 사이에 여러 장의 은닉층을 추가함으로써 기본 오토인코더보다 복잡하고 비선형적인 데이터 패턴을 잘 포착하도록 함

    ![02](/_post_refer_img/DeepLearning/06-02.png){: width="100%"}

- **희소 오토인코더(`S`parse `A`uto`E`ncoder; SAE)** : 입력 데이터를 고차원으로 확장했다가 본래 차원으로 축소하는 과정을 통해 저차원에서는 발견하기 어려운 잠재 정보나 패턴을 포착하도록 함

    ![03](/_post_refer_img/DeepLearning/06-03.png){: width="100%"}

    - **희소성 제약(Sparse Constraint)** : 특히 Sparse AutoEncoder 에서, 데이터 과적합을 방지하고 핵심 정보만 선택적으로 학습하기 위하여 은닉층 뉴런 중 일부만 활성화되도록 강제하는 기법

        $$\begin{aligned}
        \mathcal{L}=\frac{1}{N}\sum_{i=1}^{N}{\Vert \mathbf{x}_{i} - \hat{\mathbf{x}}_{i} \Vert^{2}} + \beta \cdot \underbrace{\Omega(\overrightarrow{\mathbf{h}} \mid m, \rho)}_{\begin{array}{c} \text{Sparse} \\ \text{Penalty} \end{array}}
        \end{aligned}$$

    - **Sparse Penalty Function** : 은닉층 뉴런들이 평균적으로 목표 희소성 비율에 가깝게 활성화되도록 하기 위함으로, 대개 쿨백 라이블러 발산(Kullback–Leibler Divergence)을 활용함

        $$\begin{aligned}
        \Omega(\overrightarrow{\mathbf{h}} \mid m, \rho)
        &= \sum_{j=1}^{m}{D_{KL}\left[\rho \parallel \hat{\rho}_{j}\right]}\\
        &= \sum_{j=1}^{m}{\rho \cdot \log{\frac{\rho}{\hat{\rho}_{j}}} + (1-\rho) \cdot \log{\frac{1-\rho}{1-\hat{\rho}_{j}}}}
        \end{aligned}$$

        - $m$ : 은닉층 뉴런 수
        - $\rho$ : 목표 희소성 비율
        - $$\hat{\rho}_{j}=\displaystyle\frac{1}{N}\sum_{i=1}^{N}{h_{j}^{(i)}}$$ : 은닉층 뉴런 $j$ 의 평균 출력값

- **잡음 제거 오토인코더(`D`enoising `A`uto`E`ncoder; DAE)** : 원본 데이터에 잡음을 추가하여 입력한 후, 원본을 복원함으로써 잡음에 대한 강건성을 확보하고 데이터의 본질적인 특징에 집중할 수 있도록 함

    ![04](/_post_refer_img/DeepLearning/06-04.png){: width="100%"}

    - **How to Generate Noise**

        $$\begin{aligned}
        \tilde{\mathbf{x}} &= g(\mathbf{x} \mid \theta)
        \end{aligned}$$

        - Gaussian Noise:

            $$\begin{aligned}
            g(\mathbf{x} \mid \sigma)= \mathbf{x} + \epsilon, \quad \epsilon \sim \mathcal{N}(0,\sigma^2)
            \end{aligned}$$

        - Salt and Pepper Noise:

            $$
            g(\mathbf{x} \mid p)=\begin{cases}\begin{aligned}
            \mathbf{x} \quad & 1-p\\
            \mathbf{v}_{\text{min}} \quad & \frac{p}{2}\\
            \mathbf{v}_{\text{max}} \quad & \frac{p}{2}
            \end{aligned}\end{cases}
            $$

        - Masking Noise:

            $$\begin{aligned}
            g(\mathbf{x} \mid p)= \mathbf{x} \cdot m, \quad m \sim \text{Bernouli}(1-p)
            \end{aligned}$$

## Variational AutoEncoder
-----

- **변분 오토인코더(`V`ariational `A`uto`E`ncoder; VAE)** : 입력 데이터의 확률 분포를 탐색하고(Encoder), 그 파라미터(Latent Space)에 기초하여 데이터를 확률적으로 생성하는(Decoder), 오토인코더 기반 생성 모형(Generative Model)

### How to Modeling

- 관측치 $\overrightarrow{\mathbf{x}}$ 가 파라미터 $\overrightarrow{\mathbf{z}}$ 에서 생성되었다고 가정하자. $\overrightarrow{\mathbf{x}}$ 의 우도 함수:

    $$
    \overrightarrow{\mathbf{x}} \mid \overrightarrow{\mathbf{z}} \sim \mathcal{L}\left(\Theta \right)
    $$

- 파라미터 $\overrightarrow{\mathbf{z}}$ 의 사전 확률 분포 결정:

    $$
    \overrightarrow{\mathbf{z}} \sim \mathcal{N}\left(0, \mathbf{I}_{K}\right)
    $$

- 파라미터 $\overrightarrow{\mathbf{z}}$ 의 사후 확률 분포 추정:

    $$
    p\left(\overrightarrow{\mathbf{z}} \mid \overrightarrow{\mathbf{x}} ; \Theta\right) \propto \mathcal{L}\left(\overrightarrow{\mathbf{x}} \mid \overrightarrow{\mathbf{z}} ; \Theta \right) \cdot \pi\left(\overrightarrow{\mathbf{z}}\right)
    $$

- $\overrightarrow{\mathbf{z}} \mid \overrightarrow{\mathbf{x}} \sim P(\Theta)$ 의 근사 분포 $\overrightarrow{\mathbf{z}} \sim Q(\Phi)$ 정의:

    $$\begin{aligned}
    q\left(\overrightarrow{\mathbf{z}} ; \Phi\right)
    &= \mathcal{N}\left(\mu(\overrightarrow{\mathbf{x}} ; \Phi), \text{diag}[\sigma^2(\overrightarrow{\mathbf{x}} ; \Phi)]\right)\\
    &\approx \mu(\overrightarrow{\mathbf{x}} ; \Phi) + \sigma(\overrightarrow{\mathbf{x}} ; \Phi) \odot \epsilon \quad \text{for} \quad \epsilon \sim \mathcal{N}\left(0, \mathbf{I}_{K}\right)
    \end{aligned}$$

- ELBO:

    $$
    \text{ELBO}
    = \underbrace{\mathbb{E}_{\mathbf{z} \sim Q(\Phi)}\Big[\log{\mathcal{L}\left(\overrightarrow{\mathbf{x}} \mid \overrightarrow{\mathbf{z}} ; \Theta \right)}\Big]}_{-\text{Reconstruction Loss}} - \beta \cdot \underbrace{D_{KL}\Big[q\left(\overrightarrow{\mathbf{z}} ; \Phi\right) \parallel \pi\left(\overrightarrow{\mathbf{z}}\right)\Big]}_{\text{KL Divergence}}
    $$

- Optimization:

    $$
    \hat{\Theta},\hat{\Phi} \mid K, \beta
    = \text{arg}\max_{\Theta,\Phi}{\text{ELBO}}
    $$

    - $K$ : Dimension of Latent Space
    - $0 \le \beta \le 1$ : KL Annealing Param

### Architecture

![05](/_post_refer_img/DeepLearning/06-05.png){: width="100%"}

- **Encoder** : Posterior Probability Distribution Estimatior

    $$
    q\left(\overrightarrow{\mathbf{z}} ; \Phi\right)
    $$

- **Latent Space** : Posterior Probability Distribution

    $$
    \overrightarrow{\mathbf{z}} \mid \overrightarrow{\mathbf{x}}
    $$

- **Decoder** : Data Generator

    $$
    \mathcal{L}\left(\overrightarrow{\mathbf{x}} \mid \overrightarrow{\mathbf{z}} ; \Theta \right)
    $$

-----

### 이미지 출처

- https://velog.io/@jochedda/%EB%94%A5%EB%9F%AC%EB%8B%9D-Autoencoder-%EA%B0%9C%EB%85%90-%EB%B0%8F-%EC%A2%85%EB%A5%98