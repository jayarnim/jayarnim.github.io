---
order: 1
title: Deep Learning WorkFlow
date: 2024-01-22
categories: [Machine Learning Techs, Deep Learning]
tags: [Deep Learning, FFNN]
math: true
description: >-
  Based on the lecture “Intro. to Deep Learning (2023-2)” by Prof. Seong Man An, Dept. of Data Science, The Grad. School, Kookmin Univ.
image:
  path: /_post_refer_img/DeepLearning/Thumbnail.jpg
---

## What? Deep Learning
-----

- **정의** : 심층 인공 신경망 알고리즘을 활용한 기계학습 방법
    - **심층 신경망(Deep Artificial Neural Network; DNN)** : 층을 겹겹이 쌓아올려 구성한 알고리즘
    - **알고리즘(Algorithm)** : 관측치들로부터 표현을 학습하는 수학 모델

- **인공 신경망과 심층 신경망의 구분**

    ![01](/_post_refer_img/DeepLearning/01-01.png){: width="100%"}

    - **인공 신경망(Artificial Neural Network; ANN)** : 은닉층이 하나인 신경망
    - **심층 신경망(Deep Artificial Neural Network; DNN)** : 은닉층이 둘 이상인 신경망

- **심층 신경망의 구성** : 완전 연결 계층(Fully Connected Layer)

    ![02](/_post_refer_img/DeepLearning/01-02.jpg){: width="100%"}

    - **입력층(Input Layer)** : 관측치를 입력 받는 계층
    - **평탄화층(Flatten Layer)** : 관측치 속성의 구조를 1차원으로 평탄화하는 계층
    - **은닉층(Hidden Layer)** : 입력층과 출력층 사이에 존재하는 계층
    - **출력층(Output Layer)** : 예측값(Target value)을 출력하는 계층

- **학습 목표** : 모델 파라미터에 대하여, 손실을 최소화하는 아규먼트 탐색

- **파라미터 종류**
    - **모델 파라미터(Model Parameter)**
        - Weight
        - Bias

    - **하이퍼 파라미터(Hyper Parameter)**
        - Unit Layer
        - Unit Node
        - Activation Function
        - Loss Function
        - Optimizer
        - Batch Size
        - Epoch

## 층(Layer)의 이해
-----

- **정의** : 하나 이상의 텐서를 입력값으로 받고, 하나 이상의 텐서를 출력값으로 반환하는 데이터 처리 모듈

    ![03](/_post_refer_img/DeepLearning/01-03.jpg){: width="100%"}

- **구성** : 퍼셉트론(Perceptron)

    ![04](/_post_refer_img/DeepLearning/01-04.png){: width="100%"}

- **상태** : 모델 파라미터로서 **가중치 행렬(Weight; $W$)** 과 **편향 벡터(bias; $\overrightarrow{b}$)**

- **연산** : 순입력 함수와 활성화 함수로 구성된 **정방향 패스(Forward Path)**

    - **순입력 함수(Net Input Function)**

        ![05](/_post_refer_img/DeepLearning/01-05.jpg){: width="100%"}

        $$
        W \cdot \overrightarrow{x} + \overrightarrow{b}
        $$

    - **활성화 함수(Activation Function)**
        - `Relu`
        - `Tanh`
        - `Sigmoid`
        - `Step`
        - `Softmax`

## 학습 과정의 이해
-----

- **학습 횟수의 이해**

    ![06](/_post_refer_img/DeepLearning/01-06.jpeg){: width="100%"}

    - `epochs` : 훈련용 데이터 세트를 구성하는 모든 관측치가 신경망을 통과하는 횟수
    - `iteration` : `epoch` 를 1회 마치는 동안 모델 파라미터 갱신 횟수
    - `batch_size` : `epoch` 를 1회 마치는 동안 모델 파라미터를 1회 갱신하기 위하여 동원할 관측치 갯수

- **`iteration` 과정**

    ![07](/_post_refer_img/DeepLearning/01-07.jpg){: width="100%"}

    - **Forward Path** : 관측치에 대하여 순전파 연산하여 예측값을 도출함
    - **Loss Output** : 손실 함수를 통해 손실을 도출함
    - **Backward Path** : 손실을 줄이도록 각 계층의 모델 파라미터를 갱신함

- **학습 목적 함수** : 손실 함수(Loss Function)
    - **정의** : 손실을 반응변수, 모델 파라미터를 설명변수로 가지는 함수

    - **판별 분석** : 대개 `crossentropy` 를 손실로 간주함
        - `binary_crossentropy` : 이항 분류 분석 시 사용하는 크로스 엔트로피 불순도
        - `categorical_crossentropy` : 다항 분류 분석, 반응변수가 희소 행렬인 경우
        - `sparse_categorical_crossentropy` : 다항 분류 분석, 반응변수가 희소 행렬이 아닌 경우

    - **회귀 분석** : 대개 `squared_error` 를 손실로 간주함
        - `mean_squared_error`
        - `mean_squared_log_error`
        - `mean_absolute_error`
        - `mean_absolute_percentage_error`
        - `cosine_similarity`

- **학습 방식** : 경사하강법(Gadient Descent)
    - `GD` : 모든 관측치를 검토하여 가중치 갱신 방향 및 크기 결정
    - `SGD` : 일부 관측치만 확률적으로 검토함
    - `Momentum`
    - `RMSProp`
    - `Adam`

## 텐서플로와 케라스
-----

![08](/_post_refer_img/DeepLearning/01-08.png){: width="100%"}

- **텐서플로(TensorFlow)** : 저수준 컴퓨팅 플랫폼
    - 텐서(Tensor)
    - 미분(Gradient Tape API)

- **케라스(Keras)** : 고수준 딥러닝 API
    - 층(Layer)
    - 모델(Sequential API, Functional API, Model Class)
    - 옵티마이저(Optimizer)
    - 손실 함수(Loss Function)
    - 성능 검증 지표(Metric)