---
order: 3
title: Variational Inference
date: 2024-08-07
categories: [5.BAYES, 2.posterior approx.]
tags: [bayesian, objective function, information theory, variational inference]
math: true
image:
    path: /_post_refer_img/5.BAYES/2.posterior/Thumbnail.jpg
---

## Information Theory
-----

- **정보이론(Information Theory)**: 신호에 존재하는 정보의 양을 측정하는 이론으로서, 특정 확률분포의 특성을 알아내거나, 두 확률분포 간 유사성을 정량화하는 데 사용함

- **Shannon's Information Theory Principles**
    - 자주 발생하지 않는 사건(Unlikely Event)일수록 높은 정보량을 가짐(Informative)
        - 해가 동쪽에서 뜨는 사건은 사람들이 확신하고 있는 사건이므로 정보량이 낮은 반면, 해가 서쪽에서 뜨는 사건은 자연 법칙을 거스르는 극히 드문 사건이므로 정보량이 높음

    - 독립사건은 추가적인 정보량을 가짐(Addictive Information)
        - 동전을 두 번 던져서 앞면이 두 번 나오는 사건은 동전을 한 번 던져서 앞면이 나오는 사건보다 정보량이 두 배임

- **자기정보(Self-Information)**: 확률변수 $X \sim P$ 에 대하여, 사건 $X=x$ 가 발생했을 때의 정보량

    $$\begin{aligned}
    I(X=x)
    &=-\log{P(X=x)}
    \end{aligned}$$

- **엔트로피(Entropy)**: 자기정보의 기대값으로서, 주어진 확률분포에서 발생 가능한 사건들의 평균적인 정보량

    $$\begin{aligned}
    H(P)
    = \mathbb{E}_{X \sim P}\left[I(X)\right]
    = -\sum_{X}{\log{P(x)} \cdot P(x)}
    \end{aligned}$$

    - 사건의 분포가 결정적일수록(Deterministic) 엔트로피가 감소함
    - 사건의 분포가 균등할수록(Uniform) 엔트로피가 증가함

- **교차 엔트로피(Cross Entropy)**: 확률변수 $X$ 의 분포 $P$ 와 그 근사 분포 $Q$ 에 대하여, $Q$ 가 $P$ 에 대하여 제공하는 **정보의 불확실성을** 측정하는 지표

    $$\begin{aligned}
    H(P,Q)
    = \mathbb{E}_{X \sim P}\left[-\log{Q(X)}\right]
    = -\sum_{X}{\log{Q(X)} \cdot \log{P(X)}}
    \end{aligned}$$

- **쿨백 라이블러 발산(Kullback-Leibler Divergence)**: 확률변수 $X$ 의 분포 $P$ 와 그 근사 분포 $Q$ 에 대하여, $Q$ 를 $P$ 의 **근사 분포로 사용할 때의 비효율성을** 측정하는 지표로서, $P$ 에서 샘플링된 $X$ 에 대하여, $P$ 가 제공하는 평균적인 정보량과 $Q$ 가 제공하는 평균적인 정보량의 차이

    $$\begin{aligned}
    KL[P(X) \parallel Q(X)]
    = H(P,Q) - H(P)
    = \sum_{X}{\log{\frac{P(X)}{Q(X)}} \cdot P(X)}
    \end{aligned}$$

## Variational Inference
-----

- **변분 추론(`V`ariational `I`nference)**: 정보 이론을 활용하여 제안 분포 $W \sim Q$ 를 목표 분포 $W \mid \mathcal{D} \sim P$ 에 근사하는 모수 방법론(Parametric Method)

    $$\begin{aligned}
    \hat{\Theta}
    &= \text{arg}\min{KL\big[Q(W) \parallel P(W \mid \mathcal{D})\big]}
    \end{aligned}$$

    - 목표 분포 $W \mid \mathcal{D} \sim P$ 를 잘 설명하는 제안 분포 $W \sim Q$ 를 탐색하는 것이 이상적

        $$\begin{aligned}
        D_{KL}\big[P(W \mid \mathcal{D}) \parallel Q(W)\big]
        &= \mathbb{E}_{W \mid \mathcal{D} \sim P}\left[\log{\frac{P(W \mid \mathcal{D})}{Q(W)}}\right]
        \end{aligned}$$

    - 목표 분포 $W \mid \mathcal{D} \sim P$ 에서 샘플을 추출할 수 없으므로 대안으로서 목표 분포로 잘 설명될 수 있는 제안 분포 $W \sim Q$ 를 탐색함

        $$\begin{aligned}
        D_{KL}\big[Q(W) \parallel P(W \mid \mathcal{D})\big]
        &= \mathbb{E}_{W \sim Q}\left[\log{\frac{Q(W)}{P(W \mid \mathcal{D})}}\right]
        \end{aligned}$$

- Kullback-Leibler divergence segmentation:

    $$\begin{aligned}
    &KL[Q(W) \parallel P(W \mid \mathcal{D})]\\
    &= \mathbb{E}_{W \sim Q}\left[\log{\frac{Q(W)}{P(W \mid \mathcal{D})}}\right]\\
    &= \mathbb{E}_{W \sim Q}[\log{Q(W)}] - \mathbb{E}_{W \sim Q}[\log{P(W \mid \mathcal{D})}]
    \end{aligned}$$

- posterior is decomposed by Bayes' theorem:

    $$\begin{aligned}
    &\mathbb{E}_{W \sim Q}[\log{P(W \mid \mathcal{D})}]\\
    &= \mathbb{E}_{W \sim Q}\left[\log{\frac{P(\mathcal{D} \mid W) \cdot P(W)}{P(\mathcal{D})}}\right]\\
    &= \mathbb{E}_{W \sim Q}[\log{P(\mathcal{D} \mid W)}] + \mathbb{E}_{W \sim Q}[\log{P(W)}] - \mathbb{E}_{W \sim Q}[\log{P(\mathcal{D})}]
    \end{aligned}$$

- Therefore:

    $$\begin{aligned}
    &KL[Q(W) \parallel P(W \mid \mathcal{D})]\\
    &= \mathbb{E}_{W \sim Q}[\log{Q(W)}] - \Big(\mathbb{E}_{W \sim Q}[\log{p(\mathcal{D} \mid W)}] + \mathbb{E}_{W \sim Q}[\log{P(W)}] - \mathbb{E}_{W \sim Q}[\log{P(\mathcal{D})}]\Big)\\
    &= \mathbb{E}_{W \sim Q}[\log{Q(W)}] - \mathbb{E}_{W \sim Q}[\log{P(W)}] - \mathbb{E}_{W \sim Q}[\log{P(\mathcal{D} \mid W)}] + \mathbb{E}_{W \sim Q}[\log{P(\mathcal{D})}]
    \end{aligned}$$

- Some items are tied to the Kullback Leibler divergence:

    $$\begin{aligned}
    &KL[Q(W) \parallel P(W)]\\
    &= \mathbb{E}_{W \sim Q}\left[\log{\frac{Q(W)}{P(W)}}\right]\\
    &= \mathbb{E}_{W \sim Q}[\log{Q(W)} - \log{P(W)}]\\
    &= \mathbb{E}_{W \sim Q}[\log{Q(W)}] - \mathbb{E}_{W \sim Q}[\log{P(W)}]
    \end{aligned}$$

- Finally:

    $$\begin{aligned}
    &KL[Q(W) \parallel P(W \mid \mathcal{D})]\\
    &= KL[Q(W) \parallel P(W)] - \mathbb{E}_{W \sim Q}[\log{P(\mathcal{D} \mid W)}] + \mathbb{E}_{W \sim Q}[\log{P(\mathcal{D})}]
    \end{aligned}$$

## ELBO
-----

- **증거 하한(`E`vidence `L`ower `B` `o`und)**: 변분 추론의 목적 함수로서 증거 $\log{P(\mathcal{D})}$ 의 하한

    $$\begin{aligned}
    \text{ELBO}
    &= \mathbb{E}_{W \sim Q}\left[\log{P(\mathcal{D} \mid W)}\right] - KL[Q(W) \parallel P(W)]
    \end{aligned}$$

    - $\mathbb{E}_{W \sim Q}\left[\log{P(\mathcal{D} \mid W)}\right]$: 기대 로그 우도로서 근사 분포의 데이터 적합도
    - $KL[Q(W) \parallel P(W)]$: 근사 분포와 사전 분포 간 쿨백 라이블러 발산으로서 사전 신념 기준 근사 분포의 복잡도

- Kullback-Leibler divergence segmentation:

    $$\begin{aligned}
    &KL[Q(W) \parallel P(W \mid \mathcal{D})]\\
    &= KL[Q(W) \parallel P(W)] - \mathbb{E}_{W \sim Q}[\log{P(\mathcal{D} \mid W)}] + \mathbb{E}_{W \sim Q}[\log{P(\mathcal{D})}]
    \end{aligned}$$

    - $KL\big[Q(W) \parallel P(W)\big]$ : Kullback-Leibler divergence between approx. and prior
    - $\mathbb{E}_{W \sim Q}\left[\log{P(\mathcal{D} \mid W)}\right]$ : expected log likelihood, goodness of fit of the approx. dist. to the data
    - $\log{P(\mathcal{D})}$ : log marginal likelihood

- best approx.:

    $$\begin{aligned}
    KL[Q(W) \parallel P(W)] - \mathbb{E}_{W \sim Q}[\log{P(\mathcal{D} \mid W)}] + \mathbb{E}_{W \sim Q}[\log{P(\mathcal{D})}]
    &=0
    \end{aligned}$$

- to summarize the evidence,

    $$\begin{aligned}
    \mathbb{E}_{W \sim Q}[\log{P(\mathcal{D})}]
    &\ge \mathbb{E}_{W \sim Q}[\log{P(\mathcal{D} \mid W)}] - KL[Q(W) \parallel P(W)]
    \end{aligned}$$

- since evidence is constant for the parameters,

    $$\begin{aligned}
    &\min{KL[Q(W) \parallel P(W \mid \mathcal{D})]}\\
    &\Leftrightarrow \max{\mathbb{E}_{W \sim Q}[\log{P(\mathcal{D} \mid W)}] - KL[Q(W) \parallel P(W)]}
    \end{aligned}$$