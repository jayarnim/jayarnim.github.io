---
order: 14
title: Recurrent Neural Networks
date: 2024-01-14
categories: [DATA MINING TECHS, 4.machine learning]
tags: [Deep Learning, RNN, NLP]
math: true
description: >-
    Based on the following lectures <br>
    (1) “Intro. to Deep Learning (2023-2)” by Prof. Seong Man An, Dept. of Data Science, The Grad. School, Kookmin Univ. <br>
    (2) “Text Analytics (2024-1)” by Prof. Je Hyuk Lee, Dept. of Data Science, The Grad. School, Kookmin Univ.
image:
  path: /_post_refer_img/DataMiningTechs/4.MachineLearning/Thumbnail.jpg
---

## Why? Recurrent-Net
-----

- Time series data is data\\
where there is a `sequence between features`:

    ![01](/_post_refer_img/DataMiningTechs/4.MachineLearning/14-01.png){: width="100%"}

- `Fully connected layers` treat the positions of input features equally,\\
so they do not structurally reflect `order information` between features:

    ![02](/_post_refer_img/DataMiningTechs/4.MachineLearning/14-02.png){: width="100%"}

- RNN(`R`ecurrent `N`eural `N`etworks) involves `preprocessing` operations\\
that preserve `sequence information`:

    ![03](/_post_refer_img/DataMiningTechs/4.MachineLearning/14-03.png){: width="100%"}


## Vanilla RNN
-----

![04](/_post_refer_img/DataMiningTechs/4.MachineLearning/14-04.png){: width="100%"}

- update hidden state $\overrightarrow{\mathbf{z}}_{t}$:

    $$\begin{aligned}
    \overrightarrow{\mathbf{z}}_{t}
    &= \text{tanh}(\mathbf{U}\cdot\overrightarrow{\mathbf{x}}_{t}+\mathbf{W}\cdot\overrightarrow{\mathbf{z}}_{t-1}+\overrightarrow{\mathbf{b}}_{h})
    \end{aligned}$$

    - $\text{tanh}$ : activation function
    - $\overrightarrow{\mathbf{x}}_{t}$ : input value @ $t$
    - $\mathbf{U}$ : weight matrix of input value @ $t$
    - $\overrightarrow{\mathbf{z}}_{t-1}$ : hidden state @ $t-1$
    - $\mathbf{W}$ : weight matrix of hidden state @ $t-1$
    - $\overrightarrow{\mathbf{b}}_{h}$ : bias

- print output $\overrightarrow{\mathbf{y}}_{t}$

    $$\begin{aligned}
    \overrightarrow{\mathbf{y}}_{t}
    &= \text{softmax}(\mathbf{V}\cdot\overrightarrow{\mathbf{z}}_{t}+\overrightarrow{\mathbf{b}}_{o})
    \end{aligned}$$

    - $\text{softmax}$ : activation function
    - $\overrightarrow{\mathbf{z}}_{t}$ : hidden state @ $t$
    - $\mathbf{V}$ : weight matrix of hidden state @ $t$
    - $\overrightarrow{\mathbf{b}}_{o}$ : bias

## LSTM
-----

- vanilla rnn suffers from the problems of `long-term dependencies`:

    ![05](/_post_refer_img/DataMiningTechs/4.MachineLearning/14-05.png){: width="100%"}

    - `Long-term dependencies` are a problem in which the `initial order information` is not preserved as the sequence gets longer due to the `vanishing gradient`.

- LSTM(`L`ong `S`hort-`T`erm `M`emory) is technique to alleviate vanishing gradient through `gate adjustment`:

    ![07](/_post_refer_img/DataMiningTechs/4.MachineLearning/14-07.jpeg){: width="100%"}

    - `forget gate`: generate `forget rule`
    - `input gate`: generate `remember rule` and `cell state update`
    - `cell state`: `determine` how much to remember and how much to forget
    - `output gate`: generate `hidden state` and `output`

- forget gate:

    ![08](/_post_refer_img/DataMiningTechs/4.MachineLearning/14-08.png){: width="100%"}

- input gate:

    ![09](/_post_refer_img/DataMiningTechs/4.MachineLearning/14-09.png){: width="100%"}

- cell state:

    ![10](/_post_refer_img/DataMiningTechs/4.MachineLearning/14-10.png){: width="100%"}

- output gate:

    ![11](/_post_refer_img/DataMiningTechs/4.MachineLearning/14-11.png){: width="100%"}

-----

## Sourse

- https://dgkim5360.tistory.com/entry/understanding-long-short-term-memory-lstm-kr