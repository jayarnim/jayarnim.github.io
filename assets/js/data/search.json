[
  
  {
    "title": "Boosting",
    "url": "/posts/Boosting/",
    "categories": "",
    "tags": "",
    "date": "2024-01-21 00:00:00 +0900",
    





    
    "snippet": "Index  GBM  sklearn.ensemble.GradientBoostingClassifier&lt;/br&gt;GBM      Ensemble        Boosting : ë‹¨ì¼ ëª¨ë¸ë“¤ì„ ì§ë ¬ë¡œ ì‹¬í™” ì‘ì—…í•˜ë„ë¡ ì„¤ê³„í•˜ëŠ” ë°©ì‹    &lt;img alt=\"boosting\" src=\"https://github.com/jayarnim/jayarni...",
    "content": "Index  GBM  sklearn.ensemble.GradientBoostingClassifier&lt;/br&gt;GBM      Ensemble        Boosting : ë‹¨ì¼ ëª¨ë¸ë“¤ì„ ì§ë ¬ë¡œ ì‹¬í™” ì‘ì—…í•˜ë„ë¡ ì„¤ê³„í•˜ëŠ” ë°©ì‹    &lt;img alt=\"boosting\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/020824f7-a8e4-4c67-825d-5c0cd3142f30\" width=80%&gt;                  ì´ì „ ëª¨ë¸ì— ëŒ€í•˜ì—¬ ê·¸ ì†ì‹¤ì´ í° ê´€ì¸¡ì¹˜ë“¤ì„ í•™ìŠµë¥ (Learning Rate)ë§Œí¼ ê°€ì¤‘í•˜ì—¬ ì‹¬í™”í•™ìŠµí•¨                  ê²½ì‚¬í•˜ê°•ë²•ê³¼ í•™ìŠµë¥ ì˜ ì´í•´                          Gradient Boosting Machine(GBM) : Decision Tree ì— ê¸°ë°˜í•œ Boosting Ensemble Algorithm    &lt;img alt=\"ê·¸ë¼ë””ì–¸íŠ¸ë¶€ìŠ¤íŒ…ë¨¸ì‹ \" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/353d194d-74db-4a05-b0e9-88248d3925e9\" width=80%&gt;          Decision Tree      &lt;/br&gt;sklearn.ensemble.GradientBoostingClassifierfrom sklearn.ensemble import GradientBoostingClassifierğŸ’¡ General HyperParameter  random_state(default : None)  warm_start(default : False) : ë™ì‘ ë©”ì‹œì§€ ì¶œë ¥ ì—¬ë¶€ ì„¤ì •ğŸ’¡ Model HyperParameter  criterion(default : 'friedman_mse') : ì˜¤ì°¨($\\approx$ ë¶ˆìˆœë„) ì¸¡ì • ë°©ë²•          'friedman_mse'      'squared_error'            max_features(default : None) : ê·œì¹™ ì„¤ê³„ ì‹œ ê³ ë ¤í•  ì„¤ëª…ë³€ìˆ˜ ê°¯ìˆ˜          'sqrt' : $\\sqrt{n}$      'log2' : $\\log_2{n}$      None : $n$        n_estimators(default : 100) : ë™ì›í•  Decision Tree ì˜ ê°œìˆ˜ğŸ’¡ Gradient Descent HyperParameter      learning_rate(default : 0.1) : í•™ìŠµë¥     loss(default : 'log_loss') : ì†ì‹¤ í•¨ìˆ˜          'log_loss'      'deviance'      'exponential'            tol(default : 0.0001) : í—ˆìš© ì†ì‹¤    n_iter_no_change(default : None) : ì†ì‹¤ì´ ëª‡ íšŒ ì´ìƒ ìœ íš¨í•˜ê²Œ ê°ì†Œí•˜ì§€ ì•Šì„ ê²½ìš° í•™ìŠµì„ ì¤‘ë‹¨í•  ê²ƒì¸ê°€ğŸ’¡ Pruning  Pre-Pruning          max_depth(default : 3) : íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´      max_leaf_nodes(default : None) : ë¦¬í”„ ë…¸ë“œì˜ ìµœëŒ€ ê°¯ìˆ˜      min_samples_split(default : 2) : í•˜ìœ„ ë…¸ë“œë¡œ ê°€ì§€ë¥¼ ë»—ê¸° ìœ„í•´ í•„ìš”í•œ ìµœì†Œí•œì˜ ìƒ˜í”Œ ê°œìˆ˜      min_impurity_decrease(default : 0) : í•˜ìœ„ ë…¸ë“œë¡œ ê°€ì§€ë¥¼ ë»—ê¸° ìœ„í•´ í•„ìš”í•œ ìµœì†Œí•œì˜ ë¶ˆìˆœë„ ê°œì„  ì •ë„      min_samples_leaf(default : 1) : ë¦¬í”„ ë…¸ë“œê°€ ë˜ê¸° ìœ„í•´ í•„ìš”í•œ ìµœì†Œí•œì˜ ìƒ˜í”Œ ê°œìˆ˜            Cost-Complexity Pruning          ccp_alpha(default : 0)      Index  XGBoost  xgboost.XGBClassifier&lt;/br&gt;XGBoost      Ensemble        Boosting : ë‹¨ì¼ ëª¨ë¸ë“¤ì„ ì§ë ¬ë¡œ ì‹¬í™” ì‘ì—…í•˜ë„ë¡ ì„¤ê³„í•˜ëŠ” ë°©ì‹    &lt;img alt=\"boosting\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/020824f7-a8e4-4c67-825d-5c0cd3142f30\" width=80%&gt;                  ì´ì „ ëª¨ë¸ì— ëŒ€í•˜ì—¬ ê·¸ ì†ì‹¤ì´ í° ê´€ì¸¡ì¹˜ë“¤ì„ í•™ìŠµë¥ (Learning Rate)ë§Œí¼ ê°€ì¤‘í•˜ì—¬ ì‹¬í™”í•™ìŠµí•¨                  ê²½ì‚¬í•˜ê°•ë²•ê³¼ í•™ìŠµë¥ ì˜ ì´í•´                          eXtreme Gradient Boosting(XGBoost) : GBMì„ ê· í˜• íŠ¸ë¦¬ ë¶„í•  ë°©ì‹ì„ ë³´ì¡´í•˜ë©´ì„œ ë³´ì™„í•œ ì•Œê³ ë¦¬ì¦˜                  Decision Tree                    Gradient Boosting Machine                    ê· í˜• íŠ¸ë¦¬ ë¶„í•  ë°©ì‹(Level-Wise Tree Growth) : ëª¨ë“  ë¦¬í”„ ë…¸ë“œì—ì„œ ê· ë“±í•˜ê²Œ ê°€ì§€ì¹˜ê¸°í•˜ëŠ” ë°©ì‹        &lt;img alt=\"level\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/f99789f7-ee66-4750-8bac-6c5caa9eb8a7\" width=80%&gt;                  ì¥ì  : íŠ¸ë¦¬ í˜•íƒœê°€ ë¹„ëŒ€ì¹­ì ìœ¼ë¡œ í˜•ì„±ë˜ê±°ë‚˜ ê¹Šì´ê°€ ì‹¬í™”ë˜ì§€ ì•Šì•„ ê³¼ëŒ€ì í•©ì„ ë°©ì§€í•  ìˆ˜ ìˆìŒ          ë‹¨ì  : ì†ì‹¤ì„ ìµœì†Œí™”í•˜ì§€ ëª»í•  ê°€ëŠ¥ì„±ì´ ë†’ìŒ                    &lt;/br&gt;xgboost.XGBClassifierfrom xgboost import XGBClassifierğŸ’¡ General HyperParameter  random_state(default : None)  silent(default : 1) : ë™ì‘ ë©”ì‹œì§€ ì¶œë ¥ ì—¬ë¶€ ì„¤ì •  n_jobs(default : None) : ë³‘ë ¬ë¡œ ì‘ì—…í•  ì½”ì–´ ê°¯ìˆ˜ğŸ’¡ Model HyperParameter  objective(default : 'binary:logistic') : ëª©ì  í•¨ìˆ˜ë¡œì„œ GBMì˜ criterion ê³¼ ìœ ì‚¬í•¨          'binary:logistic' : ì´í•­ ë¶„ë¥˜ ì‹œ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ì¸¡ì¹˜ê°€ ê° ë²”ì£¼ì— ì†í•  í™•ë¥ ì„ ë°˜í™˜í•¨      'multi:softprob' : ë‹¤í•­ ë¶„ë¥˜ ì‹œ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ì¸¡ì¹˜ê°€ ê° ë²”ì£¼ì— ì†í•  í™•ë¥ ì„ ë°˜í™˜í•¨      'multi:softmax' : ë‹¤í•­ ë¶„ë¥˜ ì‹œ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ì¸¡ì¹˜ê°€ ì†í•  í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ë²”ì£¼ë¥¼ ë°˜í™˜í•¨        n_estimators(default : 100) : ë™ì›í•  Decision Tree ì˜ ê°œìˆ˜ğŸ’¡ Gradient Descent HyperParameter      learning_rate(default : None) : í•™ìŠµë¥ ë¡œì„œ í†µìƒ $0.1 \\sim 0.2$ ì˜ ê°’ì„ ì„¤ì •í•¨        early_stopping_rounds(default : None) : ì†ì‹¤ì´ ëª‡ íšŒ ì´ìƒ ìœ íš¨í•˜ê²Œ ê°ì†Œí•˜ì§€ ì•Šì„ ê²½ìš° í•™ìŠµì„ ì¤‘ë‹¨í•  ê²ƒì¸ê°€  ğŸ’¡ Pruning      Pre-Pruning          max_depth(default : None) : íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´ë¡œì„œ í†µìƒ $3 \\sim 10$ ì˜ ê°’ì„ ì„¤ì •í•¨      min_child_weight(default : None) : ì–´ë–¤ ë…¸ë“œì— ìœ„ì¹˜í•œ ëª¨ë“  ê´€ì¸¡ì¹˜ì˜ ê°€ì¤‘ì¹˜ í•©ì— ëŒ€í•˜ì—¬, í•´ë‹¹ ë…¸ë“œê°€ í•˜ìœ„ ë…¸ë“œë¡œ ê°€ì§€ì¹˜ê¸°í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ìµœì†Œí•©      max_leaves(default : None) : ë¦¬í”„ ë…¸ë“œì˜ ìµœëŒ€ ê°¯ìˆ˜      gamma(default : None) : í•˜ìœ„ ë…¸ë“œë¡œ ê°€ì§€ì¹˜ê¸° í•˜ê¸° ìœ„í•œ ìµœì†Œ ì†ì‹¤ ê°ì†Œ ê°’      ğŸ’¡ Validation HyperperameterXGBClassifier.fit()      eval_set : êµì°¨ê²€ì¦ ì‹œ ì‚¬ìš©í•  ê²€ì¦ìš© ë°ì´í„° ì„¸íŠ¸        eval_metric : êµì°¨ê²€ì¦ ì‹œ ì‚¬ìš©í•  í‰ê°€ ì§€í‘œ          error : Binary classification error rate(ì„ê³„ê°’ 0.5)      logloss : Negative log-likelihood      merror : multiclass classification error rate      mlogloss : multiclass logloss      auc : area under curve      Index  LightGBM  lightgbm.LGBMClassifier&lt;/br&gt;LightGBM      Ensemble        Boosting : ë‹¨ì¼ ëª¨ë¸ë“¤ì„ ì§ë ¬ë¡œ ì‹¬í™” ì‘ì—…í•˜ë„ë¡ ì„¤ê³„í•˜ëŠ” ë°©ì‹    &lt;img alt=\"boosting\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/020824f7-a8e4-4c67-825d-5c0cd3142f30\" width=80%&gt;                  ì´ì „ ëª¨ë¸ì— ëŒ€í•˜ì—¬ ê·¸ ì†ì‹¤ì´ í° ê´€ì¸¡ì¹˜ë“¤ì„ í•™ìŠµë¥ (Learning Rate)ë§Œí¼ ê°€ì¤‘í•˜ì—¬ ì‹¬í™”í•™ìŠµí•¨                  ê²½ì‚¬í•˜ê°•ë²•ê³¼ í•™ìŠµë¥ ì˜ ì´í•´                          Light Gradient Boosting Machine(LightGBM) : GBMì„ ë¦¬í”„ ì¤‘ì‹¬ íŠ¸ë¦¬ ë¶„í•  ë°©ì‹ìœ¼ë¡œ ë³´ì™„í•œ ì•Œê³ ë¦¬ì¦˜                  Decision Tree                    Gradient Boosting Machine                    ë¦¬í”„ ì¤‘ì‹¬ íŠ¸ë¦¬ ë¶„í•  ë°©ì‹(Leaf-Wise Tree Growth) : ë¦¬í”„ ë…¸ë“œ ì¤‘ ì†ì‹¤ì´ ê°€ì¥ í° ë…¸ë“œë§Œ ì·¨ì‚¬ì„ íƒí•˜ì—¬ ë¶„í• í•˜ëŠ” ë°©ì‹        &lt;img alt=\"leaf\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/f2e375a3-baa6-43e7-ba0f-4a8940581a7f\" width=80%&gt;                  ì¥ì  : ë¦¬í”„ ë…¸ë“œ ì¤‘ ì†ì‹¤ì´ í° ë…¸ë“œë¥¼ ì‹¬í™” ì‘ì—…í•˜ë¯€ë¡œ ì†ì‹¤ ìµœì†Œí™”ì— ë›°ì–´ë‚¨          ë‹¨ì  : íŠ¸ë¦¬ í˜•íƒœê°€ ë¹„ëŒ€ì¹­ì ìœ¼ë¡œ í˜•ì„±ë˜ê±°ë‚˜ ê¹Šì´ê°€ ì‹¬í™”ë˜ì–´ ê³¼ëŒ€ì í•©ë  ê°€ëŠ¥ì„±ì´ ë†’ìŒ                    &lt;/br&gt;lightgbm.LGBMClassifierfrom lightgbm import LGBMClassifierğŸ’¡ General HyperParameter  random_state(default : None)  silent(default : 1) : ë™ì‘ ë©”ì‹œì§€ ì¶œë ¥ ì—¬ë¶€ ì„¤ì •  n_jobs(default : -1) : ë³‘ë ¬ë¡œ ì‘ì—…í•  ì½”ì–´ ê°¯ìˆ˜ğŸ’¡ Model HyperParameter  objective(default : None) : ëª©ì  í•¨ìˆ˜ë¡œì„œ GBMì˜ criterion ê³¼ ìœ ì‚¬í•¨          binary : ì´í•­ ë¶„ë¥˜ ë¶„ì„      multiclass : ë‹¤í•­ ë¶„ë¥˜ ë¶„ì„        n_estimators(default : 100) : ë™ì›í•  Decision Tree ì˜ ê°œìˆ˜ğŸ’¡ Gradient Descent HyperParameter      learning_rate(default : 0.1) : í•™ìŠµë¥ ë¡œì„œ í†µìƒ $0.1 \\sim 0.2$ ì˜ ê°’ì„ ì„¤ì •í•¨        early_stopping_rounds(default : None) : ì†ì‹¤ì´ ëª‡ íšŒ ì´ìƒ ìœ íš¨í•˜ê²Œ ê°ì†Œí•˜ì§€ ì•Šì„ ê²½ìš° í•™ìŠµì„ ì¤‘ë‹¨í•  ê²ƒì¸ê°€  ğŸ’¡ Pruning      Pre-Pruning          max_depth(default : -1) : íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´      num_leaves(default : 31) : ë¦¬í”„ ë…¸ë“œì˜ ìµœëŒ€ ê°¯ìˆ˜      min_child_samples(default : 20) : ë¦¬í”„ ë…¸ë“œ ê´€ì¸¡ì¹˜ì˜ ìµœì†Œ ê°¯ìˆ˜      min_child_weight(default : 0.001) : ì–´ë–¤ ë…¸ë“œì— ìœ„ì¹˜í•œ ëª¨ë“  ê´€ì¸¡ì¹˜ì˜ ê°€ì¤‘ì¹˜ í•©ì— ëŒ€í•˜ì—¬, í•´ë‹¹ ë…¸ë“œê°€ í•˜ìœ„ ë…¸ë“œë¡œ ê°€ì§€ì¹˜ê¸°í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ìµœì†Œí•©      min_split_gain(default : 0.0) : í•˜ìœ„ ë…¸ë“œë¡œ ê°€ì§€ì¹˜ê¸° í•˜ê¸° ìœ„í•œ ìµœì†Œ ì†ì‹¤ ê°ì†Œ ê°’      ğŸ’¡ To Prevent Underfitting      About Imbalance in the number of observations between categories                  class_weight(default : None) : ê°€ì¤‘í•  ë²”ì£¼ì™€ ê·¸ ê°’                  'balanced'          dictionary type                    ğŸ’¡ Validation HyperperameterLGBMClassifier.fit()      eval_set : êµì°¨ê²€ì¦ ì‹œ ì‚¬ìš©í•  ê²€ì¦ìš© ë°ì´í„° ì„¸íŠ¸        eval_metric : êµì°¨ê²€ì¦ ì‹œ ì‚¬ìš©í•  í‰ê°€ ì§€í‘œ          error : Binary classification error rate(ì„ê³„ê°’ 0.5)      logloss : Negative log-likelihood      merror : multiclass classification error rate      mlogloss : multiclass logloss      auc : area under curve      "
  },
  
  {
    "title": "Bagging",
    "url": "/posts/Bagging/",
    "categories": "",
    "tags": "",
    "date": "2024-01-20 00:00:00 +0900",
    





    
    "snippet": "Index  Random Forest  sklearn.ensemble.RandomForestClassifier&lt;/br&gt;Random Forest      Ensemble        Bagging(Bootstrap Aggregating) : ë‹¨ì¼ ëª¨ë¸ë“¤ì„ ë³‘ë ¬ë¡œ ì‘ì—…í•˜ë„ë¡ ì„¤ê³„í•˜ëŠ” ë°©ì‹    &lt;img alt=\"bagging\" src=\"h...",
    "content": "Index  Random Forest  sklearn.ensemble.RandomForestClassifier&lt;/br&gt;Random Forest      Ensemble        Bagging(Bootstrap Aggregating) : ë‹¨ì¼ ëª¨ë¸ë“¤ì„ ë³‘ë ¬ë¡œ ì‘ì—…í•˜ë„ë¡ ì„¤ê³„í•˜ëŠ” ë°©ì‹    &lt;img alt=\"bagging\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/901c311e-f4bf-4df5-901a-88873b1a3e66\" width=80%&gt;        Random Forest : Decision Tree ì— ê¸°ë°˜í•œ Bagging Ensemble Algorithm    &lt;img alt=\"ëœë¤í¬ë ˆìŠ¤íŠ¸\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/675e25fe-d139-4d0f-8299-f2220661d28a\" width=80%&gt;          Decision Tree      &lt;/br&gt;sklearn.ensemble.RandomForestClassifierfrom sklearn.ensemble import RandomForestClassifierğŸ’¡ General HyperParameter  random_state(default : None)  warm_start(default : False) : ë™ì‘ ë©”ì‹œì§€ ì¶œë ¥ ì—¬ë¶€ ì„¤ì •  n_jobs(default : None) : ë³‘ë ¬ë¡œ ì‘ì—…í•  ì½”ì–´ ê°¯ìˆ˜ğŸ’¡ Model HyperParameter  criterion(default : 'gini') : ê· ì¼ë„ ì¸¡ì • ë°©ë²•          'gini' : ì§€ë‹ˆì§€ìˆ˜      'entropy' : ì—”íŠ¸ë¡œí”¼ì§€ìˆ˜            max_features(default : 'sqrt') : ê·œì¹™ ì„¤ê³„ ì‹œ ê³ ë ¤í•  ì„¤ëª…ë³€ìˆ˜ ê°¯ìˆ˜          'sqrt' : $\\sqrt{n}$      'log2' : $\\log_2{n}$      None : $n$        n_estimators(default : 100) : ë™ì›í•  Decision Tree ì˜ ê°œìˆ˜ğŸ’¡ Pruning  Pre-Pruning          max_depth(default : None) : íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´      max_leaf_nodes(default : None) : ë¦¬í”„ ë…¸ë“œì˜ ìµœëŒ€ ê°¯ìˆ˜      min_samples_split(default : 2) : í•˜ìœ„ ë…¸ë“œë¡œ ê°€ì§€ë¥¼ ë»—ê¸° ìœ„í•´ í•„ìš”í•œ ìµœì†Œí•œì˜ ìƒ˜í”Œ ê°œìˆ˜      min_impurity_decrease(default : 0) : í•˜ìœ„ ë…¸ë“œë¡œ ê°€ì§€ë¥¼ ë»—ê¸° ìœ„í•´ í•„ìš”í•œ ìµœì†Œí•œì˜ ë¶ˆìˆœë„ ê°œì„  ì •ë„      min_samples_leaf(default : 1) : ë¦¬í”„ ë…¸ë“œê°€ ë˜ê¸° ìœ„í•´ í•„ìš”í•œ ìµœì†Œí•œì˜ ìƒ˜í”Œ ê°œìˆ˜            Cost-Complexity Pruning          ccp_alpha(default : 0)      ğŸ’¡ To Prevent Underfitting      About Imbalance in the number of observations between categories                  class_weight(default : None) : ê°€ì¤‘í•  ë²”ì£¼ì™€ ê·¸ ê°’                  'balanced'          dictionary type                    "
  },
  
  {
    "title": "Ensemble",
    "url": "/posts/Ensemble/",
    "categories": "",
    "tags": "",
    "date": "2024-01-19 00:00:00 +0900",
    





    
    "snippet": "Index  ëª¨ë¸ ë³µì¡ë„ì™€ ê³¼ì í•©ì˜ ìƒê´€ê´€ê³„  ë¶„ì‚°, í¸í–¥ê³¼ ê³¼ì í•©ì˜ ìƒê´€ê´€ê³„  ì •í˜•í™”ì™€ ê·œì œ  sklearn.linear_model.Lasso &amp; Ridge&lt;/br&gt;ì˜¤ì°¨ì™€ í¸í–¥, ë¶„ì‚° ê°„ ìƒê´€ê´€ê³„ ì´í•´      ì¡ìŒì˜ ì´í•´\\[\\begin{aligned}  y = f(x) + \\varepsilon  \\end{aligned}\\]    ...",
    "content": "Index  ëª¨ë¸ ë³µì¡ë„ì™€ ê³¼ì í•©ì˜ ìƒê´€ê´€ê³„  ë¶„ì‚°, í¸í–¥ê³¼ ê³¼ì í•©ì˜ ìƒê´€ê´€ê³„  ì •í˜•í™”ì™€ ê·œì œ  sklearn.linear_model.Lasso &amp; Ridge&lt;/br&gt;ì˜¤ì°¨ì™€ í¸í–¥, ë¶„ì‚° ê°„ ìƒê´€ê´€ê³„ ì´í•´      ì¡ìŒì˜ ì´í•´\\[\\begin{aligned}  y = f(x) + \\varepsilon  \\end{aligned}\\]                  $y$ : ê´€ì¸¡ì¹˜ì˜ ì‹¤ì œê°’ìœ¼ë¡œì„œ ëª¨ìˆ˜                    $f(x)$ : êµ¬í˜„í•˜ê³ ì í•˜ëŠ” ê´€ì¸¡ì¹˜ ìƒì„± í•¨ìˆ˜ë¡œì„œ ëª¨ìˆ˜ì˜ ì¶”ì •ëŸ‰                    $\\varepsilon$ : ëª¨ìˆ˜ë¥¼ ì •í™•í•˜ê²Œ ì¶”ì •í•˜ì§€ ëª»í•˜ëŠ” ì›ì¸ìœ¼ë¡œì„œ ì¡ìŒ(Noise)                  $\\varepsilon \\sim N(0,\\sigma^2)$ ê°€ì •                          ì˜¤ì°¨ì˜ êµ¬ì„±\\[\\begin{aligned}  Err  &amp;= E[\\varepsilon^2]  &amp;= E  \\end{aligned}\\]  \\[\\begin{aligned}Err&amp;= E[(y-\\hat{y})^2]\\\\&amp;= E[(f(x)+\\varepsilon-\\hat{y})^2]\\\\&amp;= E[(f(x)-\\hat{y})^2+\\varepsilon^2+2\\varepsilon(f(x)-\\hat{y})]\\\\&amp;= E[(f(x)-\\hat{y})^2]+E[\\varepsilon^2]+2 \\cdot E[\\varepsilon] \\cdot E[f(x)-\\hat{y}]\\\\&amp;= E[(f(x)-\\hat{y})^2] + \\sigma^2 \\quad (\\because \\varepsilon \\sim N(0,\\sigma^2))\\\\\\\\E[(f(x)-\\hat{y})^2]&amp;= E[(f(x)-\\overline{y}+\\overline{y}-\\hat{y})^2]\\\\&amp;= E[(f(x)-\\overline{y})^2+(\\overline{y}-\\hat{y})^2+2 \\cdot (f(x)-\\overline{y}) \\cdot (\\overline{y}-\\hat{y})]\\\\&amp;= E[(f(x)-\\overline{y})^2]+E[(\\overline{y}-\\hat{y})^2]+2 \\cdot E[f(x)-\\overline{y}] \\cdot E[\\overline{y}-\\hat{y}]\\\\&amp;= E[(f(x)-\\overline{y})^2]+E[(\\overline{y}-\\hat{y})^2] \\quad (\\because E[f(x)-\\overline{y}] = 0)\\\\\\\\Bias[\\hat{y}]&amp;= E[\\hat{y}-f(x)]\\\\Var[\\hat{y}]&amp;= E[(\\hat{y}-E[\\hat{y}])^2]\\\\&amp;= E[(\\overline{y}-\\hat{y})^2]\\\\\\\\\\therefore Err&amp;= Bias[\\hat{y}]^2 + Var[\\hat{y}] + \\sigma^2\\end{aligned}\\]ëª¨ë¸ ë³µì¡ë„ì™€ ê³¼ì í•©ì˜ ìƒê´€ê´€ê³„&lt;img alt=\"ê³¼ì í•©\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/8acf85ff-c937-41e5-9633-1613c3b4c0f7\" width=80%&gt;  ëª¨ë¸ì„ ë‹¨ìˆœí•˜ê²Œ ì„¤ê³„í• ìˆ˜ë¡ ëª¨ë¸ì´ ê´€ì¸¡ì¹˜ì˜ ì „ë°˜ì ì¸ íŠ¹ì§•ì„ ì œëŒ€ë¡œ í¬ì°©í•˜ì§€ ëª»í•´ ê³¼ì†Œì í•©(UnderFitting) ë¨  ëª¨ë¸ì„ ë³µì¡í•˜ê²Œ ì„¤ê³„í• ìˆ˜ë¡ ëª¨ë¸ì´ í‘œë³¸ì—ë§Œ êµ­í•œë˜ì–´ ë‚˜íƒ€ë‚˜ëŠ” ìƒì„¸í•œ íŠ¹ì§•ê¹Œì§€ í¬ì°©í•˜ê²Œ ë˜ì–´ í‘œë³¸ì— ê³¼ëŒ€ì í•©(OverFitting) ë¨&lt;/br&gt;ë¶„ì‚°, í¸í–¥ê³¼ ëª¨ë¸ ë³µì¡ë„ì˜ ìƒê´€ê´€ê³„  ì†ì‹¤í•¨ìˆ˜ì˜ ë¶„í•´                  ì„ í˜•íšŒê·€ì˜ ëª©ì í•¨ìˆ˜ì¸ ì”ì°¨ ììŠ¹ì˜ ê¸°ëŒ€ê°’ì„ $MSE$ ë¡œ ì •ì˜í•¨\\[\\begin{aligned}  \\min_{\\beta_0, \\beta_j^{\\forall}} {MSE}  \\end{aligned}\\]                    $MSE$ ë¥¼ ì¶”ì •ëŸ‰ $\\hat{Y}$ ì˜ ë¶„ì‚° $Var(\\hat{Y})$ ì™€ í¸í–¥ $Bias(\\hat{Y})$ ìœ¼ë¡œ ì–‘ë¶„í•  ìˆ˜ ìˆìŒ\\[\\begin{aligned}  MSE  &amp;= E[(Y-\\hat{Y})^2] \\\\  &amp;= Var(\\hat{Y}) + (Bias(\\hat{Y}))^2 \\\\  &amp;= Var(\\hat{Y}) + \\{(E(\\hat{Y})-Y)^2\\}  \\end{aligned}\\]                  ë¶„ì‚°ê³¼ í¸í–¥ì˜ ì´í•´          $Var(\\hat{Y})$ : ì¶”ì •ëŸ‰ì˜ ê¸°ëŒ€ê°’ $E(\\hat{Y})$ ê³¼ ì¶”ì •ì¹˜ $\\hat{y_i^{\\forall}}$ ì˜ ì°¨ì´      $Bias(\\hat{Y})$ : ëª¨ìˆ˜ $Y$ ì™€ ê·¸ ì¶”ì •ëŸ‰ì˜ ê¸°ëŒ€ê°’ $E(\\hat{Y})$ ì˜ ì°¨ì´        &lt;img alt=\"ë¶„ì‚°ê³¼í¸í–¥ì˜ì´í•´\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/cb542dc8-172e-4b66-9c09-93abdcbfec1a\" width=80%&gt;        ë¶„ì‚°, í¸í–¥ê³¼ ëª¨ë¸ ë³µì¡ë„ì˜ ê´€ê³„    &lt;img alt=\"ë³µì¡ë„\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/05e615db-1162-44ea-854b-11bebfc1c7a9\" width=80%&gt;          ëª¨ë¸ì„ ë³µì¡í•˜ê²Œ ì„¤ê³„í• ìˆ˜ë¡ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ëŠ” ê°ì†Œí•˜ëŠ” ë°˜ë©´, ì˜ˆì¸¡ê°’ê³¼ ê·¸ í‰ê· ì˜ ì°¨ì´ëŠ” ì¦ê°€í•¨      ëª¨ë¸ì„ ë‹¨ìˆœí•˜ê²Œ ì„¤ê³„í• ìˆ˜ë¡ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ëŠ” ì¦ê°€í•˜ëŠ” ë°˜ë©´, ì˜ˆì¸¡ê°’ê³¼ ê·¸ í‰ê· ì˜ ì°¨ì´ëŠ” ê°ì†Œí•¨      ì¦‰, ëª¨ë¸ ë³µì¡ë„ëŠ” í¸í–¥ê³¼ ìŒì˜ ìƒê´€ê´€ê³„, ë¶„ì‚°ê³¼ ì–‘ì˜ ìƒê´€ê´€ê³„ì— ìˆìŒ      &lt;/br&gt;ì •í˜•í™”ì™€ ê·œì œ&lt;img alt=\"ì •í˜•í™”\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/84cf465f-57ee-4b7e-8b04-04c1d80eff81\" width=80%&gt;      ì •í˜•í™”(Regularization) : ëª¨ë¸ì´ í‘œë³¸ì— ê³¼ì í•©ë˜ì§€ ì•„ë‹ˆí•˜ê³  ì¼ë°˜í™”ë  ìˆ˜ ìˆë„ë¡ ê·œì œí•˜ëŠ” ì‘ì—…\\[\\begin{aligned}  \\min_{\\beta_0, \\beta_j^{\\forall}}(MSE + Penalty)  &amp;= \\min_{\\beta_0, \\beta_j^{\\forall}}(E[(Y-\\hat{Y})^2] + Penalty) \\\\  &amp;= \\min_{\\beta_0, \\beta_j^{\\forall}}(Var(\\hat{Y}) + (Bias(\\hat{Y}))^2 + Penalty)  \\end{aligned}\\]          ìœ„ ë…¼ì˜ë¥¼ ì¢…í•©í•˜ë©´ í¸í–¥ì´ í´ìˆ˜ë¡ ëª¨ë¸ì´ ë‹¨ìˆœí•˜ê²Œ ì„¤ê³„ë˜ì–´ ê³¼ì†Œì í•©ë  ê°€ëŠ¥ì„±ì´ ë†’ìŒ      ë°˜ëŒ€ë¡œ ë¶„ì‚°ì´ í´ìˆ˜ë¡ ëª¨ë¸ì´ ë³µì¡í•˜ê²Œ ì„¤ê³„ë˜ì–´ í‘œë³¸ì— ê³¼ëŒ€ì í•©ë  ê°€ëŠ¥ì„±ì´ ë†’ìŒ      ëª¨ë¸ì„ ì¼ë°˜í™”í•˜ê¸° ìœ„í•´ í¸í–¥ê³¼ ë¶„ì‚°ì„ í¬ê´„í•˜ëŠ” $MSE$ ì— ê°€ì¤‘ì¹˜ì— ê´€í•œ ê·œì œ $Penalty$ ë¥¼ ë”í•˜ì—¬ ì˜ˆì¸¡ ììœ ë„ë¥¼ ì œí•œí•¨            ê·œì œ(Penalty)ì˜ ì¢…ë¥˜    &lt;img alt=\"ê·œì œë¹„êµ\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/5766884b-64dc-4099-a69f-6b295482458a\" width=80%&gt;                  L-1(Least Absolute Shrinkage and Selection Operator; LASSO)\\[\\begin{aligned}  Penalty  &amp;= \\alpha ||\\overrightarrow{w}||_{L1} \\\\  &amp;= \\alpha \\times (|\\beta_1| + |\\beta_2| + \\cdots + |\\beta_n|) \\\\  &amp;= \\alpha \\displaystyle\\sum_{i=1}^{n}{|\\beta_i|}  \\end{aligned}\\]                    L-2(Ridge)\\[\\begin{aligned}  Penalty  &amp;= \\alpha ||\\overrightarrow{w}||_{L2} \\\\  &amp;= \\alpha \\times \\{(\\beta_1)^2 + (\\beta_2)^2 + \\cdots + (\\beta_n)^2\\} \\\\  &amp;= \\alpha \\displaystyle\\sum_{i=1}^{n}{(\\beta_i)^2}  \\end{aligned}\\]            Index  Ensemble  Voting  sklearn.ensemble.VotingClassifier&lt;/br&gt;Ensemble      Ensemble : ë‹¨ì¼ ëª¨ë¸ë“¤ì„ ê²°í•©í•˜ëŠ” ê¸°ë²•        êµ¬ë¶„                  Voting : ë‹¨ì¼ ëª¨ë¸ë“¤ì„ ìˆ˜ë™ìœ¼ë¡œ ê²°í•©í•˜ëŠ” ë°©ì‹        &lt;img alt=\"ë³´íŒ…\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/505a65f5-cebd-4a62-a02d-3a6a7d19b066\" width=80%&gt;                    Bagging(Bootstrap Aggregating) : ë‹¨ì¼ ëª¨ë¸ë“¤ì„ ë³‘ë ¬ë¡œ ì‘ì—…í•˜ë„ë¡ ì„¤ê³„í•˜ëŠ” ë°©ì‹        &lt;img alt=\"bagging\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/901c311e-f4bf-4df5-901a-88873b1a3e66\" width=80%&gt;                    Boosting : ë‹¨ì¼ ëª¨ë¸ë“¤ì„ ì§ë ¬ë¡œ ì‹¬í™” ì‘ì—…í•˜ë„ë¡ ì„¤ê³„í•˜ëŠ” ë°©ì‹        &lt;img alt=\"boosting\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/020824f7-a8e4-4c67-825d-5c0cd3142f30\" width=80%&gt;            &lt;/br&gt;Voting      Hard Voting : ìµœë¹ˆê°’ìœ¼ë¡œ ê´€ì¸¡ì¹˜ì˜ ë²”ì£¼ë¥¼ ì„ ì •í•˜ëŠ” ë°©ì‹    &lt;img alt=\"í•˜ë“œë³´íŒ…\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/115cdb86-1d0d-4cd9-adc7-136798eed8d1\" width=80%&gt;        Soft Voting : ê° ë²”ì£¼ì— ì†í•  í™•ë¥ ì— ëŒ€í•œ í‰ê· ì´ ê°€ì¥ í° ë²”ì£¼ë¡œ ê´€ì¸¡ì¹˜ì˜ ë²”ì£¼ë¥¼ ì„ ì •í•˜ëŠ” ë°©ì‹    &lt;img alt=\"ì†Œí”„íŠ¸ë³´íŒ…\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/fd6eaad0-54fa-403e-9957-345999429e6e\" width=80%&gt;  &lt;/br&gt;sklearn.ensemble.VotingClassifierfrom sklearn.svm import SVCfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.ensemble import VotingClassifierestimator_list = [    (\"Support Vector Machine\", SVC()),    (\"k-NN\", KNeighborsClassifier()),    (\"Logistic Regression\", LogisticRegression())    ]voting_clf = VotingClassifier(    n_jobs = -1,    estimators = estimator_list,    voting = 'soft'    )ğŸ’¡ General HyperParameter  n_jobs(default : None) : ë³‘ë ¬ë¡œ ì‘ì—…í•  ì½”ì–´ ê°¯ìˆ˜ğŸ’¡ Model HyperParameter      estimators : ë™ì›í•  ì•Œê³ ë¦¬ì¦˜ ëª©ë¡        weights(default : None) : ê° ì•Œê³ ë¦¬ì¦˜ë³„ ê°€ì¤‘ì¹˜        voting(default : 'hard') : ê²°í•© ë°©ì‹          'hard' : Hard Voting      'soft' : Soft Voting      &lt;/br&gt;&lt;hr&gt;ì´ë¯¸ì§€ ì¶œì²˜  https://velog.io/@jochedda"
  },
  
  {
    "title": "Hierarchical_clustering",
    "url": "/posts/Hierarchical_Clustering/",
    "categories": "",
    "tags": "",
    "date": "2024-01-18 00:00:00 +0900",
    





    
    "snippet": "Index  What? Hierarchical Clustering  sklearn.cluster.AgglomerativeClustering&lt;/br&gt;What? Hierarchical ClusteringğŸ’¡ Hierarchical Clustering      ì •ì˜ : ê³„ì¸µì  íŠ¸ë¦¬ëª¨í˜•ì„ í™œìš©í•˜ì—¬ ê°œë³„ ê°œì²´ë“¤ì„ ìœ ì‚¬í•œ ê°œì²´/êµ°ì§‘ê³¼ ê³„ì¸µì ìœ¼ë¡œ í†µí•©í•˜ê±°...",
    "content": "Index  What? Hierarchical Clustering  sklearn.cluster.AgglomerativeClustering&lt;/br&gt;What? Hierarchical ClusteringğŸ’¡ Hierarchical Clustering      ì •ì˜ : ê³„ì¸µì  íŠ¸ë¦¬ëª¨í˜•ì„ í™œìš©í•˜ì—¬ ê°œë³„ ê°œì²´ë“¤ì„ ìœ ì‚¬í•œ ê°œì²´/êµ°ì§‘ê³¼ ê³„ì¸µì ìœ¼ë¡œ í†µí•©í•˜ê±°ë‚˜, í‘œë³¸ì„ ìœ ì˜ë¯¸í•˜ê²Œ êµ¬ë¶„ë˜ëŠ” ì§€ì ì—ì„œ ê³„ì¸µì ìœ¼ë¡œ ë¶„í• í•´ê°€ëŠ” ì•Œê³ ë¦¬ì¦˜        ë´ë“œë¡œê·¸ë¨(Dendrogram) : ê²°í•© í˜¹ì€ ë¶„í• í•˜ëŠ” ìˆœì„œë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê³„ì¸µì  íŠ¸ë¦¬ëª¨í˜•    &lt;img alt=\"ë´ë“œë¡œê·¸ë¨\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/d9fc5800-1012-4dfe-95f3-19bd62aac7f4\" width=80%&gt;        ì¢…ë¥˜    &lt;img alt=\"ì¢…ë¥˜\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/23998ba5-6998-4317-bfa2-2380a82b9590\" width=80%&gt;                  ìƒí–¥ì‹ êµ°ì§‘í™”(Agglomerative Clustering) : ê°œë³„ ê°œì²´ë“¤ì„ ìœ ì‚¬í•œ ê°œì²´/êµ°ì§‘ê³¼ ê³„ì¸µì ìœ¼ë¡œ í†µí•©í•´ê°€ëŠ” ë°©ì‹                    í•˜í–¥ì‹ êµ°ì§‘í™”(Divisive Clustering) : í‘œë³¸ì„ ìœ ì˜ë¯¸í•˜ê²Œ êµ¬ë¶„ë˜ëŠ” ì§€ì ë§ˆë‹¤ ê³„ì¸µì ìœ¼ë¡œ ë¶„í• í•´ê°€ëŠ” ë°©ì‹            ğŸ’¡ How to Agglomerative Clustering      ëª¨ë“  ê°œì²´ë¥¼ ê°œë³„ êµ°ì§‘ìœ¼ë¡œì„œ ì •ì˜í•¨\\[C_{i} = \\{\\overrightarrow{x}_{i}\\} \\quad \\text{for} \\quad i=1,2,\\cdots, n\\]        êµ°ì§‘ ê°„ ê±°ë¦¬ í–‰ë ¬ì„ ê³„ì‚°í•¨\\[\\mathbf{D}_{i,j}=d(C_{i},C_{j})\\]        ê°€ì¥ ê°€ê¹Œìš´ ë‘ ê°œì˜ êµ°ì§‘ì„ í•˜ë‚˜ì˜ êµ°ì§‘ìœ¼ë¡œ í†µí•©í•¨\\[\\begin{aligned} C_{k}&amp;=\\hat{C}_{i} \\cup \\hat{C}_{j}\\\\ \\hat{C}_{i},\\hat{C}_{j}&amp;=\\argmin_{C_{i},C_{j}}{d(C_{i},C_{j})} \\end{aligned}\\]        êµ°ì§‘ ê°„ ê±°ë¦¬ í–‰ë ¬ì„ ê°±ì‹ í•¨\\[\\mathbf{D}_{N} = \\mathbf{D}_{N-1} \\quad \\text{Recalculate} \\quad d(C_{i^{\\forall} \\ne k},C_{k})\\]        ëª¨ë“  ê°œì²´ê°€ í•˜ë‚˜ì˜ êµ°ì§‘ìœ¼ë¡œ í†µí•©ë  ë•Œê¹Œì§€ â‘¢, â‘£ì˜ ê³¼ì •ì„ ë°˜ë³µí•¨  ğŸ’¡ How to Calculate Distance      Single Linkage(Minimum Distance) : ê° êµ°ì§‘ì— ì†í•œ ê°œì²´ë“¤ ì‚¬ì´ ê±°ë¦¬ ìµœì†Œê°’\\[\\begin{aligned}  d(\\mathbf{A},\\mathbf{B})  &amp;= \\min_{\\overrightarrow{a} \\in \\mathbf{A},\\overrightarrow{b} \\in \\mathbf{B}}{d(\\overrightarrow{a},\\overrightarrow{b})}  \\end{aligned}\\]        Complete Linkage(Maximum Distance) : ê° êµ°ì§‘ì— ì†í•œ ê°œì²´ë“¤ ì‚¬ì´ ê±°ë¦¬ ìµœëŒ€ê°’\\[\\begin{aligned}  d(\\mathbf{A},\\mathbf{B})  &amp;= \\max_{\\overrightarrow{a} \\in \\mathbf{A},\\overrightarrow{b} \\in \\mathbf{B}}{d(\\overrightarrow{a},\\overrightarrow{b})}  \\end{aligned}\\]        Average Linkage(Mean Distance) : ê° êµ°ì§‘ì— ì†í•œ ê°œì²´ë“¤ ì‚¬ì´ ê±°ë¦¬ í‰ê· \\[\\begin{aligned}  d(\\mathbf{A},\\mathbf{B})  &amp;= \\sum_{\\overrightarrow{a} \\in \\mathbf{A}}\\sum_{\\overrightarrow{b} \\in \\mathbf{B}}{d(\\overrightarrow{a},\\overrightarrow{b})}  \\end{aligned}\\]        Centroid Linkage(Distance Between Centroids) : ê° êµ°ì§‘ ì¤‘ì‹¬ ê°„ ê±°ë¦¬\\[\\begin{aligned}  d(\\mathbf{A},\\mathbf{B})  &amp;= d(\\overrightarrow{\\mu}_{A},\\overrightarrow{\\mu}_{B})\\\\  \\overrightarrow{\\mu}_{A}  &amp;= \\frac{1}{|\\mathbf{A}|}\\sum_{\\overrightarrow{a} \\in \\mathbf{A}}{\\overrightarrow{a}}\\\\  \\overrightarrow{\\mu}_{B}  &amp;= \\frac{1}{|\\mathbf{B}|}\\sum_{\\overrightarrow{b} \\in \\mathbf{B}}{\\overrightarrow{b}}  \\end{aligned}\\]        Wardâ€™s Method : ë³‘í•© í›„ SSEì™€ ë³‘í•© ì „ ê°œë³„ êµ°ì§‘ì˜ SSEì˜ í•©ì˜ ì°¨\\[\\begin{aligned}  d(\\mathbf{A},\\mathbf{B})  &amp;= \\sum_{\\overrightarrow{c} \\in \\mathbf{C}}{d(\\overrightarrow{c},\\overrightarrow{\\mu}_{C})} - \\Big[\\sum_{\\overrightarrow{a} \\in \\mathbf{A}}{d(\\overrightarrow{a},\\overrightarrow{\\mu}_{A})} + \\sum_{\\overrightarrow{b} \\in \\mathbf{B}}{d(\\overrightarrow{b},\\overrightarrow{\\mu}_{B})}\\Big]\\\\  \\mathbf{C}  &amp;= \\mathbf{A} \\cup \\mathbf{B}  \\end{aligned}\\]  &lt;/br&gt;sklearn.cluster.AgglomerativeClusteringfrom sklearn.cluster import AgglomerativeClustering      n_clusters(default : 2) : ìƒì„±í•  í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜        distance_threshold(default : None) : ë³‘í•©í•˜ê¸° ìœ„í•œ í´ëŸ¬ìŠ¤í„° ê°„ ê±°ë¦¬ ì„ê³„ê°’    linkage(default : 'ward'): í´ëŸ¬ìŠ¤í„° ê°„ ê±°ë¦¬ ê³„ì‚° ë°©ì‹          'ward'      'complete'      'average'      'single'        affinity(default : 'euclidean'): ê´€ì¸¡ì¹˜ ê°„ ê±°ë¦¬ ê³„ì‚° ë°©ì‹          'manhattan' or 'l1'      'euclidean' or 'l2'      'cosine'      'precomputed'        memory(default : None) : ê³„ì‚°ëœ ê±°ë¦¬ í–‰ë ¬ì„ ì €ì¥í•  ìœ„ì¹˜&lt;/br&gt;&lt;hr&gt;ì´ë¯¸ì§€ ì¶œì²˜      https://towardsdatascience.com/hierarchical-clustering-explained-e59b13846da8        https://harshsharma1091996.medium.com/hierarchical-clustering-996745fe656b  "
  },
  
  {
    "title": "Dbscan",
    "url": "/posts/DBSCAN/",
    "categories": "",
    "tags": "",
    "date": "2024-01-17 00:00:00 +0900",
    





    
    "snippet": "Index  Density-Based Spatial Clustering of Applications with Noise  ìš©ì–´ì˜ ì´í•´  sklearn.cluster.DBSCAN&lt;/br&gt;Density-Based Spatial Clustering of Applications with Noise      ì •ì˜ : ë°€ë„ ê¸°ë°˜ ë°°íƒ€ì  ë¶„ë¦¬í˜• êµ°ì§‘í™” ...",
    "content": "Index  Density-Based Spatial Clustering of Applications with Noise  ìš©ì–´ì˜ ì´í•´  sklearn.cluster.DBSCAN&lt;/br&gt;Density-Based Spatial Clustering of Applications with Noise      ì •ì˜ : ë°€ë„ ê¸°ë°˜ ë°°íƒ€ì  ë¶„ë¦¬í˜• êµ°ì§‘í™” ì•Œê³ ë¦¬ì¦˜    &lt;img alt=\"vectors\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/26b9d379-207e-4822-a5f5-17876ad16271\" width=80%&gt;        êµ°ì§‘ : ì‚¬ì „ì— ì£¼ì–´ì§„ $\\varepsilon, \\text{MinPts}$ ì— ê¸°ì´ˆí–ˆì„ ë•Œ Maximality, Connectivity ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” Non-Empty Subset    &lt;img alt=\"dbscan\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/66dd2174-9d1a-4316-8d00-9c7bcc144187\" width=80%&gt;                  Maximality                  í‘œë³¸ $D$ ì— ì†í•˜ëŠ” ê´€ì¸¡ì¹˜ ë²¡í„° $\\overrightarrow{p}, \\overrightarrow{q}$ ì— ëŒ€í•˜ì—¬,&lt;/br&gt; $\\overrightarrow{p} \\in C \\subseteq D$ ì´ê³ , $\\overrightarrow{q}$ ê°€ $\\overrightarrow{p}$ ë¡œë¶€í„° ë°€ë„ ê¸°ì¤€ ë„ë‹¬ ê°€ëŠ¥í•œ(Directly Density-Reachable) ë²¡í„°ì´ë©´&lt;/br&gt; $\\overrightarrow{q} \\in C$ ì„                            Connectivity                  êµ°ì§‘ $C$ ì— ì†í•˜ëŠ” ê´€ì¸¡ì¹˜ ë²¡í„° $\\overrightarrow{p}, \\overrightarrow{q}$ ê°„ì—ëŠ” ë°€ë„ ê¸°ì¤€ ì—°ê²°ë˜ì–´ ìˆìŒ(Density-Connected)                    &lt;/br&gt;ìš©ì–´ì˜ ì´í•´      $\\varepsilon$-neighborhood of a point    &lt;img alt=\"ì´ì›ƒ\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/7bc4e4c4-8ceb-419d-bcc2-b5d155c5437c\" width=80%&gt;          í‘œë³¸ $D$ ì— ì†í•˜ëŠ” ê´€ì¸¡ì¹˜ ë²¡í„° $\\overrightarrow{p}$ ì— ëŒ€í•˜ì—¬,&lt;/br&gt;$\\overrightarrow{p}$ ì˜ ì´ì›ƒ ì§‘í•© $N_{\\varepsilon}(\\overrightarrow{p})$ ì€ $\\overrightarrow{p}$ ì™€ì˜ ê±°ë¦¬ê°€ $\\varepsilon$ ì´í•˜ì¸ ê´€ì¸¡ì¹˜ ë²¡í„° $\\overrightarrow{q}$ ì˜ ì§‘í•©ì„    \\[N_{\\varepsilon}(\\overrightarrow{p})  =\\{\\overrightarrow{q} \\in D \\big| d(\\overrightarrow{p},\\overrightarrow{q}) \\le \\varepsilon\\}\\]        Directly Density-Reachable    &lt;img alt=\"directly\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/b9fbff94-7204-4487-a216-14497414ff7a\" width=80%&gt;          Core Point Condition ì„ ë§Œì¡±í•˜ëŠ” ê´€ì¸¡ì¹˜ ë²¡í„° $\\overrightarrow{p} \\in D$ ì— ëŒ€í•˜ì—¬,&lt;/br&gt;ê·¸ ì´ì›ƒ ê´€ì¸¡ì¹˜ ë²¡í„°($\\varepsilon$-neighborhood of a point) $\\overrightarrow{q}$ ëŠ” $\\overrightarrow{p}$ ë¡œë¶€í„° ë°€ë„ ê¸°ì¤€ ì§ì ‘ ë„ë‹¬ ê°€ëŠ¥í•œ(Directly Density-Reachable) ê´€ì¸¡ì¹˜ ë²¡í„°ì„                      Core Point Condition\\[|N_{\\varepsilon}(\\overrightarrow{p})| \\ge \\text{MinPts}\\]                    Reachability\\[\\overrightarrow{q} \\in N_{\\varepsilon}(\\overrightarrow{p})\\]                  Density-Reachable    &lt;img alt=\"non-directly\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/7cf67916-6655-4c97-881c-a65d34154ea1\" width=80%&gt;          Core Point Condition ì„ ë§Œì¡±í•˜ëŠ” ê´€ì¸¡ì¹˜ ë²¡í„° $\\overrightarrow{p} \\in D$ ì— ëŒ€í•˜ì—¬,&lt;/br&gt; $\\overrightarrow{p}$ ì™€ $\\overrightarrow{q}$ ì‚¬ì´ì— $\\overrightarrow{p}$ ë¡œë¶€í„° ë°€ë„ ê¸°ì¤€ ì§ì ‘ ë„ë‹¬ ê°€ëŠ¥í•œ ê´€ì¸¡ì¹˜ ë²¡í„° $\\overrightarrow{x}{1},\\overrightarrow{x}{2},\\cdots,\\overrightarrow{x}_{n}$ ì´ ì—°ì‡„ì ìœ¼ë¡œ ì¡´ì¬í•œë‹¤ë©´,&lt;/br&gt; $\\overrightarrow{q}$ ëŠ” $\\overrightarrow{p}$ ë¡œë¶€í„° ë°€ë„ ê¸°ì¤€ ë„ë‹¬ ê°€ëŠ¥í•œ(Density-Reachable) ê´€ì¸¡ì¹˜ ë²¡í„°ì„                                                          $              N_{\\varepsilon}(\\overrightarrow{p})              \\ge \\text{MinPts}$                                          $\\overrightarrow{x}{1} \\in N{\\varepsilon}(\\overrightarrow{p})$                                                  $              N_{\\varepsilon}(\\overrightarrow{x}_{\\forall})              \\ge \\text{MinPts}$                                          $\\overrightarrow{x}{i+1} \\in N{\\varepsilon}(\\overrightarrow{x}_{i})$      $\\overrightarrow{q} \\in N_{\\varepsilon}(\\overrightarrow{x}_{n})$            Density-Connected    &lt;img alt=\"connected\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/6ed03019-24a7-4812-bee3-8a825934bcd2\" width=80%&gt;          Core Point Condition ì„ ë§Œì¡±í•˜ëŠ” ê´€ì¸¡ì¹˜ ë²¡í„° $\\overrightarrow{p},\\overrightarrow{q} \\in D$ ì— ëŒ€í•˜ì—¬,&lt;/br&gt; $\\overrightarrow{p}$ ë¡œë¶€í„° ë°€ë„ ê¸°ì¤€ ë„ë‹¬ ê°€ëŠ¥í•œ(Density-Connected) ë™ì‹œì— $\\overrightarrow{q}$ ë¡œë¶€í„° ë°€ë„ ê¸°ì¤€ ë„ë‹¬ ê°€ëŠ¥í•œ(Density-Connected) ê´€ì¸¡ì¹˜ ë²¡í„° $\\overrightarrow{x} \\in D$ ê°€ ì ì–´ë„ í•˜ë‚˜ ì¡´ì¬í•œë‹¤ë©´,&lt;/br&gt; $\\overrightarrow{p},\\overrightarrow{q}$ ëŠ” ë°€ë„ ê¸°ì¤€ ì—°ê²°ë˜ì–´ ìˆìŒ(Density-Connected)                                                          $              N_{\\varepsilon}(\\overrightarrow{p})              \\ge \\text{MinPts}$                                          $\\overrightarrow{x} \\in N_{\\varepsilon}(\\overrightarrow{p})$                                                  $              N_{\\varepsilon}(\\overrightarrow{q})              \\ge \\text{MinPts}$                                          $\\overrightarrow{x} \\in N_{\\varepsilon}(\\overrightarrow{q})$      &lt;/br&gt;sklearn.cluster.DBSCANfrom sklearn.cluster import DBSCANğŸ’¡ General HyperParameter  random_state(default : None)  n_jobs(default : None) : ë³‘ë ¬ë¡œ ì‘ì—…í•  ì½”ì–´ ê°¯ìˆ˜ğŸ’¡ Model HyperParameter      eps(default : 0.5) : ì§ê²½    metric(default : 'euclidean') : ì§ê²½ ì¸¡ì • ë°©ë²•          'l1', 'manhattan' or 'cityblock' : ë§¨í•´íŠ¼ ê±°ë¦¬ ì¸¡ì •ë²•      'l2' or 'euclidean' : ìœ í´ë¦¬ë“œ ê±°ë¦¬ ì¸¡ì •ë²•      'cosine' : ì½”ì‚¬ì¸ ê±°ë¦¬ ì¸¡ì •ë²•      'haversine' : í•˜ë²„ì‚¬ì¸ ê±°ë¦¬ ì¸¡ì •ë²•      callable        p(default : None) : metric ì˜ ì•„ê·œë¨¼íŠ¸ê°€ 'minkowski' ì¸ ê²½ìš° ì¶”ê°€ ì„¤ì •          1 : ë§¨í•´íŠ¼ ê±°ë¦¬ ì¸¡ì •ë²•      2 : ìœ í´ë¦¬ë“œ ê±°ë¦¬ ì¸¡ì •ë²•        min_samples(default : 5) : ìµœì†Œ ìš”ì†Œ ê°¯ìˆ˜ğŸ’¡ Atrribute  labels_ : ê° ê´€ì¸¡ì¹˜ê°€ ì†í•œ êµ°ì§‘ ë²ˆí˜¸          -1 : ì´ìƒì¹˜ êµ°ì§‘        core_sample_indices_ : êµ°ì§‘ë³„ í•µì‹¬ ìš”ì†Œì˜ í–‰ ë²ˆí˜¸&lt;/br&gt;&lt;hr&gt;ì´ë¯¸ì§€ ì¶œì²˜      https://ai.plainenglish.io/dbscan-density-based-clustering-aaebd76e2c8c        https://journals.sagepub.com/doi/10.1177/1748301817735665  "
  },
  
  {
    "title": "Kmeans",
    "url": "/posts/kMeans/",
    "categories": "",
    "tags": "",
    "date": "2024-01-16 00:00:00 +0900",
    





    
    "snippet": "Index  What? k-Means  ì¤‘ì‹¬ì  íƒìƒ‰ ê³¼ì •  sklearn.cluster.KMeans&lt;/br&gt;What? k-MeansğŸ’¡ k-Means      ì •ì˜ : ì¤‘ì‹¬ì  ê¸°ë°˜ ë°°íƒ€ì  ë¶„ë¦¬í˜• êµ°ì§‘í™” ì•Œê³ ë¦¬ì¦˜    &lt;img alt=\"kmeans\" src=\"https://github.com/jayarnim/jayarnim/assets/1...",
    "content": "Index  What? k-Means  ì¤‘ì‹¬ì  íƒìƒ‰ ê³¼ì •  sklearn.cluster.KMeans&lt;/br&gt;What? k-MeansğŸ’¡ k-Means      ì •ì˜ : ì¤‘ì‹¬ì  ê¸°ë°˜ ë°°íƒ€ì  ë¶„ë¦¬í˜• êµ°ì§‘í™” ì•Œê³ ë¦¬ì¦˜    &lt;img alt=\"kmeans\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/3d082c2b-2fe2-47af-8b00-e8c358febd20\" width=80%&gt;        ëª©í‘œ : ê° êµ°ì§‘ì— ëŒ€í•˜ì—¬, ê´€ì¸¡ì¹˜ì™€ ì¤‘ì‹¬ì (Centroid) ê°„ í‰ê·  ê±°ë¦¬(Means)ë¥¼ ìµœì†Œí™”í•¨\\[\\min_{\\overrightarrow{\\mu}_{i}}{\\sum_{i=1}^{k}\\sum_{\\overrightarrow{x}_{j} \\in C_{i}}{||\\overrightarrow{x}_{j}-\\overrightarrow{\\mu}_{i}||^2}}\\]  ğŸ’¡ í•œê³„ì       ì´ˆê¸° êµ°ì§‘ ì¤‘ì‹¬ì— ë¯¼ê°í•¨    &lt;img alt=\"ì´ˆê¸°êµ°ì§‘ì¤‘ì‹¬\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/380e04d5-14d6-401f-ab6b-a19caa547f09\" width=80%&gt;        ì´ìƒì¹˜ì— ë¯¼ê°í•¨    &lt;img alt=\"ì´ìƒì¹˜\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/0bd2c989-77ba-4d19-aea4-423e3299133f\" width=80%&gt;        êµ¬í˜•ì´ ì•„ë‹Œ í˜•íƒœì˜ êµ°ì§‘ì„ íƒì§€í•˜ê¸° ì–´ë ¤ì›€    &lt;img alt=\"êµ¬í˜•\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/972cdc52-f7f3-4d4b-9802-19ac3e08326f\" width=80%&gt;        ì„œë¡œ ë‹¤ë¥¸ ê·œëª¨ì˜ êµ°ì§‘ì„ íƒì§€í•˜ê¸° ì–´ë ¤ì›€    &lt;img alt=\"ê·œëª¨\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/6eb887df-fc0e-4fc8-97c3-1cf725a5e453\" width=80%&gt;        ì„œë¡œ ë‹¤ë¥¸ ë°€ë„ì˜ êµ°ì§‘ì„ íƒì§€í•˜ê¸° ì–´ë ¤ì›€    &lt;img alt=\"ë°€ë„\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/b1d6f3a3-9acf-489d-b75f-205c2576f3a1\" width=80%&gt;  &lt;/br&gt;ì¤‘ì‹¬ì  íƒìƒ‰ ê³¼ì •&lt;img alt=\"íƒìƒ‰ê³¼ì •\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/22e0d1b3-d282-423f-bb56-9a46d76b7f33\" width=80%&gt;      êµ°ì§‘ ê°¯ìˆ˜ë¥¼ ì„¤ì •í•¨\\[X=C_{1} \\cup C_{2} \\cup \\cdots \\cup C_{k}\\\\ \\\\ C_{i} \\cap C_{j \\ne i} = \\emptyset\\]        $k$ ê°œì˜ ì´ˆê¸° êµ°ì§‘ ì¤‘ì‹¬ ë²¡í„° $\\overrightarrow{c}$ ë¥¼ ì„ì˜ë¡œ ì„ ì •í•¨\\[M=\\{\\overrightarrow{\\mu}_{1},\\overrightarrow{\\mu}_{2},\\cdots,\\overrightarrow{\\mu}_{k}\\}\\]        ëª¨ë“  ê´€ì¸¡ì¹˜ ë²¡í„° $\\overrightarrow{x}$ ë¥¼ ê°€ì¥ ê°€ê¹Œìš´ ê±°ë¦¬ì— ìœ„ì¹˜í•œ ì¤‘ì‹¬ ë²¡í„° $\\overrightarrow{\\mu}$ ì˜ êµ°ì§‘ì— ë°°íƒ€ì ìœ¼ë¡œ í• ë‹¹í•¨\\[\\overrightarrow{x}_{j} \\rightarrow C_{i}\\\\ \\begin{aligned} \\\\\\text{s.t.} \\quad  &amp; i=\\argmin_{i}{||\\overrightarrow{x}_{j}-\\overrightarrow{\\mu}_{i}||^2}\\\\ &amp;\\overrightarrow{\\mu}_{i} \\in C_{i} \\end{aligned}\\]        ê° êµ°ì§‘ë³„ í• ë‹¹ëœ ê´€ì¸¡ì¹˜ ë²¡í„°ë“¤ì˜ í‰ê·  ë²¡í„°ë¡œ êµ°ì§‘ ì¤‘ì‹¬ ë²¡í„°ë¥¼ ê°±ì‹ í•¨\\[\\begin{aligned} \\overrightarrow{\\mu}_{i} &amp;=\\displaystyle\\frac{1}{|C_{i}|}\\sum_{\\overrightarrow{x}_{j}\\in C_{i}}{\\overrightarrow{x}_{j}} \\end{aligned}\\]        â‘¢, â‘£ì˜ ê³¼ì •ì„ ë°˜ë³µí•˜ì—¬ ìµœì ì˜ êµ°ì§‘ ì¤‘ì‹¬ ë²¡í„° ì§‘í•© $\\hat{M}$ ì„ íƒìƒ‰í•¨\\[\\begin{aligned} \\hat{M} &amp;= \\{\\overrightarrow{\\mu}_{i} \\big| \\argmin_{\\overrightarrow{\\mu}_{i}}{\\sum_{i=1}^{k}\\sum_{\\overrightarrow{x}_{j} \\in C_{i}}{||\\overrightarrow{x}_{j}-\\overrightarrow{\\mu}_{i}||^2}}\\} \\end{aligned}\\]  &lt;/br&gt;sklearn.cluster.KMeansfrom sklearn.cluster import KMeansğŸ’¡ General HyperParameter  random_state(default : None)ğŸ’¡ Model HyperParameter      n_cluster(default : 8) : êµ°ì§‘ ê°¯ìˆ˜    init(default : 'k-means++') : ì¤‘ì‹¬ì  ì´ˆê¸°í™” ë°©ë²•          'k-means++'      'random'      callable        n_init(default : 10) : ì¤‘ì‹¬ì  ì´ˆê¸°í™” íšŸìˆ˜          'auto'      int            max_iter(default : 300) : í•™ìŠµ(Means ìµœì†Œí™”) ìµœëŒ€ íšŸìˆ˜    tol(default : 1e-4) : í—ˆìš© ì†ì‹¤ğŸ’¡ Attribute  labels_ : ê° ê´€ì¸¡ì¹˜ê°€ ì†í•œ êµ°ì§‘ ë²ˆí˜¸  cluster_centers_ : êµ°ì§‘ë³„ ì¤‘ì‹¬ì  ìœ„ì¹˜  n_iter_ : ì¤‘ì‹¬ì  ì´ë™ íšŸìˆ˜  inertia_ : êµ°ì§‘ë³„ Means í‰ê· ìœ¼ë¡œì„œ ìˆ˜ì¹˜ê°€ ë‚®ì„ìˆ˜ë¡ ì‘ì§‘ë„ê°€ ë†’ë‹¤ê³  íŒë‹¨í•¨&lt;/br&gt;&lt;hr&gt;ì´ë¯¸ì§€ ì¶œì²˜      https://ai-times.tistory.com/158        https://github.com/pilsung-kang/multivariate-data-analysis/blob/master/09%20Clustering/09-2_K-Means%20Clustering.pdf        https://github.com/lovit/python_ml_intro/blob/master/lecture_notes/10_clustering.pdf        https://paulvanderlaken.com/2018/12/12/visualizing-the-inner-workings-of-the-k-means-clustering-algorithm/  "
  },
  
  {
    "title": "Clustering",
    "url": "/posts/Clustering/",
    "categories": "",
    "tags": "",
    "date": "2024-01-15 00:00:00 +0900",
    





    
    "snippet": "Index  What? Cluster Analysis  Metrics&lt;/br&gt;What? Cluster AnalysisğŸ’¡ Cluster Analysis      ì •ì˜ : í‘œë³¸ì„ ê´€ì¸¡ì¹˜ ê°„ ìœ ì‚¬ì„±ê³¼ ìƒì´ì„±ì„ ê³„ì‚°í•˜ì—¬ kê°œì˜ êµ°ì§‘ìœ¼ë¡œ ë¶„í• í•˜ëŠ” ì‘ì—…          Cluster analysis or clustering is the task of ...",
    "content": "Index  What? Cluster Analysis  Metrics&lt;/br&gt;What? Cluster AnalysisğŸ’¡ Cluster Analysis      ì •ì˜ : í‘œë³¸ì„ ê´€ì¸¡ì¹˜ ê°„ ìœ ì‚¬ì„±ê³¼ ìƒì´ì„±ì„ ê³„ì‚°í•˜ì—¬ kê°œì˜ êµ°ì§‘ìœ¼ë¡œ ë¶„í• í•˜ëŠ” ì‘ì—…          Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group are more similar to each other than to those in other groups. (by.Wikipedia)            ëª©í‘œ : êµ°ì§‘ ë‚´ ì‘ì§‘ë„ ìµœëŒ€í™” ë° êµ°ì§‘ ê°„ ë¶„ë¦¬ë„ ìµœëŒ€í™”    &lt;img alt=\"êµ°ì§‘í™”ëª©í‘œ\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/10be2560-d7ea-4fc9-aaa9-3f85eaac58b8\" width=80%&gt;        íŒë³„ ë¶„ì„ê³¼ ë¹„êµ    &lt;img alt=\"ë¶„ë¥˜ì™€êµ°ì§‘ì˜ë¹„êµ\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/0fe4326f-2a10-4236-ad7c-f15b58aa9185\" width=80%&gt;          íŒë³„ ë¶„ì„(Classification Analysis)                  í•™ìŠµ ë°©ì‹ : ì§€ë„í•™ìŠµ(í›ˆë ¨ ê´€ì¸¡ì¹˜ì˜ ì •ë‹µ ì •ë³´ë¥¼ ì‚¬ì „ì— ì•Œê³  ìˆìŒ)          í•™ìŠµ ëª©í‘œ : $X$ ì™€ $Y$ ì˜ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í•¨ìˆ˜ë¥¼ íƒìƒ‰í•¨                    êµ°ì§‘ ë¶„ì„(Cluster Analysis)                  í•™ìŠµ ë°©ì‹ : ë¹„ì§€ë„í•™ìŠµ(í›ˆë ¨ ê´€ì¸¡ì¹˜ì˜ ì •ë‹µ ì •ë³´ë¥¼ ì‚¬ì „ì— ì•Œ ìˆ˜ ì—†ê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŒ)          í•™ìŠµ ëª©í‘œ : ì£¼ì–´ì§„ í‘œë³¸ì„ ë¶„í• í•˜ëŠ” ì—¬ëŸ¬ ê°œì˜ êµ°ì§‘ì„ íƒìƒ‰í•¨                    ğŸ’¡ êµ¬ë¶„      ê´€ì¸¡ì¹˜ ì¤‘ë³µ ì—¬ë¶€ì— ë”°ë¥¸ êµ¬ë¶„    &lt;img alt=\"ì¤‘ë³µì—¬ë¶€\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/cb85a880-4899-47df-9b85-284b85090dfe\" width=80%&gt;          Hard(or Crisp) Clustering : ê´€ì¸¡ì¹˜ê°€ í•˜ë‚˜ì˜ êµ°ì§‘ì—ë§Œ í• ë‹¹ë¨      Soft(or Fuzzy) Clustering : ê´€ì¸¡ì¹˜ê°€ ì—¬ëŸ¬ ê°œì˜ êµ°ì§‘ì— í• ë‹¹ë  ìˆ˜ ìˆìŒ            êµ°ì§‘ ê°„ ìœ„ê³„ ì—¬ë¶€ì— ë”°ë¥¸ êµ¬ë¶„    &lt;img alt=\"ê³„ì¸µì—¬ë¶€\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/27bf6947-dd2a-425f-b3ce-ddb8cade0989\" width=80%&gt;          Partitional Clustering : êµ°ì§‘ ê°„ ìœ„ê³„ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ      Hierarchical Clustering : êµ°ì§‘ ê°„ ìœ„ê³„ê°€ ì¡´ì¬í•¨      &lt;/br&gt;MetricsğŸ’¡ êµ°ì§‘í™” ìœ íš¨ì„± í‰ê°€ ê¸°ì¤€  External : ì •ë‹µ ì •ë³´ì™€ì˜ ë¹„êµ          Rand Statistic      Jaccard Coefficient      Folks and Mallows Index      Hurbert $\\Gamma$ Statistic      V-Measure        Internal : êµ°ì§‘ ë‚´ ì‘ì§‘ë„          Cophenetic Correlation Coefficient      Sum of Squared Error(SSE)      Cohesion and Separation        Relative : êµ°ì§‘ ê°„ ë¶„ë¦¬ë„          Dunn Family of Indices      Davies-Bouldin Index      Semi-partial R-squared      SD Validity Index      Silhouette      ğŸ’¡ External Metrics      V-Measure : ì •í™•ì„±ê³¼ ì™„ì „ì„±ì˜ ì¡°í™”í‰ê· \\[\\begin{aligned}  \\text{V}  &amp;= 2\\times\\frac{H(C|K) \\cdot C(K|C)}{H(C|K) + C(K|C)}  \\end{aligned}\\]                                                      $H(C              K)$ : ì •í™•ì„±(Homogeneity)                                                                                      $C(K              C)$ : ì™„ì „ì„±(Completeness)                                                ì •í™•ì„±(Homogeneity) : ê° êµ°ì§‘ì˜ í´ë˜ìŠ¤ì— ëŒ€í•œ ì—”íŠ¸ë¡œí”¼ í•©ê³„\\[\\begin{aligned}  H(C|K)  &amp;= -\\sum_{k=1}^{K}{\\sum_{c=1}^{C}{P(c|k) \\cdot \\log{P(c|k)}}}  \\end{aligned}\\]          $k$ : êµ°ì§‘ ë²ˆí˜¸      $c$ : í´ë˜ìŠ¤ ë²ˆí˜¸                                                  $P(c              k)$ : êµ°ì§‘ $K=k$ ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, í´ë˜ìŠ¤ $C=c$ ê°€ ë°œìƒí•  ê°€ëŠ¥ì„±                                                ì™„ì „ì„±(Completeness) : ê° í´ë˜ìŠ¤ì˜ êµ°ì§‘ì— ëŒ€í•œ ì—”íŠ¸ë¡œí”¼ í•©ê³„\\[\\begin{aligned}  C(K|C)  &amp;= -\\sum_{c=1}^{C}{\\sum_{k=1}^{K}{P(k|c) \\cdot \\log{P(k|c)}}}  \\end{aligned}\\]                                                      $P(k              c)$ : í´ë˜ìŠ¤ $C=c$ ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, êµ°ì§‘ $K=k$ ê°€ ë°œìƒí•  ê°€ëŠ¥ì„±                                          ğŸ’¡ Internal Metrics      Sum of Squared Error(SSE) : ê° êµ°ì§‘ì˜ ì¤‘ì‹¬ì  ë²¡í„°ì™€ í•´ë‹¹ êµ°ì§‘ ë‚´ ê´€ì¸¡ì¹˜ ë²¡í„° ê°„ ê±°ë¦¬ ììŠ¹ìœ¼ë¡œ ì¸¡ì •í•œ êµ°ì§‘ ì‘ì§‘ë„\\[\\begin{aligned}  \\text{SSE}  &amp;= \\sum_{k=1}^{K}{\\sum_{\\overrightarrow{x}_{i} \\in C_{k}}{||\\overrightarrow{x}_{i}-\\overrightarrow{\\mu}_{k}||^2}}  \\end{aligned}\\]          $k$ : êµ°ì§‘ ë²ˆí˜¸      $C_{k}$ : $k$ ë²ˆì§¸ êµ°ì§‘      $\\overrightarrow{x}{i} \\in C{k}$ : êµ°ì§‘ $C_{k}$ ì˜ $i$ ë²ˆì§¸ ê´€ì¸¡ì¹˜ ë²¡í„°      $\\overrightarrow{\\mu}{k} \\in C{k}$ : êµ°ì§‘ $C_{k}$ ì˜ ì¤‘ì‹¬ ë²¡í„°      ğŸ’¡ Relative Metrics      Dunn Index : êµ°ì§‘ ë‚´ ì‘ì§‘ë„(Cohesion) ìµœëŒ€ê°’ ëŒ€ë¹„ êµ°ì§‘ ê°„ ë¶„ë¦¬ë„(Separation) ìµœì†Œê°’ ë¹„ìœ¨\\[\\begin{aligned}  \\text{DI}  &amp;= \\frac{\\min_{1 \\le i \\ne j \\le K}{d_{C}(C_{i},C_{j})}}{\\max_{1 \\le k \\le K}{\\Delta(C_{k})}}  \\end{aligned}\\]          $\\min_{1 \\le i \\ne j \\le K}{d_{C}(C_{i},C_{j})}$ : êµ°ì§‘ ê°„ ë¶„ë¦¬ë„ ìµœì†Œê°’ìœ¼ë¡œì„œ ë¶„ë¦¬ë„ì— ëŒ€í•œ ìµœì•…ì˜ ê²½ìš°      $\\max_{1 \\le k \\le K}{\\Delta(C_{k})}$ : êµ°ì§‘ ë‚´ ì‘ì§‘ë„ ìµœëŒ€ê°’ìœ¼ë¡œì„œ ì‘ì§‘ë„ì— ëŒ€í•œ ìµœì•…ì˜ ê²½ìš°            ì‹¤ë£¨ì—£ ê³„ìˆ˜(Silhouette) : êµ°ì§‘ ë‚´ ì‘ì§‘ì„±(cohesion)ê³¼ êµ°ì§‘ ê°„ ë¶„ë¦¬ë„(separation)ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ì¸¡ì •í•œ ê°œë³„ ê´€ì¸¡ì¹˜ ë²¡í„°ì— ëŒ€í•œ êµ°ì§‘í™” ì í•©ì„±ì˜ í‰ê· ê°’\\[\\begin{aligned}  \\text{S}  &amp;= \\frac{1}{n}\\sum_{i=1}^{n}{\\frac{b(\\overrightarrow{x}_{i})-a(\\overrightarrow{x}_{i})}{\\max{[a(\\overrightarrow{x}_{i}),b(\\overrightarrow{x}_{i})]}}}  \\end{aligned}\\]          $a(\\overrightarrow{x}{i})$ : ê´€ì¸¡ì¹˜ ë²¡í„° $\\overrightarrow{x}{i}$ ê¸°ì¤€ êµ°ì§‘ ë‚´ ì‘ì§‘ë„ë¡œì„œ, í•´ë‹¹ ê´€ì¸¡ì¹˜ì™€ ê°™ì€ êµ°ì§‘ì— ì†í•œ ê´€ì¸¡ì¹˜ì™€ì˜ í‰ê·  ê±°ë¦¬      $b(\\overrightarrow{x}{i})$ : ê´€ì¸¡ì¹˜ ë²¡í„° $\\overrightarrow{x}{i}$ ê¸°ì¤€ êµ°ì§‘ ê°„ ë¶„ë¦¬ë„ë¡œì„œ, í•´ë‹¹ ê´€ì¸¡ì¹˜ì™€ ë‹¤ë¥¸ êµ°ì§‘ì— ì†í•œ ê´€ì¸¡ì¹˜ì™€ì˜ í‰ê·  ê±°ë¦¬ ì¤‘ ìµœì†Œê°’      &lt;/br&gt;&lt;hr&gt;ì´ë¯¸ì§€ ì¶œì²˜      https://www.scaler.com/topics/supervised-and-unsupervised-learning/        https://towardsdatascience.com/a-brief-introduction-to-unsupervised-learning-20db46445283        https://tyami.github.io/machine%20learning/clustering/  "
  },
  
  {
    "title": "Tsne",
    "url": "/posts/tSNE/",
    "categories": "",
    "tags": "",
    "date": "2024-01-14 00:00:00 +0900",
    





    
    "snippet": "Index  t-SNE  ìˆ˜í•™ì  ì´í•´  sklearn.manifold.TSNE&lt;/br&gt;What? t-SNEğŸ’¡ Stochastic Neighbor Embedding      ì •ì˜ : ê´€ì¸¡ì¹˜ ê°„ ê³ ì°¨ì› ê³µê°„ ìƒ í™•ë¥ ì  ìœ ì‚¬ë„ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ì €ì°¨ì›ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œ    &lt;img alt=\"ë‹¤ì–‘ì²´í•™ìŠµ\" src=\"https://use...",
    "content": "Index  t-SNE  ìˆ˜í•™ì  ì´í•´  sklearn.manifold.TSNE&lt;/br&gt;What? t-SNEğŸ’¡ Stochastic Neighbor Embedding      ì •ì˜ : ê´€ì¸¡ì¹˜ ê°„ ê³ ì°¨ì› ê³µê°„ ìƒ í™•ë¥ ì  ìœ ì‚¬ë„ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ì €ì°¨ì›ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œ    &lt;img alt=\"ë‹¤ì–‘ì²´í•™ìŠµ\" src=\"https://user-images.githubusercontent.com/116495744/224497076-8a2e6100-88a5-444c-abb9-377e61e961ee.jpeg\" width=80%&gt;  ğŸ’¡ t-distributed Stochastic Neighbor Embedding      SNEì˜ ë¬¸ì œì  : Crowding Problem    &lt;img alt=\"ê°€ìš°ì‹œì•ˆ\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/1a3abcc6-062c-469e-9d44-aba6e5f3ecb0\" width=80%&gt;          ê°€ìš°ì‹œì•ˆ ë¶„í¬ëŠ” ì–‘ìª½ ê¼¬ë¦¬ ë¶€ë¶„ì´ ì¶©ë¶„íˆ ë‘í…ì§€ ì•Šì€ í˜•íƒœë¥¼ ë³´ì„                                                  ì¦‰, í™•ë¥ ë³€ìˆ˜ $              Â               \\overrightarrow{x}{i}-\\overrightarrow{x}{j}              Â               $ ê°€ ì¼ì •í•œ ê°’ ì´ìƒë¶€í„°ëŠ” ìœ ì‚¬ë„ì— í° ì°¨ì´ê°€ ì—†ìŒ                                                t-SNEì˜ í•´ê²°ì±… : Student t-Dristribution    &lt;img alt=\"studentt\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/61a37781-b23f-424a-8193-8898a8850505\" width=80%&gt;\\[\\begin{aligned}  T  &amp;= \\frac{Z}{\\sqrt{\\displaystyle\\frac{V}{\\nu}}}\\\\  V  &amp;= \\sum_{i=1}^{k}{Z_{i}^{2}}  \\end{aligned}\\]          $Z$ : í‘œì¤€ ê°€ìš°ì‹œì•ˆ ë¶„í¬      $V$ : ììœ ë„ê°€ $\\nu$ ì¸ ì¹´ì´ì œê³± ë¶„í¬      &lt;/br&gt;ìˆ˜í•™ì  ì´í•´ğŸ’¡ ë‘ ê´€ì¸¡ì¹˜ì˜ ê³ ì°¨ì› ê³µê°„ ìƒ ìœ ì‚¬ë„ ë„ì¶œ      í™•ë¥ ë³€ìˆ˜ $X$ ê°€ ê°€ìš°ì‹œì•ˆ ë¶„í¬ $N(\\mu, \\sigma^{2})$ ì„ ë”°ë¥¸ë‹¤ê³  í–ˆì„ ë•Œ, $X$ ì˜ í™•ë¥ ë°€ë„í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ\\[\\begin{aligned}  f(x)  &amp;= \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp{\\left[-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right]}  \\end{aligned}\\]        ê´€ì¸¡ì¹˜ $\\overrightarrow{x}{i}$ ë¡œë¶€í„°ì˜ ê±°ë¦¬ì— ëŒ€í•œ $\\overrightarrow{x}{i}$ ì™€ì˜ ìœ ì‚¬ë„ì˜ ê°€ìš°ì‹œì•ˆ ë¶„í¬ì— ê¸°ì´ˆí–ˆì„ ë•Œ, $\\overrightarrow{x}{j}$ ê°€ $\\overrightarrow{x}{i}$ ì™€ ìœ ì‚¬í•  ê°€ëŠ¥ì„±ì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•˜ì\\[\\begin{aligned}  p_{j|i}   = \\frac{\\exp{\\left[-\\frac{||\\overrightarrow{x}_{i}-\\overrightarrow{x}_{j}||^{2}}{2\\sigma^{2}}\\right]}}{\\sum_{k \\ne i}{\\exp{\\left[-\\frac{||\\overrightarrow{x}_{i}-\\overrightarrow{x}_{k}||^{2}}{2\\sigma^{2}}\\right]}}}\\\\\\\\  ||\\overrightarrow{x}_{i}-\\overrightarrow{x}_{j}|| \\sim N_{i}(0,\\sigma^{2})  \\end{aligned}\\]        ë˜í•œ ê´€ì¸¡ì¹˜ $\\overrightarrow{x}{j}$ ë¡œë¶€í„°ì˜ ê±°ë¦¬ì— ëŒ€í•œ $\\overrightarrow{x}{j}$ ì™€ì˜ ìœ ì‚¬ë„ì˜ ê°€ìš°ì‹œì•ˆ ë¶„í¬ì— ê¸°ì´ˆí–ˆì„ ë•Œ, $\\overrightarrow{x}{i}$ ê°€ $\\overrightarrow{x}{j}$ ì™€ ìœ ì‚¬í•  ê°€ëŠ¥ì„±ì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•˜ì\\[\\begin{aligned}  p_{i|j}  = \\frac{\\exp{\\left[-\\frac{|\\overrightarrow{x}_{i}-\\overrightarrow{x}_{j}|^{2}}{2\\sigma^{2}}\\right]}}{\\sum_{k \\ne j}{\\exp{\\left[-\\frac{|\\overrightarrow{x}_{j}-\\overrightarrow{x}_{k}|^{2}}{2\\sigma^{2}}\\right]}}}\\\\\\\\  ||\\overrightarrow{x}_{i}-\\overrightarrow{x}_{j}|| \\sim N_{j}(0,\\sigma^{2})  \\end{aligned}\\]        ë‘ ê°€ëŠ¥ì„±ì€ ì„œë¡œ ë‹¤ë¥¸ í™•ë¥  ë¶„í¬ì— ê¸°ì´ˆí•˜ê³  ìˆìœ¼ë¯€ë¡œ ê·¸ ê°’ì´ ë°˜ë“œì‹œ ì¼ì¹˜í•œë‹¤ê³  ë³¼ ìˆ˜ ì—†ìŒ\\[p_{j|i} \\ne p_{i|j}\\]        ë”°ë¼ì„œ $\\overrightarrow{x}{i}$ ì™€ $\\overrightarrow{x}{j}$ ì˜ ìœ ì‚¬ë„ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•¨\\[\\begin{aligned}  p_{i,j}  &amp;= \\frac{p_{j|i} + p_{i|j}}{2n}  \\end{aligned}\\]  ğŸ’¡ ë‘ ê´€ì¸¡ì¹˜ì˜ 2ì°¨ì› ê³µê°„ ìƒ ìœ ì‚¬ë„ ë„ì¶œ      ê´€ì¸¡ì¹˜ $\\overrightarrow{y}{i}$ ë¡œë¶€í„°ì˜ ê±°ë¦¬ì— ëŒ€í•œ $\\overrightarrow{y}{i}$ ì™€ì˜ ìœ ì‚¬ë„ì˜ ê°€ìš°ì‹œì•ˆ ë¶„í¬ì— ê¸°ì´ˆí–ˆì„ ë•Œ, $\\overrightarrow{y}{j}$ ê°€ $\\overrightarrow{y}{i}$ ì™€ ìœ ì‚¬í•  ê°€ëŠ¥ì„±ì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•˜ì\\[\\begin{aligned}  q_{j|i}   = \\frac{\\exp{\\left[-||\\overrightarrow{y}_{i}-\\overrightarrow{y}_{j}||^{2}\\right]}}{\\sum_{k \\ne i}{\\exp{\\left[-||\\overrightarrow{y}_{i}-\\overrightarrow{y}_{k}||^{2}\\right]}}}\\\\\\\\  ||\\overrightarrow{y}_{i}-\\overrightarrow{y}_{j}|| \\sim N_{i}(0,\\sigma^{2})  \\end{aligned}\\]          $\\overrightarrow{y}$ : ê³ ì°¨ì› ê³µê°„ ìƒì˜ ê´€ì¸¡ì¹˜ ë²¡í„° $\\overrightarrow{x}$ ë¥¼ 2ì°¨ì› ê³µê°„ ìƒì— ë§¤í•‘í•œ ë²¡í„°            $\\overrightarrow{y}{i}$ ì™€ $\\overrightarrow{y}{j}$ ì˜ ìœ ì‚¬ë„ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•¨\\[\\begin{aligned}  q_{i,j}  &amp;= \\frac{q_{j|i} + q_{i|j}}{2n}  \\end{aligned}\\]  ğŸ’¡ ë¹„ìš©í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ì•„ê·œë¨¼íŠ¸ ë„ì¶œ      KL(Kullback-Leibler Divergence) : í™•ë¥ ë³€ìˆ˜ $X$ ì˜ ë™ì¼í•œ ì•„ê·œë¨¼íŠ¸ $x_{i}$ ì— ëŒ€í•˜ì—¬ ë‘ í™•ë¥  ë¶„í¬ $P,Q$ ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œ\\[\\text{KL}(P|Q)  = \\sum_{i=1}^{n}{P(X=x_{i})\\cdot\\log{\\frac{P(X=x_{i})}{Q(X=x_{i})}}}\\]        KLì— ê¸°ì´ˆí•œ ë¹„ìš©í•¨ìˆ˜ ì •ì˜\\[\\begin{aligned}  Cost  &amp;= \\sum_{i}{\\text{KL}(P_{i}|Q_{i})}\\\\  &amp;= \\sum_{i}{\\sum_{j}{p_{i,j}\\cdot\\log{\\frac{p_{i,j}}{q_{i,j}}}}}  \\end{aligned}\\]        $\\hat{\\mathbf{Y}}$ ë„ì¶œ\\[\\begin{aligned}  \\hat{\\mathbf{Y}}  &amp;= \\{\\hat{\\overrightarrow{y}}_{i}|\\argmin_{\\overrightarrow{y}_{i}}{Cost}\\}  \\end{aligned}\\]  &lt;/br&gt;sklearn.manifold.TSNEfrom sklearn.manifold import TSNEğŸ’¡ General HyperParameter  random_state = None  n_jobs(default : None) : ë³‘ë ¬ë¡œ ì‘ì—…í•  ì½”ì–´ ê°¯ìˆ˜ğŸ’¡ Model HyperParameter  n_components(default : 2) : ì¶•ì†Œí•  ì°¨ì›ì˜ ê°¯ìˆ˜  metric(default : 'euclidean') : ê´€ì¸¡ì¹˜ ê°„ ê¸°í•˜ ê±°ë¦¬ ì¸¡ì • ë°©ë²•  perplexity(default : 30.0) : ì°¸ì¡°í•  Nearest Neighbors ê°¯ìˆ˜ë¡œì„œ 5~50 ì‚¬ì´ì˜ ê°’ì„ ê¶Œì¥í•¨  early_exaggeration(default :12.0) : perplexity ì— ê¸°ì´ˆí•˜ì—¬ í˜•ì„±ëœ êµ°ì§‘ ê°„ ê±°ë¦¬ğŸ’¡ Learning Task HyperParameter  learning_rate(default : 'auto') : í•™ìŠµë¥   n_iter(default : 1000) : ë‹¤ì–‘ì²´ íƒìƒ‰ íšŸìˆ˜ë¡œì„œ 250 ì´ìƒì˜ ê°’ì„ ê¶Œì¥í•¨  n_iter_without_progress(default : 300) : ì†ì‹¤ í•¨ìˆ˜ê°€ ëª‡ ë²ˆ ì´ìƒ ê°œì„ ë˜ì§€ ì•Šì„ ê²½ìš° í•™ìŠµì„ ì¤‘ë‹¨í•  ê²ƒì¸ê°€"
  },
  
  {
    "title": "Pca&lda",
    "url": "/posts/PCA&LDA/",
    "categories": "",
    "tags": "",
    "date": "2024-01-13 00:00:00 +0900",
    





    
    "snippet": "Index  Prerequisite  PCA  LDA  sklearn.decomposition.PCA&lt;/br&gt;PrerequisiteğŸ’¡ Projection&lt;img alt=\"projection\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/5573d87c-f94c-4634-a11...",
    "content": "Index  Prerequisite  PCA  LDA  sklearn.decomposition.PCA&lt;/br&gt;PrerequisiteğŸ’¡ Projection&lt;img alt=\"projection\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/5573d87c-f94c-4634-a11b-7af4edd89640\" width=80%&gt;      ë²¡í„° $\\overrightarrow{a}$ ë¥¼ ë²¡í„° $\\overrightarrow{b}$ ì— ì •ì‚¬ì˜í–ˆì„ ë•Œ, ì •ì‚¬ì˜ ë²¡í„° $\\text{proj}_{\\overrightarrow{b}}(\\overrightarrow{a})$ ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ\\[\\begin{aligned}  \\cos{90\\degree}  &amp;= \\frac{(\\overrightarrow{a}-p\\overrightarrow{b})^{T}\\overrightarrow{b}}{||\\overrightarrow{a}||\\cdot||\\overrightarrow{b}||}\\\\  &amp;= 0\\\\  \\therefore \\text{proj}_{\\overrightarrow{b}}(\\overrightarrow{a})  &amp;= p\\overrightarrow{b}\\\\  &amp;= \\left(\\frac{\\overrightarrow{a}^{T}\\overrightarrow{b}}{||\\overrightarrow{b}||^{2}}\\right)\\overrightarrow{b}  \\end{aligned}\\]                                                      $p=\\displaystyle\\frac{\\overrightarrow{a}^{T}\\overrightarrow{b}}{              Â               \\overrightarrow{b}              Â               ^{2}}$ : ì •ì‚¬ì˜ ë²¡í„°ì˜ í¬ê¸°                                          $\\overrightarrow{b}$ : ì •ì‚¬ì˜ ë²¡í„°ì˜ ë°©í–¥      ğŸ’¡ Covariance Matrix      ê³µë¶„ì‚°(Covariance) : ë‘ í™•ë¥ ë³€ìˆ˜ì˜ ì„ í˜•ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œë¡œì„œ, ë‘ í™•ë¥ ë³€ìˆ˜ì˜ í¸ì°¨(ê´€ì¸¡ì¹˜ì™€ í‰ê·  ì‚¬ì´ ê±°ë¦¬)ë¥¼ ê³±í•œ ê°’ì˜ í‰ê· \\[\\sigma_{XY} = \\frac{1}{N}\\sum_{i=1}^{N}(X_{i}-\\mu_X)(Y_{i}-\\mu_Y)\\]        ê³µë¶„ì‚°í–‰ë ¬(Covariance Matrix) : $n$ ê°œ ë³€ìˆ˜ë“¤ ê°„ ê³µë¶„ì‚°ì„ ë‚˜ì—´í•œ $n \\times n$ ì •ë°©í–‰ë ¬\\[\\Sigma=  \\begin{matrix}  &amp; \\overrightarrow{A} &amp; \\overrightarrow{B} &amp; \\overrightarrow{C} \\\\  \\overrightarrow{A} &amp; \\sigma_{A}^2 &amp; \\sigma_{AB} &amp; \\sigma_{AC} \\\\  \\overrightarrow{B} &amp; \\sigma_{BA} &amp; \\sigma_{B}^2 &amp; \\sigma_{BC} \\\\  \\overrightarrow{C} &amp; \\sigma_{CA} &amp; \\sigma_{CB} &amp; \\sigma_{C}^2  \\end{matrix}\\]  ğŸ’¡ Linear Transformation&lt;img alt=\"ì„ í˜•ë³€í™˜\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/4e5847af-09b2-4050-be5b-68f0277a2a34\" width=80%&gt;      í–‰ë ¬ $\\mathbf{X}$ ì„ í†µí•œ ì„ í˜•ë³€í™˜ì€ ì–´ë–¤ ì¢Œí‘œë¥¼ $\\begin{pmatrix}1\\0\\end{pmatrix},\\begin{pmatrix}0\\1\\end{pmatrix}$ ë¥¼ ê¸°ì €ë¡œ ì‚¬ìš©í•˜ëŠ” 2ì°¨ì› ì¢Œí‘œê³„ì—ì„œ $\\overrightarrow{x}{1},\\overrightarrow{x}{2}$ ë¥¼ ê¸°ì €ë¡œ ì‚¬ìš©í•˜ëŠ” 2ì°¨ì› ì¢Œí‘œê³„ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•¨\\[\\begin{aligned}  \\mathbf{X}  &amp;= \\begin{pmatrix} 1&amp;3\\\\-2&amp;0 \\end{pmatrix}\\\\  &amp;= \\begin{pmatrix} \\overrightarrow{x}_{1}&amp;\\overrightarrow{x}_{2} \\end{pmatrix}  \\end{aligned}\\]        ë²¡í„° $\\overrightarrow{v}$ ëŠ” $\\begin{pmatrix}1\\0\\end{pmatrix},\\begin{pmatrix}0\\1\\end{pmatrix}$ ë¥¼ ê¸°ì €ë¡œ ì‚¬ìš©í•˜ëŠ” 2ì°¨ì› ì¢Œí‘œê³„ì˜ ì¢Œí‘œ $(-1,2)$ ë¥¼ ë‚˜íƒ€ëƒ„\\[\\begin{aligned}  \\overrightarrow{v}  &amp;= \\begin{pmatrix} 1\\\\-2 \\end{pmatrix}\\\\  &amp;= -1\\begin{pmatrix}1\\\\0\\end{pmatrix} + 2\\begin{pmatrix}0\\\\1\\end{pmatrix}\\\\  \\end{aligned}\\]        $\\mathbf{X}$ ë¥¼ í†µí•œ ì„ í˜• ë³€í™˜ ê²°ê³¼ $\\overrightarrow{v}$ ëŠ” $\\overrightarrow{x}{1},\\overrightarrow{x}{2}$ ë¥¼ ê¸°ì €ë¡œ ì‚¬ìš©í•˜ëŠ” 2ì°¨ì› ì¢Œí‘œê³„ì˜ ì¢Œí‘œ $(-1,2)$ ë¡œ ë³€í™˜ë˜ì—ˆìŒ\\[\\begin{aligned}  \\mathbf{X}\\cdot\\overrightarrow{v}  &amp;= \\begin{pmatrix} 1&amp;3\\\\-2&amp;0 \\end{pmatrix} \\cdot \\begin{pmatrix} 1\\\\-2 \\end{pmatrix}\\\\  &amp;= \\begin{pmatrix}-5\\\\2\\end{pmatrix}\\\\  &amp;= -1\\overrightarrow{x}_{1} + 2\\overrightarrow{x}_{2}  \\end{aligned}\\]  ğŸ’¡ Eigen-Vector&lt;img alt=\"ê³ ìœ ë²¡í„°\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/34f3ec69-c070-4538-843c-dfb3a6922d6f\" width=80%&gt;      ê³ ìœ ë²¡í„°(Eigen-Vector; $\\overrightarrow{v}$) : ì •ë°©í–‰ë ¬ $A_n$ ìœ¼ë¡œ ì„ í˜•ë³€í™˜í–ˆì„ ë•Œ, ê·¸ ë°©í–¥ì€ ë³€í•˜ì§€ ì•Šê³  ë‹¨ì§€ í¬ê¸°ë§Œ ë³€í•˜ëŠ” $\\overrightarrow{0}$ ì´ ì•„ë‹Œ ë²¡í„°\\[\\begin{aligned}  \\begin{pmatrix}  a_{11}&amp;a_{12}&amp;\\cdots&amp;a_{1n}\\\\  a_{21}&amp;a_{22}&amp;\\cdots&amp;a_{1n}\\\\  \\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\\\  a_{n1}&amp;a_{n2}&amp;\\cdots&amp;a_{nn}  \\end{pmatrix}  \\begin{pmatrix}  v_{1} \\\\ v_{2} \\\\ \\vdots \\\\ v_{n}  \\end{pmatrix}  =  \\lambda  \\begin{pmatrix}  v_{1} \\\\ v_{2} \\\\ \\vdots \\\\ v_{n}  \\end{pmatrix}  \\Leftrightarrow  A_{n \\times n} \\overrightarrow{v}   = \\lambda \\overrightarrow{v}  \\end{aligned}\\]        ê³ ìœ ê°’(Eigen-Value; $\\lambda$) : ê³ ìœ ë²¡í„°ì˜ ì„ í˜•ë³€í™˜ ì „ í¬ê¸° ëŒ€ë¹„ ì„ í˜•ë³€í™˜ í›„ í¬ê¸°ì˜ ë¹„ìœ¨  &lt;/br&gt;PCAğŸ’¡ Principal Component Analysis      ì •ì˜ : ê³ ì°¨ì› ë°ì´í„°ì— ëŒ€í•˜ì—¬, Xì˜ ë°©í–¥ì  ë¶„í¬ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” ìƒˆë¡œìš´ ì €ì°¨ì› ì§êµ ì¢Œí‘œë¥¼ í•™ìŠµí•˜ëŠ” ê¸°ë²•    &lt;img alt=\"ì£¼ì„±ë¶„ë¶„ì„\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/1db1d27b-58c2-4e3b-aab9-847e42c87166\" width=80%&gt;          ì£¼ì„±ë¶„(Principal Component; PC) : ìƒˆë¡œìš´ ì €ì°¨ì› ì§êµ ì¢Œí‘œ            ë°©ë²• : ê´€ì¸¡ì¹˜ ê°„ ìƒëŒ€ì  íŠ¹ì„±ì„ ì˜ ë³´ì¡´í•˜ëŠ” ì„±ë¶„ë“¤ì„ ì¶”ì¶œí•¨    &lt;img alt=\"SS\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/79450a8b-aa57-4087-9007-96104ac155b1\" width=80%&gt;          $\\text{component}$ : ì£¼ì„±ë¶„ ë²¡í„° $\\overrightarrow{w}$      $\\text{datapoint}$ : ê´€ì¸¡ì¹˜ ë²¡í„° $\\overrightarrow{x}\\in \\mathbf{X}$      $\\text{projected data}$ : ì£¼ì„±ë¶„ ë²¡í„°ì— ëŒ€í•œ ê´€ì¸¡ì¹˜ ë²¡í„°ì˜ ì •ì‚¬ì˜ ë²¡í„° $\\text{proj}_{\\overrightarrow{w}}(\\overrightarrow{x})$      $D_{1}$ : ê´€ì¸¡ì¹˜ ë²¡í„°ì— ëŒ€í•˜ì—¬ ë³´ì¡´í•˜ëŠ” ì •ë³´ë¡œì„œ ë¶„ì‚°      $D_{2}$ : ê´€ì¸¡ì¹˜ ë²¡í„°ì— ëŒ€í•˜ì—¬ ìœ ì‹¤í•˜ëŠ” ì •ë³´      $D_{3}$ : ê´€ì¸¡ì¹˜ ë²¡í„°ì˜ ë³¸ë˜ ì •ë³´      ğŸ’¡ ì£¼ì„±ë¶„ ë„ì¶œ ê³¼ì •ì˜ ì´í•´&lt;img alt=\"ì£¼ì„±ë¶„\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/c0d5f8e9-7cf8-4f80-9f10-b68d4b167ed8\" width=80%&gt;      ê´€ì¸¡ì¹˜ í–‰ë ¬ $X_{N \\times P}$ ë¥¼ ë‹¨ìœ„ë²¡í„° $\\overrightarrow{w}$ ì— ì •ì‚¬ì˜í•œë‹¤ê³  í•˜ì\\[\\begin{aligned}  proj_{\\overrightarrow{w}}(\\mathbf{X})  &amp;= \\frac{&lt;\\mathbf{X},\\overrightarrow{w}&gt;}{||w||^2}\\cdot\\overrightarrow{w}\\\\  &amp;= (\\overrightarrow{w}^{T}\\mathbf{X})\\cdot\\overrightarrow{w}(\\because ||w||=1)  \\end{aligned}\\]          $\\overrightarrow{w}$ : ì •ì‚¬ì˜ ë²¡í„°ì˜ ë°©í–¥      $\\overrightarrow{w}^{T}\\mathbf{X}$ : ì •ì‚¬ì˜ ë²¡í„°ì˜ í¬ê¸°            $\\overrightarrow{w}$ ì— ì •ì‚¬ì˜ëœ ê´€ì¸¡ì¹˜ë“¤ì˜ ë¶„ì‚° $\\mathbf{V}$ ì€ ë‹¤ìŒê³¼ ê°™ìŒ\\[\\begin{aligned}  \\mathbf{V}  &amp;= \\frac{1}{n}(\\overrightarrow{w}^{T}\\mathbf{X})(\\overrightarrow{w}^{T}\\mathbf{X})^{T}\\\\  &amp;= \\frac{1}{n}(\\overrightarrow{w}^{T}\\mathbf{X}\\mathbf{X}^{T}\\overrightarrow{w})\\\\  &amp;= \\overrightarrow{w}^{T}\\Sigma\\overrightarrow{w}  \\end{aligned}\\]          $\\Sigma=\\displaystyle\\frac{1}{n}\\mathbf{X}\\mathbf{X}^{T}$ : ê´€ì¸¡ì¹˜ í–‰ë ¬ $X$ ì˜ ê³µë¶„ì‚° í–‰ë ¬            $\\mathbf{V}$ ì„ ìµœëŒ€í™”í•˜ëŠ” $\\overrightarrow{w}$ ë¥¼ ì±„íƒí•œë‹¤ê³  í•˜ì\\[\\hat{\\overrightarrow{w}}  = \\argmax_{\\overrightarrow{w}}{\\overrightarrow{w}^{T}\\Sigma\\overrightarrow{w}}\\\\  \\begin{aligned}  \\\\\\text{s.t.} \\quad  &amp; \\overrightarrow{w}^{T}\\overrightarrow{w}=1  \\end{aligned}\\]        ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•ì— ê¸°ì´ˆí•˜ì—¬ $\\hat{\\overrightarrow{w}}$ ë„ì¶œ\\[\\begin{aligned}  L(\\overrightarrow{w},\\lambda)  &amp;= \\overrightarrow{w}^{T}\\Sigma\\overrightarrow{w}-\\lambda(\\overrightarrow{w}^{T}\\overrightarrow{w}-1)\\\\\\\\  \\frac{\\partial L(\\overrightarrow{w},\\lambda)}{\\overrightarrow{w}}  &amp;= \\Sigma\\overrightarrow{w}-\\lambda\\overrightarrow{w}\\\\  &amp;= 0\\\\\\\\  \\therefore (\\Sigma-\\lambda\\mathbf{I})\\hat{\\overrightarrow{w}}  &amp;=0  \\end{aligned}\\]        $\\mathbf{V}$ ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ì£¼ì„±ë¶„ $\\overrightarrow{w}$ ì€ $\\mathbf{X}$ ì˜ ê³µë¶„ì‚° í–‰ë ¬ $\\Sigma$ ì˜ ê³ ìœ ë²¡í„°ì„\\[\\begin{aligned}  \\Sigma  &amp;= \\mathbb{V}\\mathbb{\\Lambda}\\mathbb{V}^{-1},\\\\  \\mathbb{V}  &amp;= \\begin{pmatrix}\\overrightarrow{w}_{1}&amp;\\overrightarrow{w}_{2}&amp;\\cdots&amp;\\overrightarrow{w}_{p}\\end{pmatrix}\\\\  \\mathbb{\\Lambda}  &amp;= \\text{diag}(\\lambda_{1},\\lambda_{2},\\cdots,\\lambda_{p})  \\end{aligned}\\]  ğŸ’¡ ì£¼ì„±ë¶„ ë²¡í„°ì˜ ì„¤ëª…ë ¥ ì´í•´      ì£¼ì„±ë¶„ ë²¡í„°ì˜ ê³ ìœ ê°’ : ê´€ì¸¡ì¹˜ í–‰ë ¬ $\\mathbf{X}$ ì— ëŒ€í•˜ì—¬ ì£¼ì„±ë¶„ ë²¡í„°ì— ëŒ€í•œ ì •ì‚¬ì˜ ë²¡í„° ê°„ ë¶„ì‚°\\[\\begin{aligned}  \\mathbf{V}  &amp;= \\frac{1}{n}(\\overrightarrow{w}^{T}\\mathbf{X})(\\overrightarrow{w}^{T}\\mathbf{X})^{T}\\\\  &amp;= \\frac{1}{n}\\overrightarrow{w}^{T}\\mathbf{X}\\mathbf{X}^{T}\\overrightarrow{w}\\\\  &amp;= \\overrightarrow{w}^{T}\\Sigma\\overrightarrow{w}\\\\  &amp;= \\hat{\\overrightarrow{w}}^{T}\\lambda\\hat{\\overrightarrow{w}}(\\because \\Sigma\\hat{\\overrightarrow{w}}-\\lambda\\hat{\\overrightarrow{w}}=0)\\\\  &amp;= \\lambda(\\because \\overrightarrow{w}^{T}\\overrightarrow{w}=1)  \\end{aligned}\\]        ì£¼ì„±ë¶„ ë²¡í„°ì˜ ì„¤ëª…ë ¥ : ê´€ì¸¡ì¹˜ í–‰ë ¬ $\\mathbf{X}_{N \\times P}$ ì— ëŒ€í•˜ì—¬ ìƒì„± ê°€ëŠ¥í•œ $P$ ê°œì˜ ì£¼ì„±ë¶„ ë²¡í„° ê³ ìœ ê°’ í•©ê³„ ëŒ€ë¹„ í•´ë‹¹ ì£¼ì„±ë¶„ ë²¡í„° ê³ ìœ ê°’ ë¹„ìœ¨\\[\\frac{\\lambda_{k}}{\\sum_{i=1}^{p}{\\lambda_{i}}}\\]  &lt;/br&gt;LDAğŸ’¡ Linear Discriminant Analysis      ì •ì˜ : ê³ ì°¨ì› ë°ì´í„°ì— ëŒ€í•˜ì—¬, ì£¼ì–´ì§„ í´ë˜ìŠ¤ë¥¼ ê°€ì¥ ì˜ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ì €ì°¨ì› ì§êµ ì¢Œí‘œë¥¼ ì°¾ëŠ” ê¸°ë²•    &lt;img alt=\"lda\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/081f75ba-505a-4e94-be61-2568697232e7\" width=80%&gt;        ë°©ë²• : í´ë˜ìŠ¤ ê°„ ë¶„ì‚°ì€ ìµœëŒ€í™”í•˜ëŠ” ë™ì‹œì— í´ë˜ìŠ¤ ë‚´ ê´€ì¸¡ì¹˜ ê°„ ë¶„ì‚°ì€ ìµœì†Œí™”í•˜ëŠ” ì„±ë¶„ë“¤ì„ ì¶”ì¶œí•¨\\[\\hat{\\overrightarrow{w}}  =\\argmax_{\\overrightarrow{w}}{\\frac{\\Sigma^{2}}{\\sigma_{1}^{2}+\\sigma_{2}^{2}}}\\\\  \\begin{aligned}  \\\\\\text{s.t.} \\quad  &amp; \\overrightarrow{w}^{T}\\overrightarrow{w}=1  \\end{aligned}\\]          $\\Sigma^{2}$ : ì •ì‚¬ì˜ í›„ í´ë˜ìŠ¤ ê°„ ë¶„ì‚°      $\\sigma_{i}^{2}$ : ì •ì‚¬ì˜ í›„ $i$ ë²ˆì§¸ í´ë˜ìŠ¤ ë‚´ ê´€ì¸¡ì¹˜ ê°„ ë¶„ì‚°      ğŸ’¡ ì„ í˜• íŒë³„ í•¨ìˆ˜ ë„ì¶œ ê³¼ì •ì˜ ì´í•´      ì •ì‚¬ì˜ í›„ ë²”ì£¼ ê°„ ë¶„ì‚° $\\Sigma^{2}$\\[\\begin{aligned}  \\Sigma^{2}  &amp;= (\\overrightarrow{\\mu}_{1}-\\overrightarrow{\\mu}_{2})(\\overrightarrow{\\mu}_{1}-\\overrightarrow{\\mu}_{2})^{T}\\\\  &amp;= (\\overrightarrow{w}^{T}\\overrightarrow{m}_{1}-\\overrightarrow{w}^{T}\\overrightarrow{m}_{2})(\\overrightarrow{w}^{T}\\overrightarrow{m}_{1}-\\overrightarrow{w}^{T}\\overrightarrow{m}_{2})^{T}\\quad(\\because \\overrightarrow{\\mu}_{i}=\\overrightarrow{w}^{T}\\overrightarrow{m}_{i})\\\\  &amp;= \\overrightarrow{w}^{T}(\\overrightarrow{m}_{1}-\\overrightarrow{m}_{2})(\\overrightarrow{m}_{1}-\\overrightarrow{m}_{2})^{T}\\overrightarrow{w}\\\\  &amp;= \\overrightarrow{w}^{T}\\mathbf{S}_{B}\\overrightarrow{w}  \\end{aligned}\\]          $\\overrightarrow{m}{i}$ : $i$ ë²ˆì§¸ ë²”ì£¼ $C{i}$ ì˜ ì¤‘ì‹¬ì  ë²¡í„°      $\\overrightarrow{\\mu}{i}=\\text{proj}{\\overrightarrow{w}}(\\overrightarrow{m}{i})$ : $\\overrightarrow{m}{i}$ ì˜ ì •ì‚¬ì˜ ë²¡í„°      $\\mathbf{S}{B}$ : ë²”ì£¼ $C{i},C_{j}$ ê°„ í¸ì°¨      $\\Sigma$ : ì •ì‚¬ì˜ í›„ ë²”ì£¼ $C_{i},C_{j}$ ê°„ í¸ì°¨            ì •ì‚¬ì˜ í›„ ë²”ì£¼ ë‚´ ë¶„ì‚° $\\sigma_{i}^{2}$\\[\\begin{aligned}  \\sigma_{i}^{2}  &amp;= \\sum_{j=1}^{|C_{i}|}{(\\overrightarrow{y}_{j}-\\overrightarrow{\\mu}_{i})(\\overrightarrow{y}_{j}-\\overrightarrow{\\mu}_{i})^{T}}\\quad(\\overrightarrow{x}_{j} \\in C_{i})\\\\  &amp;= \\sum_{j=1}^{|C_{i}|}{(\\overrightarrow{w}^{T}\\overrightarrow{x}_{j}-\\overrightarrow{w}^{T}\\overrightarrow{m}_{i})(\\overrightarrow{w}^{T}\\overrightarrow{x}_{j}-\\overrightarrow{w}^{T}\\overrightarrow{m}_{i})^{T}}\\quad(\\because \\overrightarrow{y}_{j}=\\overrightarrow{w}^{T}\\overrightarrow{x}_{j})\\\\  &amp;= \\overrightarrow{w}^{T}\\left[\\sum_{j=1}^{|C_{i}|}{(\\overrightarrow{x}_{j}-\\overrightarrow{m}_{i})(\\overrightarrow{x}_{j}-\\overrightarrow{m}_{i})^{T}}\\right]\\overrightarrow{w}\\\\  &amp;= \\overrightarrow{w}^{T}\\mathbf{S}_{i}\\overrightarrow{w}  \\end{aligned}\\]          $\\overrightarrow{x}{j} \\in C{i}$ : $i$ ë²ˆì§¸ ë²”ì£¼ $C_{i}$ ì˜ $j$ ë²ˆì§¸ ê´€ì¸¡ì¹˜ ë²¡í„°      $\\overrightarrow{y}{j}=\\text{proj}{\\overrightarrow{w}}(\\overrightarrow{x}{j})$ : $\\overrightarrow{x}{j}$ ì˜ ì •ì‚¬ì˜ ë²¡í„°      $S_{i}$ : $i$ ë²ˆì§¸ ë²”ì£¼ $C_{i}$ ì˜ ë²”ì£¼ ë‚´ ê´€ì¸¡ì¹˜ ê°„ í¸ì°¨      $\\sigma_{i}$ : ì •ì‚¬ì˜ í›„ $i$ ë²ˆì§¸ ë²”ì£¼ $C_{i}$ ì˜ ë²”ì£¼ ë‚´ ê´€ì¸¡ì¹˜ ê°„ í¸ì°¨            ëª©ì  í•¨ìˆ˜ ì¬ì •ì˜\\[\\hat{\\overrightarrow{w}}  =\\argmax_{\\overrightarrow{w}}{\\frac{\\overrightarrow{w}^{T}\\mathbf{S}_{B}\\overrightarrow{w}}{\\overrightarrow{w}^{T}(\\mathbf{S}_{1}+\\mathbf{S}_{2})\\overrightarrow{w}}}\\\\  \\begin{aligned}  \\\\\\text{s.t.} \\quad  &amp; \\overrightarrow{w}^{T}\\overrightarrow{w}=1  \\end{aligned}\\]        ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•ì„ í†µí•œ ìµœì í™” ë¬¸ì œ í’€ì´\\[\\begin{aligned}  L(\\overrightarrow{w},\\lambda)  &amp;= \\frac{\\overrightarrow{w}^{T}\\mathbf{S}_{B}\\overrightarrow{w}}{\\overrightarrow{w}^{T}(\\mathbf{S}_{1}+\\mathbf{S}_{2})\\overrightarrow{w}}-\\lambda(\\overrightarrow{w}^{T}\\overrightarrow{w}-1)\\\\\\\\  \\frac{\\partial L(\\overrightarrow{w},\\lambda)}{\\partial \\overrightarrow{w}}  &amp;= 0\\\\\\\\  \\therefore \\left[\\mathbf{S}_{B}^{-1}(\\mathbf{S}_{1}+\\mathbf{S}_{2})-\\lambda\\mathbf{I}\\right]\\hat{\\overrightarrow{w}}  &amp;=0  \\end{aligned}\\]  &lt;/br&gt;sklearn.decomposition.PCAfrom sklearn.decomposition import PCAğŸ’¡ General HyperParameter  random_state = NoneğŸ’¡ Model HyperParameter  n_components(default : 5) : ì¶•ì†Œí•  ì°¨ì›ì˜ ê°œìˆ˜  whiten(default : False) : Standard Scaling ì—¬ë¶€ğŸ’¡ Attribute  n_features_ : ì¶•ì†Œ ì „ ì°¨ì›ì˜ ê°œìˆ˜  feature_names_in_ : ì¶•ì†Œ ì „ ì°¨ì›ëª…  mean_ : ì¶•ì†Œ ì „ ì°¨ì›ë³„ í‰ê·   n_components_ : ì¶•ì†Œ í›„ ì°¨ì›ì˜ ê°œìˆ˜  components_ : ê³ ìœ ë²¡í„°  explained_variance_ : ê° ê³ ìœ ë²¡í„°ì˜ ê³ ìœ ê°’  explained_variance_ratio_ : ì „ì²´ ê³ ìœ ë²¡í„°ì˜ ê³ ìœ ê°’ ëŒ€ë¹„ ê° ê³ ìœ ë²¡í„°ì˜ ê³ ìœ ê°’&lt;/br&gt;&lt;hr&gt;ì´ë¯¸ì§€ ì¶œì²˜      http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/        https://github.com/lovit/python_ml_intro  "
  },
  
  {
    "title": "Curse_of_dimensionality",
    "url": "/posts/Curse_of_Dimensionality/",
    "categories": "",
    "tags": "",
    "date": "2024-01-12 00:00:00 +0900",
    





    
    "snippet": "Index  Curse of Dimensionality  Feature Selection&lt;/br&gt;Curse of DimensionalityğŸ’¡ ì°¨ì›ì˜ ì €ì£¼      ì •ì˜ : ê³ ì°¨ì›ì¼ìˆ˜ë¡ ì•Œê³ ë¦¬ì¦˜ì´ ì œëŒ€ë¡œ í•™ìŠµí•˜ì§€ ëª»í•˜ëŠ” í˜„ìƒ    &lt;img alt=\"ì°¨ì›ì˜ì €ì£¼\" src=\"https://github.com/jayarnim/jayarnim/a...",
    "content": "Index  Curse of Dimensionality  Feature Selection&lt;/br&gt;Curse of DimensionalityğŸ’¡ ì°¨ì›ì˜ ì €ì£¼      ì •ì˜ : ê³ ì°¨ì›ì¼ìˆ˜ë¡ ì•Œê³ ë¦¬ì¦˜ì´ ì œëŒ€ë¡œ í•™ìŠµí•˜ì§€ ëª»í•˜ëŠ” í˜„ìƒ    &lt;img alt=\"ì°¨ì›ì˜ì €ì£¼\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/0ee13c18-824c-4676-a4bb-a9b874ba4baf\" width=80%&gt;          ê´€ì¸¡ì¹˜ ê°„ ê±°ë¦¬ê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ë©€ì–´ì§ì— ë”°ë¼ ì°¨ì›ë³„ í•™ìŠµ ê°€ëŠ¥í•œ ê´€ì¸¡ì¹˜ê°€ í¬ì†Œí•´ì§            ì°¨ì› ì¶•ì†Œì˜ ë‹¹ìœ„ì„±    &lt;img alt=\"ë‹¤ì–‘ì²´í•™ìŠµ\" src=\"https://user-images.githubusercontent.com/116495744/224497076-8a2e6100-88a5-444c-abb9-377e61e961ee.jpeg\" width=80%&gt;          Manifold hypothesis      Many high-dimensional data sets that occur in the real world actually lie along low-dimensional latent manifolds inside that high-dimensional space.      ğŸ’¡ ì°¨ì› ì¶•ì†Œ ê¸°ë²•ì˜ ì¢…ë¥˜  ì°¨ì› ì„ íƒ(Feature Selection) : ìœ íš¨í•œ ì°¨ì›ì„ ì„ ë³„í•˜ëŠ” ë°©ë²•          Filter Approach                  Odds Ratio                    Wrapper Approach                  Forward Selection          Backward Elimination          Stepwise Selection                      ì°¨ì› ì¶”ì¶œ(Feature Extraction) : ì›ë³¸ì˜ íŠ¹ì§•ì„ ë³´ì¡´í•˜ëŠ” ìƒˆë¡œìš´ ì°¨ì›ì„ ì¶”ì¶œí•˜ëŠ” ë°©ë²•          $\\argmax_{\\overrightarrow{w}}{\\sigma^{2}}$                  ì£¼ì„±ë¶„ ë¶„ì„(Principle Component Analysis; PCA)          ì„ í˜• íŒë³„ ë¶„ì„(Linear Discriminant Analysis; LDA)                    $\\argmax_{\\overrightarrow{w}}{dist}$                  ë‹¤ì°¨ì› ì²™ë„ë²•(Multi-Dimensional Scaling; MDS)                    Reveal Non-Linear Structure                  t-SNE(t-distributed Stochastic Neighbor Embedding)          LLE(Locally Linear Embedding)          ISOMAP(ISOmetric feature MAPping)                    &lt;/br&gt;Feature Selection  Occamâ€™s Razor  Entities should not be multiplied beyond necessity.ğŸ’¡ Filter Approach      ìŠ¹ì‚°(Odds) : ë³€ìˆ˜ $Y$ ê°€ ë°˜ì‘í•  ê°€ëŠ¥ì„±ì´ ë°˜ì‘í•˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ë³´ë‹¤ ëª‡ ë°° ë†’ì€ê°€\\[\\begin{aligned}  \\text{odds}(Y)  &amp;= \\frac{P(Y=1)}{1-P(Y=1)}  \\end{aligned}\\]        ìŠ¹ì‚°ë¹„(Odds Ratio; OR) : ë³€ìˆ˜ $X$ ê°€ ì°¸ì¼ ë•Œ $Y$ ê°€ ë°˜ì‘í•  ê°€ëŠ¥ì„±ì´, $X$ ê°€ ê±°ì§“ì¼ ë•Œ $Y$ ê°€ ë°˜ì‘í•  ê°€ëŠ¥ì„±ë³´ë‹¤ ëª‡ ë°° ë†’ì€ê°€\\[\\begin{aligned}  \\text{OR}(Y|X)  &amp;= \\frac{\\text{odds}(X=1)}{\\text{odds}(X=0)}\\\\  &amp;= \\frac{\\frac{P(Y=1\\big|X=1)}{1-P(Y=1\\big|X=1)}}{\\frac{P(Y=1\\big|X=0)}{1-P(Y=1\\big|X=0)}}  \\end{aligned}\\]                                                      $\\text{OR}(Y              X) = 1$ : $X$ ì˜ ë³€ë™ì´ $Y$ ì˜ ë³€ë™ì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠìŒ                                                                                      $\\text{OR}(Y              X) &lt; 1$ : $X$ ëŠ” $Y$ ì™€ ìŒì˜ ìƒê´€ê´€ê³„ì— ìˆìŒ                                                                                      $\\text{OR}(Y              X) &gt; 1$ : $X$ ëŠ” $Y$ ì™€ ì–‘ì˜ ìƒê´€ê´€ê²Œì— ìˆìŒ                                                ë¡œì§€ìŠ¤í‹± íšŒê·€ì‹ ê°€ì¤‘ì¹˜ì™€ ìŠ¹ì‚°ë¹„ì˜ ìƒê´€ê´€ê³„ ì´í•´                  ë‹¨ìˆœ íšŒê·€ ë¶„ì„ í•˜ ë¡œì§€ìŠ¤í‹± íšŒê·€ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŒ\\[\\ln{\\frac{P(Y=1)}{1-P(Y=1)}}  =\\beta_0 + \\beta_1 X\\]                    $X=1$ ì¼ ë•Œì˜ ë¡œì§€ìŠ¤í‹± íšŒê·€ì‹\\[\\begin{aligned}  \\ln{\\displaystyle\\frac{P(Y=1 \\Big| X=1)}{1-P(Y=1 \\Big| X=1)}}  &amp;= \\beta_0 + \\beta_1 \\times 1 \\\\  &amp;= \\beta_0 + \\beta_1  \\end{aligned}\\]                    $X=0$ ì¼ ë•Œì˜ ë¡œì§€ìŠ¤í‹± íšŒê·€ì‹\\[\\begin{aligned}  \\ln{\\displaystyle\\frac{P(Y=1 \\Big| X=0)}{1-P(Y=1 \\Big| X=0)}}  &amp;= \\beta_0 + \\beta_1 \\times 0 \\\\  &amp;= \\beta_0  \\end{aligned}\\]                    ë‘ íšŒê·€ì‹ì„ ë¹¼ë©´ ë‹¤ìŒê³¼ ê°™ìŒ\\[\\begin{aligned}  &amp;\\ln{\\displaystyle\\frac{P(Y=1 \\Big| X=1)}{1-P(Y=1 \\Big| X=1)}} - \\ln{\\displaystyle\\frac{P(Y=1 \\Big| X=0)}{1-P(Y=1 \\Big| X=0)}}\\\\  &amp;= \\ln{\\frac{\\frac{P(Y=1 \\big| X=1)}{1-P(Y=1 \\big| X=1)}}{\\frac{P(Y=1 \\big| X=0)}{1-P(Y=1 \\big| X=0)}}}\\\\  &amp;= (\\beta_0 + \\beta_1) - \\beta_0\\\\  &amp;= \\beta_1  \\end{aligned}\\]                                                        ë”°ë¼ì„œ ì„¤ëª…ë³€ìˆ˜ $X$ ì˜ ê°€ì¤‘ì¹˜ $\\beta_1$ ê³¼ ìŠ¹ì‚°ë¹„ $\\text{OR}(Y              X)$ ê°„ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê´€ê³„ê°€ ì„±ë¦½í•¨                              \\[\\begin{aligned}  \\therefore \\text{OR}(Y|X)  &amp;= \\frac{\\frac{P(Y=1 \\big| X=1)}{1-P(Y=1 \\big| X=1)}}{\\frac{P(Y=1 \\big| X=0)}{1-P(Y=1 \\big| X=0)}} \\\\  &amp;= \\exp[\\beta_1]  \\end{aligned}\\]            ğŸ’¡ Wrapper Approach      Forward Selection : ì–´ë–¤ ë³€ìˆ˜ë„ ì„ íƒë˜ì§€ ì•Šì€ ìƒíƒœì—ì„œ ê°€ì¥ ì„¤ëª…ë ¥ì´ ì¢‹ì€ ë³€ìˆ˜ë¥¼ í•˜ë‚˜ì”© ì¶”ê°€í•˜ëŠ” ë°©ë²•\\[\\begin{aligned}  \\hat{x}_{i}&amp;=\\argmax_{x_{i}}R^2[y,f(x_{i})]\\\\  \\hat{x}_{j}&amp;=\\argmax_{x_{j}}R^2[y,f(x_{j \\ne i};\\hat{x}_{i})]\\\\  \\hat{x}_{k}&amp;=\\argmax_{x_{k}}R^2[y,f(x_{k \\ne i,j};\\hat{x}_{i},\\hat{x}_{j})]  \\end{aligned}\\\\  \\vdots\\]        Backward Elimination : ëª¨ë“  ë³€ìˆ˜ê°€ í¬í•¨ëœ ìƒíƒœì—ì„œ ì‹œì‘í•˜ì—¬ ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ë¥¼ í•˜ë‚˜ì”© ì œê±°í•˜ëŠ” ë°©ë²•\\[\\begin{aligned}  \\hat{x}_{i}&amp;=\\argmax_{x_{i}}R^2[y,f(\\not{x_{i}})]\\\\  \\hat{x}_{j}&amp;=\\argmax_{x_{j}}R^2[y,f(\\not{x_{j \\ne i}};\\not{\\hat{x}_{i}})]\\\\  \\hat{x}_{k}&amp;=\\argmax_{x_{k}}R^2[y,f(\\not{x_{k \\ne i,j}};\\not{\\hat{x}_{i}},\\not{\\hat{x}_{j}})]  \\end{aligned}\\\\  \\vdots\\]        Stepwise Selection : ì–´ë–¤ ë³€ìˆ˜ë„ ì„ íƒë˜ì§€ ì•Šì€ ìƒíƒœì—ì„œ Forward Selection ê³¼ Backward Elimination ì„ ë²ˆê°ˆì•„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•  ğŸ’¡ Metrics      Akaike Information Criteria(AIC)\\[\\begin{aligned}  \\text{AIC}  &amp;= -2\\ln{\\hat{L}}+2k  \\end{aligned}\\]                  $\\hat{L}$ : ëª¨ë¸ ì í•©ë„ë¡œì„œ $\\overrightarrow{x}{i}$ ê°€ ì£¼ì–´ì¡Œì„ ë•Œ $y{i}$ ê°€ ë°œìƒí•  ê°€ëŠ¥ì„±\\[\\begin{aligned}  \\hat{L}  &amp;= \\prod_{i=1}^{n}{P(Y=y_{i}|X=\\overrightarrow{x}_{i};\\hat{\\overrightarrow{\\theta}})}\\\\  \\hat{\\overrightarrow{\\theta}}  &amp;= \\begin{pmatrix}\\hat{\\beta_{0}}&amp;\\hat{\\beta_{1}}&amp;\\cdots&amp;\\hat{\\beta_{d}}\\end{pmatrix}  \\end{aligned}\\]                    $k$ : ëª¨ë¸ ë³µì¡ë„                  Bayesian Information Criteria(BIC)\\[\\begin{aligned}  \\text{BIC}  &amp;= -2\\ln{\\hat{L}}+k\\ln{n}  \\end{aligned}\\]  &lt;/br&gt;&lt;hr&gt;ì´ë¯¸ì§€ ì¶œì²˜      https://www.incodom.kr/%EC%B0%A8%EC%9B%90%EC%B6%95%EC%86%8C#h_85f3fb207a586b3f9b5702a3be7799e1        http://matrix.skku.ac.kr/math4ai-intro/W12/  "
  },
  
  {
    "title": "Logistic Regression",
    "url": "/posts/Logistic_Regression/",
    "categories": "Artificial Intelligence, Machine Learning",
    "tags": "writing",
    "date": "2024-01-11 00:00:00 +0900",
    





    
    "snippet": "ğŸ’¡ What? Logistic RegressionLogistic Regression      ì •ì˜ : íšŒê·€ ê¸°ë²•ì„ íŒë³„ë¶„ì„ì— í™œìš©í•˜ëŠ” ë¹„ì„ í˜• í•¨ìˆ˜ ì•Œê³ ë¦¬ì¦˜    &lt;img src=\"https://user-images.githubusercontent.com/116495744/221402155-596e45c2-5d0d-40a6-ae23-9589b48f...",
    "content": "ğŸ’¡ What? Logistic RegressionLogistic Regression      ì •ì˜ : íšŒê·€ ê¸°ë²•ì„ íŒë³„ë¶„ì„ì— í™œìš©í•˜ëŠ” ë¹„ì„ í˜• í•¨ìˆ˜ ì•Œê³ ë¦¬ì¦˜    &lt;img src=\"https://user-images.githubusercontent.com/116495744/221402155-596e45c2-5d0d-40a6-ae23-9589b48f807c.png\" width=80%&gt;\\[P(c=1)  = \\frac{1}{1+\\exp[-(\\beta_{0}+\\beta_{1}x_{1}+\\cdots+\\beta_{d}x_{d})]}\\]  Logistic Function      ì„¤ëª…ë³€ìˆ˜ì™€ ë°˜ì‘ë³€ìˆ˜ ê°€ì •                  ì–´ë– í•œ ê´€ì¸¡ì¹˜ ë²¡í„° $\\overrightarrow{x}$ ê°€ ë‹¤ìŒê³¼ ê°™ì´ ì£¼ì–´ì¡Œë‹¤ê³  í•˜ì\\[\\begin{aligned}  f(\\overrightarrow{x})  &amp;=\\beta_{0}+\\beta_{1}x_{1}+\\cdots+\\beta_{d}x_{d} \\in (-\\infty,\\infty)  \\end{aligned}\\]                    $\\overrightarrow{x}$ ì˜ ë°˜ì‘ë³€ìˆ˜ $y$ ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ\\[\\begin{aligned}  y  &amp;=c \\in \\{0,1\\}  \\end{aligned}\\]                    $y$ ì™€ $\\overrightarrow{x}$ ê°„ì—ëŠ” ê³µì—­ì´ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë“±ì‹ì´ ì„±ë¦½í•˜ì§€ ì•ŠìŒ\\[y \\ne \\beta_{0}+\\beta_{1}x_{1}+\\cdots+\\beta_{d}x_{d}\\]                  ë°˜ì‘ë³€ìˆ˜ ì¬ì •ì˜ : ë²”ì£¼ $1$ ì— ì†í•  í™•ë¥ \\[\\begin{aligned}  y  &amp;= P(c=1) \\in [0,1]  \\end{aligned}\\]        ê³µì—­ ì¡°ì •ì„ í†µí•œ ì—°ê²°í•¨ìˆ˜ $f(x)$ ë„ì¶œ                  ìŠ¹ì‚°(Odds) : ë²”ì£¼ $1$ ì— ì†í•˜ì§€ ì•Šì„ í™•ë¥  ëŒ€ë¹„ ì†í•  í™•ë¥ \\[\\begin{aligned}  f(x)  &amp;= \\text{odds}\\\\  &amp;= \\frac{P(c=1)}{1-P(c=1)} \\in [0, \\infty)  \\end{aligned}\\]                    ë¡œì§“(Logit) : ìŠ¹ì‚°ì— ìì—°ë¡œê·¸ë¥¼ ì·¨í•œ ê°’\\[\\begin{aligned}  f(x)  &amp;= \\text{logit}\\\\  &amp;= \\ln{\\frac{P(c=1)}{1-P(c=1)}} \\in (-\\infty, \\infty)  \\end{aligned}\\]                    ë¡œì§“ í•¨ìˆ˜ì™€ $\\overrightarrow{x}$ ì—°ê²°\\[\\begin{aligned}  \\text{logit}  &amp;= f(x)\\\\  &amp;= \\beta_{0}+\\beta_{1}x_{1}+\\cdots+\\beta_{d}x_{d}  \\end{aligned}\\]                  ì—°ê²°í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ ì¬ì •ì˜ëœ ë°˜ì‘ë³€ìˆ˜ì™€ ì„¤ëª…ë³€ìˆ˜ ì—°ê²°\\[\\begin{aligned}  \\ln{\\frac{P(c=1)}{1-P(c=1)}}  &amp;= f(x)\\\\  &amp;= \\beta_{0}+\\beta_{1}x_{1}+\\cdots+\\beta_{d}x_{d}\\\\\\\\  \\frac{P(c=1)}{1-P(c=1)}  &amp;= e^{f(x)}\\\\  &amp;= \\exp[\\beta_{0}+\\beta_{1}x_{1}+\\cdots+\\beta_{d}x_{d}]\\\\\\\\  \\therefore  y  &amp;= P(c=1)\\\\  &amp;= \\frac{1}{1+\\exp[-(\\beta_{0}+\\beta_{1}x_{1}+\\cdots+\\beta_{d}x_{d})]}  \\end{aligned}\\]  ğŸ’¡ sklearn.linear_model.LogisticRegressionfrom sklearn.linear_model import LogisticRegressionğŸ’¡ General HyperParameter  random_state(default : None)  warm_start(default : False) : ë™ì‘ ë©”ì‹œì§€ ì¶œë ¥ ì—¬ë¶€ ì„¤ì •  n_jobs(default : None) : ë³‘ë ¬ë¡œ ì‘ì—…í•  ì½”ì–´ ê°¯ìˆ˜ğŸ’¡ Model HyperParameter  solver(default : 'lbfgs') : íšŒê·€ì‹ ì¢…ë¥˜ë¡œì„œ ê·¸ ì•„ê·œë¨¼íŠ¸ì— ë”°ë¼ penalty ì˜ ì•„ê·œë¨¼íŠ¸ê°€ í•œì •ë¨          'lbfgs' : l2 or None      'liblinear' : l1 or l2      'newton-cg' : l2 or None      'newton-cholesky' : l2 or None      'sag' : l2 or None      'saga' : elasticnet, l1, l2 or None            fit_intercept(default : True) : ìƒìˆ˜í•­ $\\beta_0$ ì„¤ì • ì—¬ë¶€    multi_class(default : 'auto') : ë‹¤í•­ ë¶„ë¥˜ ì‹œ ë¶„ë¥˜ ê·œì¹™ ì„¤ì •          'auto' : ì´í•­ ë¶„ë¥˜ í˜¹ì€ solver ì•„ê·œë¨¼íŠ¸ê°€ 'liblinear' ì¸ ê²½ìš° 'ovr', ê·¸ì™¸ì—ëŠ” 'multinomial' ì„ ì„ íƒí•¨      'ovr' : One VS Rest      'multinomial' : solver ì•„ê·œë¨¼íŠ¸ê°€ 'liblinear' ì¸ ê²½ìš° ì ìš© ë¶ˆê°€      ğŸ’¡ To Prevent OverFittingAbout Iteration  max_iter(default : 100) : ìµœì  íšŒê·€ê³„ìˆ˜ íƒìƒ‰ ìµœëŒ€ íšŸìˆ˜About Early Stopping  tol(default : 0.0001) : í—ˆìš© ì˜¤ì°¨About Penalty  penalty(default : l2) : ê°€ì¤‘ì¹˜ ê·œì œ ë°©ë²•          None      'l1' : LASSO      'l2' : Ridge      'elasticnet' : l1, l2 í˜¼í•©            C(default : 1.0) : ê°€ì¤‘ì¹˜ ê·œì œ ì‹œ ê·œì œ ê°•ë„    l1_ratio(default : None) : elasticnet ì„¤ì • ì‹œ ì „ì²´ ê·œì œ ê°•ë„ ëŒ€ë¹„ l1 ê·œì œ ë¹„ì¤‘ğŸ’¡ To Prevent Underfitting      class_weight(default : None) : ê°€ì¤‘í•  ë²”ì£¼ì™€ ê·¸ ê°’          'balanced'      dictionary type      "
  },
  
  {
    "title": "Weight Regulation",
    "url": "/posts/Weight_Regulation/",
    "categories": "Artificial Intelligence, Machine Learning",
    "tags": "writing",
    "date": "2024-01-10 00:00:00 +0900",
    





    
    "snippet": "ğŸ’¡ Weight Regulation      p-norm : $n$ ì°¨ì› ë²¡í„° $\\overrightarrow{x}=\\begin{pmatrix}x_{1}&amp;x_{2}&amp;\\cdots&amp;x_{n}\\end{pmatrix}$ ì˜ í¬ê¸°ë¥¼ ì •ì˜í•˜ëŠ” ë°©ë²•    &lt;img alt=\"p\" src=\"https://github.com/jayarnim/j...",
    "content": "ğŸ’¡ Weight Regulation      p-norm : $n$ ì°¨ì› ë²¡í„° $\\overrightarrow{x}=\\begin{pmatrix}x_{1}&amp;x_{2}&amp;\\cdots&amp;x_{n}\\end{pmatrix}$ ì˜ í¬ê¸°ë¥¼ ì •ì˜í•˜ëŠ” ë°©ë²•    &lt;img alt=\"p\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/cfb01e04-13e7-4159-8b75-1c9201e2ca57\" width=80%&gt;\\[||x||_{p}=(|x_{1}|^{p}+|x_{2}|^{p}+\\cdots+|x_{n}|^{p})^{\\frac{1}{p}}\\]        ê°€ì¤‘ì¹˜ ê·œì œ(Weight Regulation) : íšŒê·€ê³„ìˆ˜ ìµœì ê°’ì„ íƒìƒ‰í•¨ì— ìˆì–´ íšŒê·€ê³„ìˆ˜ ë²¡í„° $\\overrightarrow{\\beta}$ ì˜ í¬ê¸°ì— ì œì•½ì„ ë‘ëŠ” ê²ƒ    &lt;img alt=\"ê°€ì¤‘ì¹˜ê·œì œ\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/49f73e40-78f6-41ce-b04c-0bfc0c54f545\" width=80%&gt;\\[\\begin{aligned}  \\overrightarrow{\\hat{\\beta}}  &amp;= \\argmin_{\\overrightarrow{\\beta}}{\\left[L_{OLS}+\\lambda||\\beta||_{p}^{2}\\right]}  \\end{aligned}\\]                  $L_{OLS}(\\overrightarrow{\\beta})$ : ìµœì†ŒììŠ¹ë²•ì— ê¸°ì´ˆí•œ ì†ì‹¤ í•¨ìˆ˜                    $\\overrightarrow{\\beta}$ : íšŒê·€ê³„ìˆ˜ ë²¡í„°                    $\\lambda$ : íšŒê·€ê³„ìˆ˜ ë²¡í„° $\\overrightarrow{\\beta}$ í¬ê¸° ì œì•½ ê°•ë„                    p : ë²¡í„° í¬ê¸° ì •ì˜ ë°©ë²•                  p=1 : LASSO          p=2 : Ridge                    ğŸ’¡ sklearn.linear_model.LassoğŸ’¡ sklearn.linear_model.Ridgefrom sklearn.linear_model import Lasso, RidgeGeneral HyperParameter  random_state(default : None)Model HyperParameter  alpha(default : 1.0) : ê·œì œ ê°•ë„  fit_intercept(default : True) : ìƒìˆ˜í•­ $\\beta_0$ ì„¤ì • ì—¬ë¶€To Prevent OverFitting  max_iter(default : 1000) : ìµœì  íšŒê·€ê³„ìˆ˜ íƒìƒ‰ ìµœëŒ€ íšŸìˆ˜Attribute  n_features_in_ : ì„¤ëª…ë³€ìˆ˜ ê°¯ìˆ˜  feature_names_in_ : ì„¤ëª…ë³€ìˆ˜ ì´ë¦„  coef_ : ì„¤ëª…ë³€ìˆ˜ë³„ ê°€ì¤‘ì¹˜  intercept_ : í¸í–¥ì´ë¯¸ì§€ ì¶œì²˜  https://ekamperi.github.io/machine%20learning/2019/10/19/norms-in-machine-learning.html  https://observablehq.com/@petulla/l1-l2l_1-l_2l1-l2-norm-geometric-interpretation"
  },
  
  {
    "title": "Gradient Descent",
    "url": "/posts/Gradient_Descent/",
    "categories": "Artificial Intelligence, Machine Learning",
    "tags": "writing",
    "date": "2024-01-09 00:00:00 +0900",
    





    
    "snippet": "ğŸ’¡ What? Gradient      ì •ì˜ : ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì— ëŒ€í•˜ì—¬ ëª¨ë“  ë°©í–¥ìœ¼ë¡œì˜ ìˆœê°„ë³€í™”ìœ¨ ë²¡í„°    &lt;img alt=\"g\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/51963f6a-3135-45b2-a61e-8f11ed724300\" width=80%&gt;\\[\\begin{al...",
    "content": "ğŸ’¡ What? Gradient      ì •ì˜ : ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì— ëŒ€í•˜ì—¬ ëª¨ë“  ë°©í–¥ìœ¼ë¡œì˜ ìˆœê°„ë³€í™”ìœ¨ ë²¡í„°    &lt;img alt=\"g\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/51963f6a-3135-45b2-a61e-8f11ed724300\" width=80%&gt;\\[\\begin{aligned}  \\nabla{f(x_{1},x_{2},\\cdots,x_{n})}  &amp;= \\begin{pmatrix}  \\displaystyle\\frac{\\partial f(x^{\\forall})}{\\partial x_{1}}\\\\  \\displaystyle\\frac{\\partial f(x^{\\forall})}{\\partial x_{2}}\\\\  \\vdots\\\\  \\displaystyle\\frac{\\partial f(x^{\\forall})}{\\partial x_{n}}\\\\  \\end{pmatrix}  \\end{aligned}\\]  ğŸ’¡ What? Gradient DescentGradient Descent      ì •ì˜ : íšŒê·€ê³„ìˆ˜ ìµœì ê°’ì„ íƒìƒ‰í•¨ì— ìˆì–´ ì†ì‹¤ í•¨ìˆ˜ì˜ ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ í™œìš©í•˜ëŠ” ë°©ë²•    &lt;img alt=\"gd\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/7659ff42-651c-49ab-9d56-b69db4454af6\" width=80%&gt;        ì ˆì°¨          íšŒê·€ê³„ìˆ˜ ë²¡í„° $\\overrightarrow{w}$ ì˜ ì´ˆê¸° ì•„ê·œë¨¼íŠ¸ ì„¤ì •      í˜„ì¬ ì•„ê·œë¨¼íŠ¸ $\\overrightarrow{w}{prev}$ ì—ì„œ ì†ì‹¤ í•¨ìˆ˜ì˜ ê·¸ë¼ë””ì–¸íŠ¸ $\\nabla{L{OLS}(\\overrightarrow{w}_{prev})}$ ê³„ì‚°      í˜„ì¬ì˜ ì•„ê·œë¨¼íŠ¸ì—ì„œ ìŒì˜ ë°©í–¥ìœ¼ë¡œ $\\alpha \\times \\nabla{L_{OLS}(\\overrightarrow{w}{prev})}$ ë§Œí¼ ì´ë™í•˜ì—¬ ìƒˆë¡œìš´ ì•„ê·œë¨¼íŠ¸ $\\overrightarrow{w}{new}$ ì ìš©      â‘¡, â‘¢ì„ ë°˜ë³µí•˜ì—¬ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ì§€ì  íƒìƒ‰      íšŒê·€ê³„ìˆ˜ ê°±ì‹  ê·œì¹™\\[\\begin{aligned}\\overrightarrow{w}_{new}&amp;= \\overrightarrow{w}_{prev} - \\alpha \\times \\nabla{L_{OLS}(\\overrightarrow{w}_{prev})}\\end{aligned}\\]  $\\overrightarrow{w}_{prev}$ : í˜„ì¬ íšŒê·€ê³„ìˆ˜ ì•„ê·œë¨¼íŠ¸  $\\overrightarrow{w}_{new}$ : ìƒˆë¡œìš´ íšŒê·€ê³„ìˆ˜ ì•„ê·œë¨¼íŠ¸  $\\alpha$ : í•™ìŠµë¥   $\\nabla{L_{OLS}(\\overrightarrow{w}_{prev})}$ : ìµœì†ŒììŠ¹ë²•ì— ê¸°ì´ˆí•˜ì—¬ ë„ì¶œí•œ ì†ì‹¤ í•¨ìˆ˜ì˜ ê·¸ë¼ë””ì–¸íŠ¸Learning Rate      ì •ì˜ : ì†ì‹¤ í•¨ìˆ˜ì— ëŒ€í•˜ì—¬ ê·¸ ê·¹ì†Œì ì„ íƒìƒ‰í•˜ê¸° ìœ„í•œ íšŒê·€ê³„ìˆ˜ ê°±ì‹  ë³´í­        í•™ìŠµë¥ ì´ ë‚®ì„ìˆ˜ë¡ ê³¼ëŒ€ì í•©ë  ê°€ëŠ¥ì„±ì´ ë†’ìŒ    &lt;img alt=\"í•™ìŠµë¥ ì´ ë‚®ì„ ë•Œ\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/24dc38b6-16d6-400b-bdaa-5ba9fcf9c286\" width=80%&gt;        í•™ìŠµë¥ ì´ ë†’ì„ìˆ˜ë¡ ê³¼ì†Œì í•©ë  ê°€ëŠ¥ì„±ì´ ë†’ìŒ    &lt;img alt=\"í•™ìŠµë¥ ì´ ë†’ì„ ë•Œ\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/406ebc5a-7555-4bce-ba95-52703e10e0fc\" width=80%&gt;  ğŸ’¡ sklearn.linear_model.SGDRegressorfrom sklearn.linear_model import SGDRegressorGeneral HyperParameter  random_state(default : None)Model HyperParameter      eta0(default : 0.1) : í•™ìŠµë¥     loss(default : 'squared_error') : ì†ì‹¤ í•¨ìˆ˜          'squared_error'      'huber'      'epsilon_insensitive'      'squared_epsilon_insensitive'        fit_intercept(default : True) : ìƒìˆ˜í•­ $\\beta_0$ ì„¤ì • ì—¬ë¶€To Prevent OverFittingAbout Iteration  max_iter : ìµœì  íšŒê·€ê³„ìˆ˜ íƒìƒ‰ ìµœëŒ€ íšŸìˆ˜About Early Stopping  early_stopping(default : False) : í•™ìŠµ ì¡°ê¸° ì¤‘ë‹¨ ì—¬ë¶€  n_iter_no_change(default : 5) : ì†ì‹¤ì´ ëª‡ íšŒ ì´ìƒ ê°œì„ ë˜ì§€ ì•Šì„ ê²½ìš° ì¤‘ë‹¨í•  ê²ƒì¸ê°€  tol(default : 0.001) : í—ˆìš© ì˜¤ì°¨About Penalty  penalty(default : None) : ê°€ì¤‘ì¹˜ ê·œì œ ë°©ë²•          'l1' : LASSO      'l2' : Ridge      'elasticnet' : l1, l2 í˜¼í•©            alpha(default : 0.0001) : ê°€ì¤‘ì¹˜ ê·œì œ ì‹œ ê·œì œ ê°•ë„    l1_ratio(default : 0.15) : elasticnet ì„¤ì • ì‹œ ì „ì²´ ê·œì œ ê°•ë„ ëŒ€ë¹„ l1 ê·œì œ ë¹„ì¤‘Attribute  n_features_in_ : ì„¤ëª…ë³€ìˆ˜ ê°¯ìˆ˜  feature_names_in_ : ì„¤ëª…ë³€ìˆ˜ ì´ë¦„  coef_ : ì„¤ëª…ë³€ìˆ˜ë³„ ê°€ì¤‘ì¹˜  intercept_ : í¸í–¥ì´ë¯¸ì§€ ì¶œì²˜  https://towardsdatascience.com/an-intuitive-explanation-of-gradient-descent-83adf68c9c33"
  },
  
  {
    "title": "Linear Regression",
    "url": "/posts/Linear_Regression/",
    "categories": "Artificial Intelligence, Machine Learning",
    "tags": "writing",
    "date": "2024-01-08 00:00:00 +0900",
    





    
    "snippet": "ğŸ’¡ What? Regression      ì •ì˜ : ë°˜ì‘ë³€ìˆ˜ì™€ ê·¸ ì„¤ëª…ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ ì¶”ì„¸ë¥¼ ìš”ì•½í•˜ëŠ” ìˆ˜ë ´ ì‘ì—…    ì„¤ëª…ë³€ìˆ˜ì˜ ê°œìˆ˜ì— ë”°ë¥¸ êµ¬ë¶„                  ë‹¨ìˆœ íšŒê·€ ëª¨í˜•(Simple Regression Model) : ë°˜ì‘ë³€ìˆ˜ì— ëŒ€í•œ ì„¤ëª…ë³€ìˆ˜ê°€ í•˜ë‚˜ì¸ ê²½ìš°(ì´í•˜ëŠ” ì„ í˜•ëª¨í˜•ì„ ê°€ì •)\\[Y=\\beta_0+\\beta_1X+\\vare...",
    "content": "ğŸ’¡ What? Regression      ì •ì˜ : ë°˜ì‘ë³€ìˆ˜ì™€ ê·¸ ì„¤ëª…ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ ì¶”ì„¸ë¥¼ ìš”ì•½í•˜ëŠ” ìˆ˜ë ´ ì‘ì—…    ì„¤ëª…ë³€ìˆ˜ì˜ ê°œìˆ˜ì— ë”°ë¥¸ êµ¬ë¶„                  ë‹¨ìˆœ íšŒê·€ ëª¨í˜•(Simple Regression Model) : ë°˜ì‘ë³€ìˆ˜ì— ëŒ€í•œ ì„¤ëª…ë³€ìˆ˜ê°€ í•˜ë‚˜ì¸ ê²½ìš°(ì´í•˜ëŠ” ì„ í˜•ëª¨í˜•ì„ ê°€ì •)\\[Y=\\beta_0+\\beta_1X+\\varepsilon\\]                    ë‹¤ì¤‘ íšŒê·€ ëª¨í˜•(Multiple Regression Model) : ë°˜ì‘ë³€ìˆ˜ì— ëŒ€í•œ ì„¤ëª…ë³€ìˆ˜ê°€ ë‘ ê°€ì§€ ì´ìƒì¸ ê²½ìš°(ì´í•˜ëŠ” ì„ í˜•ëª¨í˜•ì„ ê°€ì •)\\[Y=\\beta_0+\\beta_1 X_1+\\beta_2 X_2+\\cdots+\\beta_k X_k+\\varepsilon\\]              ì„ í˜• ê°€ì • ì—¬ë¶€ì— ë”°ë¥¸ êµ¬ë¶„          ì„ í˜• íšŒê·€ ëª¨í˜•(Linear Regression Model) : ì„¤ëª…ë³€ìˆ˜ì™€ ë°˜ì‘ë³€ìˆ˜ ê°„ ì„ í˜•ê´€ê³„ë¥¼ ê°€ì •í•˜ëŠ” ê²½ìš°                  sklearn.linear_model.LinearRegression          sklearn.linear_model.SGDRegressor                    ë¹„ì„ í˜• íšŒê·€ ëª¨í˜•(Non-Linear Regression Model) : ì„¤ëª…ë³€ìˆ˜ì™€ ë°˜ì‘ë³€ìˆ˜ ê°„ ì„ í˜•ê´€ê³„ë¥¼ ê°€ì •í•˜ì§€ ì•ŠëŠ” ê²½ìš°                  sklearn.svm.SVR          sklearn.tree.DecisionTreeRegressor                    ğŸ’¡ Linear Regressionë‹¨ìˆœ ì„ í˜• íšŒê·€ ëª¨í˜•&lt;img alt=\"reg\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/08286995-d705-4e40-927b-4b96cbd202fb\" width=80%&gt;      ë‹¨ìˆœ ì„ í˜• íšŒê·€ ëª¨í˜•ì˜ ì´í•´\\[\\begin{aligned}  &amp;Y=\\beta_0+\\beta_1X+\\varepsilon\\\\  &amp;\\Rightarrow y_i=\\beta_0+\\beta_1x_i+\\varepsilon_i  \\end{aligned}\\]          $\\varepsilon$ : ì”ì°¨í•­(Residual)                  $E[\\varepsilon]=0$                            $\\beta_0$ : í¸í–¥(Bias)            $\\beta_1$ : ê°€ì¤‘ì¹˜(Weight)            ìµœì†ŒììŠ¹ë²•ì— ê¸°ì´ˆí•œ íšŒê·€ê³„ìˆ˜ ìµœì ê°’ ë„ì¶œ                  ìµœì†ŒììŠ¹ë²•(Least Square Method; OLS) : íšŒê·€ê³„ìˆ˜ ìµœì ê°’ì„ ì”ì°¨í•­ $\\varepsilon_{i}$ ììŠ¹ì˜ í•©ê³„ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê°’ìœ¼ë¡œ íƒìƒ‰í•˜ëŠ” ë°©ë²•\\[\\begin{aligned}  \\hat{\\beta_{0}},\\hat{\\beta_{1}}  &amp;= \\argmin_{\\beta_{0},\\beta_{1}}{L_{OLS}}  \\end{aligned}\\]                    ìµœì†ŒììŠ¹ë²•ì— ê¸°ì´ˆí•œ ì†ì‹¤ í•¨ìˆ˜ $L_{OLS}$\\[\\begin{aligned}  L_{OLS}  &amp;= \\sum_{i=1}^{n}{\\varepsilon_{i}^2}\\\\  \\varepsilon_{i}  &amp;= y_{i}-\\hat{y}_{i}\\\\  &amp;= y_{i}-(\\beta_{0}+\\beta_{1}\\hat{x}_{i})  \\end{aligned}\\]                    ìµœì†ŒììŠ¹ë²•ì— ê¸°ì´ˆí•œ íšŒê·€ê³„ìˆ˜ $\\beta_{0}$ ì˜ ìµœì ê°’ $\\hat{\\beta_{0}}$\\[\\begin{aligned}  \\hat{\\beta_{0}}  &amp;= \\overline{y}-\\beta_{1}\\overline{x}  \\end{aligned}\\]                    ìµœì†ŒììŠ¹ë²•ì— ê¸°ì´ˆí•œ íšŒê·€ê³„ìˆ˜ $\\beta_{1}$ ì˜ ìµœì ê°’ $\\hat{\\beta_{1}}$\\[\\begin{aligned}  \\hat{\\beta_{1}}  &amp;= \\frac{\\sum_{i=1}^{n}{(x_{i}-\\overline{x})(y_{i}-\\overline{y})}}{\\sum_{i=1}^{n}{(x_{i}-\\overline{x})^{2}}}\\\\  &amp;=\\frac{\\sigma_{xy}}{\\sigma_{x}^{2}}  \\end{aligned}\\]            ë‹¤ì¤‘ ì„ í˜• íšŒê·€ ëª¨í˜•&lt;img alt=\"multireg\" src=\"https://github.com/jayarnim/jayarnim/assets/116495744/3c75c602-e85c-42aa-8050-ed054dec289f\" width=80%&gt;      ê´€ì¸¡ì¹˜ $i$ ì— ëŒ€í•œ ë‹¤ì¤‘ ì„ í˜• íšŒê·€ ëª¨í˜•ì´ ë‹¤ìŒê³¼ ê°™ë‹¤ê³  í•˜ì\\[y_{i}=\\beta_{0}+\\beta_{1}x_{i,1}+\\beta_{2}x_{i,2}+\\cdots+\\beta_{d}x_{i,d}\\]          $i\\in{1,2,\\cdots,n}$ : ê´€ì¸¡ì¹˜ ë²ˆí˜¸      $k\\in{1,2,\\cdots,d}$ : ì„¤ëª…ë³€ìˆ˜ ë²ˆí˜¸            $n$ ê°œì˜ ê´€ì¸¡ì¹˜ê°€ ì¡´ì¬í•œë‹¤ê³  í–ˆì„ ë•Œ ìœ„ ëª¨í˜•ì„ ì„ í˜•ëŒ€ìˆ˜ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŒ\\[\\hat{\\overrightarrow{y}} = \\hat{\\mathbf{X}}\\overrightarrow{\\beta}\\]        ìµœì†ŒììŠ¹ë²•ì— ê¸°ì´ˆí•˜ì—¬ ë„ì¶œí•œ ê°€ì¤‘ì¹˜ ë²¡í„°ë¥¼ ì •ê·œë°©ì •ì‹(Normal Equation)ì´ë¼ê³  ì •ì˜í•¨\\[\\begin{aligned}  \\hat{\\overrightarrow{\\beta}}  &amp;= \\argmin_{\\overrightarrow{\\beta}}{L_{OLS}}\\\\  &amp;= (\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\overrightarrow{y}  \\end{aligned}\\]  ğŸ’¡ sklearn.linear_model.LinearRegressionfrom sklearn.linear_model import LinearRegressionGeneral HyperParameter  n_jobs(default : None) : ë³‘ë ¬ë¡œ ì‘ì—…í•  ì½”ì–´ ê°¯ìˆ˜Model HyperParameter  fit_intercept(default : True) : ìƒìˆ˜í•­ $\\beta_0$ ì„¤ì • ì—¬ë¶€Attribute  n_features_in_ : ì„¤ëª…ë³€ìˆ˜ ê°¯ìˆ˜  feature_names_in_ : ì„¤ëª…ë³€ìˆ˜ ì´ë¦„  coef_ : ì„¤ëª…ë³€ìˆ˜ë³„ ê°€ì¤‘ì¹˜  intercept_ : í¸í–¥ì´ë¯¸ì§€ ì¶œì²˜  https://medium.com/analytics-vidhya/multiple-linear-regression-an-intuitive-approach-f874f7a6a7f9"
  },
  
  {
    "title": "Support Vector Machine",
    "url": "/posts/SVM/",
    "categories": "Artificial Intelligence, Machine Learning",
    "tags": "writing",
    "date": "2024-01-07 00:00:00 +0900",
    





    
    "snippet": "ğŸ’¡ Support Vector Machine      ì •ì˜ : ë§ˆì§„(Margin)ì„ ìµœëŒ€ë¡œ ê°€ì ¸ê°€ëŠ” ì´ˆí‰ë©´(Hyper Plane)ì„ ê·œì¹™ìœ¼ë¡œ í•˜ì—¬ ê´€ì¸¡ì¹˜ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜            ìš©ì–´ì˜ ì´í•´              ì´ˆí‰ë©´(Hyper Plane) : ë²”ì£¼ë¥¼ êµ¬ë¶„í•˜ëŠ” ê²½ê³„      ì„œí¬íŠ¸ ë²¡í„°(Support Vector) : ì¸ì ‘í•œ ë²”ì£¼ì— ...",
    "content": "ğŸ’¡ Support Vector Machine      ì •ì˜ : ë§ˆì§„(Margin)ì„ ìµœëŒ€ë¡œ ê°€ì ¸ê°€ëŠ” ì´ˆí‰ë©´(Hyper Plane)ì„ ê·œì¹™ìœ¼ë¡œ í•˜ì—¬ ê´€ì¸¡ì¹˜ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜            ìš©ì–´ì˜ ì´í•´              ì´ˆí‰ë©´(Hyper Plane) : ë²”ì£¼ë¥¼ êµ¬ë¶„í•˜ëŠ” ê²½ê³„      ì„œí¬íŠ¸ ë²¡í„°(Support Vector) : ì¸ì ‘í•œ ë²”ì£¼ì— ê°€ì¥ ê°€ê¹Œì´ ìœ„ì¹˜í•œ ë²¡í„°      ë§ˆì§„(Margin) : ì¸ì ‘í•œ ë‘ ë²”ì£¼ì˜ ì„œí¬íŠ¸ ë²¡í„°ë¥¼ ì§€ë‚˜ëŠ” í‰í–‰í•œ ë‘ ì§ì„  ì‚¬ì´ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬      ğŸ’¡ ê²°ì • í•¨ìˆ˜ ë„ì¶œì •ì˜      ì´ˆí‰ë©´ ì •ì˜\\[\\overrightarrow{w}^{T}\\overrightarrow{x}+b=0\\]          $\\overrightarrow{x}$ : ì´ˆí‰ë©´ ìœ„ì— ìœ„ì¹˜í•œ ë²¡í„°      $\\overrightarrow{w}$ : ì´ˆí‰ë©´ì˜ ë²•ì„  ë²¡í„°      $b$ : í¸í–¥ìœ¼ë¡œì„œ ì„¸ë¡œì¶• ì ˆí¸            ë²”ì£¼ ì •ì˜\\[y_{i} = \\begin{cases} +1,\\;if\\;\\overrightarrow{x}_{i} \\in X^{+}\\\\ -1,\\;if\\;\\overrightarrow{x}_{i} \\in X^{-}\\end{cases}\\]        ì„œí¬íŠ¸ ë²¡í„° ì •ì˜                  í¸ì˜ìƒ ê´€ì¸¡ì¹˜ ë²¡í„° $\\overrightarrow{x}_{\\forall}$ ì™€ ì´ˆí‰ë©´ ì‚¬ì´ ê±°ë¦¬ ì ˆëŒ€ê°’ì€ ìµœì†Œ $1$ ì´ë¼ê³  í•˜ì                    ì¢Œì¸¡ ì„œí¬íŠ¸ ë²¡í„° $\\overrightarrow{x}^{+}$ : ë²”ì£¼ $X^{+}$ ì—ì„œ ì´ˆí‰ë©´ì— ê°€ì¥ ê°€ê¹Œì´ ìœ„ì¹˜í•œ ë²¡í„°\\[\\overrightarrow{w}^{T}\\overrightarrow{x}^{+}+b=+1\\]                    ìš°ì¸¡ ì„œí¬íŠ¸ ë²¡í„° $\\overrightarrow{x}^{-}$ : ë²”ì£¼ $X^{-}$ ì—ì„œ ì´ˆí‰ë©´ì— ê°€ì¥ ê°€ê¹Œì´ ìœ„ì¹˜í•œ ë²¡í„°\\[\\overrightarrow{w}^{T}\\overrightarrow{x}^{-}+b=-1\\]            ë§ˆì§„ ë„ì¶œ      ìš°ì¸¡ ì„œí¬íŠ¸ ë²¡í„° $\\overrightarrow{x}^{-}$ ì— ëŒ€í•˜ì—¬ ë°©í–¥ $\\overrightarrow{w}$ ìœ¼ë¡œ í¬ê¸° $margin$ ë§Œí¼ ì´ë™í•˜ë©´ ì¢Œì¸¡ ì„œí¬íŠ¸ ë²¡í„° $\\overrightarrow{x}^{+}$ ì— ì•ˆì°©í•œë‹¤ê³  í•˜ì\\[\\overrightarrow{x}^{+} = \\overrightarrow{x}^{-} + margin \\cdot \\overrightarrow{w}\\]        $\\overrightarrow{w}^{T}\\overrightarrow{x}^{+}+b=1$ ì„ ë‹¤ìŒê³¼ ê°™ì´ ì¬ì •ì˜í•  ìˆ˜ ìˆìŒ\\[\\begin{aligned}  \\overrightarrow{w}^{T}\\overrightarrow{x}^{+}+b=1\\\\  \\overrightarrow{w}^{T}(\\overrightarrow{x}^{-} + margin \\cdot \\overrightarrow{w})+b=1\\\\  \\overrightarrow{w}^{T}\\overrightarrow{x}^{-} + margin \\cdot \\overrightarrow{w}^{T}\\overrightarrow{w} + b = 1\\\\  (\\overrightarrow{w}^{T}\\overrightarrow{x}^{-} + b) + margin \\cdot \\overrightarrow{w}^{T}\\overrightarrow{w} = 1\\\\  -1 + margin \\cdot ||w||^2 = 1  \\end{aligned}\\]        ë”°ë¼ì„œ ë§ˆì§„ì„ ë‹¤ìŒê³¼ ê°™ì´ ë„ì¶œí•  ìˆ˜ ìˆìŒ\\[margin = \\frac{2}{||w||^2}\\]  ë§ˆì§„ ìµœëŒ€í™”  ìµœì í™” ë¬¸ì œ ì •ì˜                  ëª©ì  í•¨ìˆ˜\\[\\max{\\frac{2}{||w||^2}}  \\Rightarrow \\min{\\frac{1}{2}||w||^2}\\]                    ì œì•½ ì¡°ê±´\\[y_{i}(\\overrightarrow{w}^{T}\\overrightarrow{x}_{i}+b) \\ge 1\\]                  ë¼ê·¸ë‘ì£¼ í•¨ìˆ˜ ë„ì¶œ\\[\\begin{aligned}  L(w,b,\\lambda)&amp;=\\frac{1}{2}||w||^2 - \\sum_{i=1}^{n}{\\lambda_{i}\\cdot\\{y_{i}(\\overrightarrow{w}^{T}\\overrightarrow{x}_{i}+b)-1\\}}  \\end{aligned}\\]          $\\lambda_{i}\\ge0$ : ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜        KKT ì¡°ê±´ í•˜ ë¼ê·¸ë‘ì£¼ ë“€ì–¼ í•¨ìˆ˜ë¡œ ë³€í™˜                  ëª©ì  í•¨ìˆ˜\\[\\begin{aligned}  g(\\lambda) &amp;= \\inf_{w,b}{L(w,b,\\lambda)}\\\\  &amp;= \\max_{\\lambda}{\\min_{w,b}{L(w,b,\\lambda)}}  \\end{aligned}\\]                    ì œì•½ ì¡°ê±´ ($\\because$ Complementary Slackness, KKT)\\[\\lambda_{i}\\cdot\\{y_{i}(\\overrightarrow{w}^{T}\\overrightarrow{x}_{i}+b)-1\\}=0\\]                  ì„œí¬íŠ¸ ë²¡í„° : \\(y_{i \\in SV}(\\overrightarrow{w}^{T}\\overrightarrow{x}_{i \\in SV}+b)-1=0\\) (\\(\\because \\overrightarrow{w}^{T}\\overrightarrow{x}_{i \\in SV}+b = 1\\))          ê·¸ ì™¸ ë²¡í„° : \\(\\lambda_{i \\notin SV}=0\\) (\\(\\because \\overrightarrow{w}^{T}\\overrightarrow{x}_{i \\notin SV}+b &gt; 1\\))                          ë¼ê·¸ë‘ì£¼ ë“€ì–¼ í•¨ìˆ˜ í’€ì´                  $\\overrightarrow{w}$, $b$ ì— ëŒ€í•˜ì—¬ í¸ë¯¸ë¶„\\[\\begin{aligned}  \\frac{\\partial L(w,b,\\lambda)}{\\partial w}  &amp;= \\overrightarrow{w} - \\sum_{i=1}^{n}{\\lambda_{i}y_{i}\\overrightarrow{x}_{i}}\\\\  &amp;= 0\\\\  \\therefore \\overrightarrow{w}^{*}  &amp;= \\sum_{i=1}^{n}{\\lambda_{i}y_{i}\\overrightarrow{x}_{i}}\\\\\\\\  \\frac{\\partial L(w,b,\\lambda)}{\\partial b}  &amp;= 0 - \\sum_{i=1}^{n}{\\lambda_{i}y_{i}}\\\\  &amp;= 0\\\\  \\therefore \\sum_{i=1}^{n}{\\lambda_{i}y_{i}}   &amp;= 0  \\end{aligned}\\]                    í¸ë¯¸ë¶„í•œ ê²°ê³¼ë¥¼ ë¼ê·¸ë‘ì£¼ ë“€ì–¼ í•¨ìˆ˜ì— ëŒ€ì…\\[\\begin{aligned}  \\frac{1}{2}||w^{*}||^2  &amp;= \\frac{1}{2}\\overrightarrow{w}^{*}\\cdot\\overrightarrow{w}^{*}\\\\  &amp;= \\frac{1}{2}\\sum_{i=1}^{n}{\\lambda_{i}y_{i}\\overrightarrow{x}_{i}} \\cdot \\sum_{j=1}^{n}{\\lambda_{j}y_{j}\\overrightarrow{x}_{j}}\\\\  &amp;= \\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}{\\lambda_{i}\\lambda_{j}y_{i}y_{j}\\overrightarrow{x}_{i}^{T}\\overrightarrow{x}_{j}}\\\\\\\\  \\sum_{i=1}^{n}{\\lambda_{i}\\cdot\\{y_{i}(\\overrightarrow{w}^{T}\\overrightarrow{x}_{i}+b)-1\\}}  &amp;= \\sum_{i=1}^{n}{\\lambda_{i}y_{i}\\overrightarrow{w}^{T}\\overrightarrow{x}_{i}} + b\\sum_{i=1}^{n}{\\lambda_{i}y_{i}} - \\sum_{i=1}^{n}{\\lambda_{i}}\\\\  &amp;= \\sum_{i=1}^{n}{\\lambda_{i}y_{i}(\\sum_{j=1}^{n}\\lambda_{j}y_{j}\\overrightarrow{x}_{j})\\overrightarrow{x}_{i}} + b \\cdot 0 - \\sum_{i=1}^{n}{\\lambda_{i}}\\\\  &amp;= \\sum_{i=1}^{n}\\sum_{j=1}^{n}{\\lambda_{i}\\lambda_{j}y_{i}y_{j}\\overrightarrow{x}_{i}\\overrightarrow{x}_{j}} - \\sum_{i=1}^{n}{\\lambda_{i}}\\\\\\\\  \\therefore g(\\lambda)  &amp;= \\inf_{w,b}{L(w,b,\\lambda)}\\\\  &amp;= \\max_{\\lambda}\\min_{w,b}{L(w,b,\\lambda)}\\\\  &amp;= \\max_{\\lambda}{L(\\lambda)}\\\\  &amp;= \\max_{\\lambda}{\\left[\\frac{1}{2}||w^{*}||^2 - \\sum_{i=1}^{n}{\\lambda_{i}\\cdot\\{y_{i}(\\overrightarrow{w}^{T}\\overrightarrow{x}_{i}+b)-1\\}}\\right]}\\\\  &amp;= \\max_{\\lambda}{\\left[\\sum_{i=1}^{n}{\\lambda_{i}} - \\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}{\\lambda_{i}\\lambda_{j}y_{i}y_{j}\\overrightarrow{x}_{i}^{T}\\overrightarrow{x}_{j}}\\right]}  \\end{aligned}\\]            ì„œí¬íŠ¸ ë²¡í„° ë„ì¶œ      ë§ˆì§„ì„ ìµœëŒ€í™”í•˜ëŠ” ë²•ì„  ë²¡í„° $\\overrightarrow{w}^{*}$\\[\\overrightarrow{w}^{*}  = \\sum_{i \\in SV}{\\lambda_{i}y_{i}\\overrightarrow{x}_{i}}\\]    ë§ˆì§„ì„ ìµœëŒ€í™”í•˜ëŠ” í¸í–¥ $b^{*}$          ë¼ê·¸ë‘ì£¼ ë“€ì–¼ í•¨ìˆ˜ì˜ ì œì•½ ì¡°ê±´ ($\\because$ Complementary Slackness, KKT)                  ì„œí¬íŠ¸ ë²¡í„° : \\(y_{i \\in SV}(\\overrightarrow{w}^{T}\\overrightarrow{x}_{i \\in SV}+b)-1=0\\) (\\(\\because \\overrightarrow{w}^{T}\\overrightarrow{x}_{i \\in SV}+b = 1\\))          ê·¸ ì™¸ ë²¡í„° : $\\lambda_{i \\notin SV}=0$ ($\\because \\overrightarrow{w}^{T}\\overrightarrow{x}_{i \\notin SV}+b &gt; 1$)                            ë§ˆì§„ì„ êµ¬í•¨ì— ìˆì–´ ê·¸ ì™¸ ë²¡í„°ëŠ” í•„ìš”í•˜ì§€ ì•ŠìŒ\\[y_{i \\in SV}(\\overrightarrow{w}^{T}\\overrightarrow{x}_{i \\in SV}+b)-1=0\\]                    ë”°ë¼ì„œ ë§ˆì§„ì„ ìµœëŒ€í™”í•˜ëŠ” ì„œí¬íŠ¸ ë²¡í„°ì˜ í¸í–¥ $b^{*}_{SV}$ ì„ ë‹¤ìŒê³¼ ê°™ì´ ë„ì¶œí•  ìˆ˜ ìˆìŒ\\[\\begin{aligned}  b^{*}  &amp;= y_{i \\in SV} - \\overrightarrow{w}^{T}\\overrightarrow{x}_{i \\in SV}\\\\  &amp;= \\frac{1}{|SV|}\\sum_{i \\in SV}{\\left[y_{i} - \\overrightarrow{w}^{T}\\overrightarrow{x}_{i}\\right]}\\\\  &amp;= \\frac{1}{|SV|}\\sum_{i \\in SV}{\\left[y_{i} - \\left(\\sum_{j \\in SV}{\\lambda_{j}y_{j}\\overrightarrow{x}_{j}}\\right)\\overrightarrow{x}_{i}\\right]}\\;(\\because \\overrightarrow{w}^{*}=\\sum_{i}{\\lambda_{i}y_{i}\\overrightarrow{x}_{i}})\\\\  &amp;= \\frac{1}{|SV|}\\sum_{i \\in SV}\\sum_{j \\in SV}{\\left[y_{i} - \\lambda_{j}y_{j}\\overrightarrow{x}_{j}\\overrightarrow{x}_{i}\\right]}  \\end{aligned}\\]                  ë§ˆì§„ì„ ìµœëŒ€í™”í•˜ëŠ” ì„œí¬íŠ¸ ë²¡í„° $\\overrightarrow{x}_{SV} \\in SV$\\[\\begin{aligned}  SV   &amp;= \\{\\overrightarrow{x}_{SV}|\\overrightarrow{w}^{*} \\cdot \\overrightarrow{x}_{SV} + b^{*}=|1|\\}\\\\  &amp;= \\left\\{ \\overrightarrow{x}_{SV}|\\left(\\sum_{i=1}^{n}{\\lambda_{i}^{*}y_{i}\\overrightarrow{x}_{i}}\\right) \\cdot \\overrightarrow{x}_{SV} + \\frac{1}{|SV|}\\sum_{i \\in SV}\\sum_{j \\in SV}{\\left[y_{i} - \\lambda_{j}y_{j}\\overrightarrow{x}_{j}\\overrightarrow{x}_{i}\\right]}=|1| \\right\\}  \\end{aligned}\\]  ìš”ì•½\\[\\begin{aligned}y_{q}&amp;= \\begin{cases}+1,\\;if\\;f(\\overrightarrow{q}) &gt; 0 \\\\-1,\\;if\\;f(\\overrightarrow{q}) &lt; 0 \\\\\\end{cases}\\\\\\\\f(\\overrightarrow{q})&amp;= \\overrightarrow{w}^{*} \\cdot \\overrightarrow{q} + b^{*}\\\\&amp;= \\left(\\sum_{i \\in SV}{\\lambda_{i}y_{i}\\overrightarrow{x}_{i}}\\right) \\cdot \\overrightarrow{q} + \\frac{1}{|SV|}\\sum_{i \\in SV}\\sum_{j \\in SV}{\\left[y_{i} - \\lambda_{j}y_{j}\\overrightarrow{x}_{j}\\overrightarrow{x}_{i}\\right]}\\end{aligned}\\]  $\\overrightarrow{q}$ : ì‹ ê·œ ê´€ì¸¡ì¹˜ ë²¡í„°  $y_{q}$ : $\\overrightarrow{q}$ ì˜ ë²”ì£¼  $f(\\overrightarrow{q})$ : ê²°ì • í•¨ìˆ˜ë¡œì„œ ì´ˆí‰ë©´ $\\overrightarrow{w}^{T}\\overrightarrow{x}+b=0$ ê³¼ ë²¡í„°ì˜ ì‚¬ì˜ ê±°ë¦¬ğŸ’¡ ì†Œí”„íŠ¸ ë§ˆì§„      í•˜ë“œ ë§ˆì§„(Hard Margin)ì˜ ë¬¸ì œì  : ì´ìƒ ê´€ì¸¡ì¹˜ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš° ë§ˆì§„ì„ ìµœëŒ€í™”í•˜ëŠ” ì´ˆí‰ë©´ì„ íƒìƒ‰í•˜ê¸° ì–´ë ¤ì›€            ì†Œí”„íŠ¸ ë§ˆì§„(Soft Margin)ì˜ í•´ê²°ì±… : ë§ˆì§„ ìœ„ë°˜ $\\xi$ ë¥¼ í—ˆìš©í•˜ì—¬ ì¼ë¶€ ì´ìƒ ê´€ì¸¡ì¹˜ë¥¼ ë°°ì œí–ˆì„ ë•Œ ë§ˆì§„ì„ ìµœëŒ€í™”í•˜ëŠ” ì´ˆí‰ë©´ì„ íƒìƒ‰í•¨              ë§ˆì§„ ìœ„ë°˜(Margin Violation; $\\xi$) : ì´ˆí‰ë©´ ê·¼ë°©ì—ì„œ ë°œìƒ ê°€ëŠ¥í•œ ì†Œìˆ˜ì˜ ì´ìƒ ê´€ì¸¡ì¹˜ì— ëŒ€í•œ ì˜¤ë¥˜ë¡œì„œ, í•´ë‹¹ ê´€ì¸¡ì¹˜ë¡œë¶€í„° ì„œí¬íŠ¸ ë²¡í„°ë¥¼ ì§€ë‚˜ê³  ì´ˆí‰ë©´ê³¼ í‰í–‰í•œ ì§ì„ ê¹Œì§€ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬      ë§ˆì§„ ìœ„ë°˜ $\\xi$ ë¥¼ ê³ ë ¤í•˜ëŠ” ìµœì í™” ë¬¸ì œ ì •ì˜      í•˜ë“œ ë§ˆì§„ì˜ ìµœì í™” ë¬¸ì œ\\[\\begin{aligned}  &amp;\\min{\\frac{1}{2}||w||^2}\\\\\\\\  \\quad \\text{s.t.} \\quad &amp;y_{i}(\\overrightarrow{w}^{T}\\overrightarrow{x}_{i}+b) \\ge 1  \\end{aligned}\\]        ì†Œí”„íŠ¸ ë§ˆì§„ì˜ ìµœì í™” ë¬¸ì œ\\[\\begin{aligned}  &amp;\\min{\\left[\\frac{1}{2}{||w||^2}+C\\sum_{i=1}^{n}{\\xi_i}\\right]}\\\\\\\\  \\quad \\text{s.t.} \\quad &amp;y_{i}(\\overrightarrow{w}^{T}\\overrightarrow{x}_{i}+b) \\ge 1-\\xi_i,\\\\  &amp;\\xi_i \\ge 0  \\end{aligned}\\]                  $\\xi_{i}$ : ê´€ì¸¡ì¹˜ ë²¡í„° $\\overrightarrow{x}_{i}$ ì— ëŒ€í•œ ë§ˆì§„ ìœ„ë°˜                    $C$ : ë§ˆì§„ ìœ„ë°˜ì— ëŒ€í•œ ê·œì œ ê°•ë„                    ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•ì— ê¸°ì´ˆí•œ í’€ì´      ë¼ê·¸ë‘ì£¼ í•¨ìˆ˜ ë„ì¶œ\\[\\begin{aligned}  L(w,b,\\lambda,\\xi,\\mu)  = &amp;\\left[\\frac{1}{2}||w||^2 - \\sum_{i=1}^{n}{\\lambda_{i}\\{y_{i}(\\overrightarrow{w}^{T}+b)-(1-\\xi_{i})\\}}\\right]\\\\  &amp;+ \\left[C\\sum_{i=1}^{n}{\\xi_{i}}-\\sum_{i=1}^{b}{\\mu_{i}\\xi_{i}}\\right]  \\end{aligned}\\]          $\\lambda \\ge 0$ : ì œì•½ ì¡°ê±´ $y_{i}(\\overrightarrow{w}^{T}\\overrightarrow{x}+b) \\ge 1-\\xi_{i}$ ì— ëŒ€í•œ ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜      $\\mu \\ge 0$ : ì œì•½ ì¡°ê±´ $\\xi_{i} \\ge 0$ ì— ëŒ€í•œ ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜            KKT ì¡°ê±´ í•˜ ë¼ê·¸ë‘ì£¼ ë“€ì–¼ í•¨ìˆ˜ ë„ì¶œ\\[\\begin{aligned}  g(\\lambda,\\mu)  &amp;= \\inf_{w,b,\\xi}{L(w,b,\\lambda,\\xi,\\mu)}\\\\  &amp;= \\max_{\\lambda,\\mu}{\\min_{w,b,\\xi}{L(w,b,\\lambda,\\xi,\\mu)}}\\\\\\\\  \\quad \\text{s.t.} \\quad &amp;\\lambda_{i}\\{y_{i}(\\overrightarrow{w}^{T}\\overrightarrow{x}+b)-(1-\\xi_{i})\\}=0,\\\\  &amp;\\mu_{i}\\xi_{i}=0  \\end{aligned}\\]        $\\overrightarrow{w}$,$b$,$\\xi$ ì— ëŒ€í•˜ì—¬ í¸ë¯¸ë¶„\\[\\begin{aligned}  \\frac{\\partial L(w,b,\\lambda,\\xi,\\mu)}{\\partial w}  &amp;= \\overrightarrow{w} - \\sum_{i=1}^{n}{\\lambda_{i}y_{i}\\overrightarrow{x}_{i}}\\\\  &amp;= 0\\\\\\\\  \\frac{\\partial L(w,b,\\lambda,\\xi,\\mu)}{\\partial b}  &amp;= \\sum_{i=1}^{n}{\\lambda_{i}y_{i}}\\\\  &amp;= 0\\\\\\\\  \\frac{\\partial L(w,b,\\lambda,\\xi,\\mu)}{\\partial \\xi}  &amp;= C-\\lambda_{i}-\\mu_{i}\\\\  &amp;= 0\\\\\\\\  \\therefore \\overrightarrow{w}^{*}  &amp;= \\sum_{i=1}^{n}{\\lambda_{i}y_{i}\\overrightarrow{x}_{i}},\\\\  \\sum_{i=1}^{n}{\\lambda_{i}y_{i}}  &amp;= 0,\\\\  \\mu_{i}  &amp;= C - \\lambda_{i}  \\end{aligned}\\]        ìœ„ ê²°ê³¼ë¥¼ ë¼ê·¸ë‘ì£¼ ë“€ì–¼ í•¨ìˆ˜ì— ëŒ€ì…í•˜ì—¬ $\\overrightarrow{w}^{}$,$b^{}$ ë„ì¶œ\\[\\begin{aligned}  \\overrightarrow{w}^{*}  &amp;= \\sum_{i \\in SV}{\\lambda_{i}y_{i}\\overrightarrow{x}_{i}}\\\\  b^{*}  &amp;= \\sum_{i \\in SV}{\\sum_{j \\in SV}{\\left[y_{i}-\\lambda_{j}y_{j}\\overrightarrow{x}_{j}\\overrightarrow{x}_{i}\\right]}}  \\end{aligned}\\]  ğŸ’¡ Kernel Trick      ì •ì˜ : ì„ í˜•ìœ¼ë¡œëŠ” êµ¬ë¶„í•˜ê¸° ì–´ë ¤ìš´ ì €ì°¨ì› ê³µê°„ìƒì˜ ë°ì´í„° ì„¸íŠ¸ë¥¼, ì ì ˆí•œ ê²°ì • ê²½ê³„ë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” ê³ ì°¨ì› ê³µê°„ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ê¸°ë²•      ë¨¸ì„œì˜ ì •ë¦¬(Mercerâ€™s Theorem)  ì €ì°¨ì› ê³µê°„ $L$ ì—ì„œ ê³ ì°¨ì› ê³µê°„ $H$ ë¡œ ê´€ì¸¡ì¹˜ë“¤ì„ ë§¤í•‘í•˜ëŠ” ì»¤ë„í•¨ìˆ˜ $K$ ëŠ” $L$ ì—ì„œ í‘œí˜„ëœ ê´€ì¸¡ì¹˜ë“¤ ê°„ ìœ í´ë¦¬ë“œ ê±°ë¦¬ì™€ $H$ ì—ì„œ í‘œí˜„ëœ ê´€ì¸¡ì¹˜ë“¤ ê°„ ìœ í´ë¦¬ë“œ ê±°ë¦¬ë¥¼ ë³´ì¡´í•¨      ì„ì˜ì˜ ê´€ì¸¡ì¹˜ $X_a, X_b$ ì— ëŒ€í•˜ì—¬, $2$ ì°¨ì› ê³µê°„ì—ì„œ í•´ë‹¹ ê´€ì¸¡ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„° $\\overrightarrow{a}, \\overrightarrow{b}$ ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•˜ì\\[\\begin{aligned}  \\overrightarrow{a}  &amp;= \\begin{pmatrix}a_1\\\\a_2\\end{pmatrix}\\\\  \\overrightarrow{b}  &amp;= \\begin{pmatrix}b_1\\\\b_2\\end{pmatrix}  \\end{aligned}\\]        $\\overrightarrow{a}, \\overrightarrow{b}$ ì„ $3$ ì°¨ì›ìƒì˜ ë²¡í„° $\\Phi(\\overrightarrow{a}),\\Phi(\\overrightarrow{b})$ ë¡œ ë§¤í•‘í•˜ëŠ” ì»¤ë„í•¨ìˆ˜ $K(\\overrightarrow{a}, \\overrightarrow{b})$ ëŠ” ë‹¤ìŒì˜ ì¡°ê±´ì„ ë§Œì¡±í•¨\\[\\begin{aligned}  K(\\overrightarrow{a}, \\overrightarrow{b})  &amp;= (\\overrightarrow{a}^T \\overrightarrow{b})^2 \\\\  &amp;= a_1^2b_1^2 + 2(a_1b_1a_2b_2) + a_2^2b_2^2 \\\\  &amp;= (a_1^2, \\sqrt{2}a_1a_2, a_2^2) \\cdot (b_1^2, \\sqrt{2}b_1b_2, b_2^2) \\\\  &amp;= \\Phi(\\overrightarrow{a}) \\cdot \\Phi(\\overrightarrow{b})  \\end{aligned}\\]  ì»¤ë„í•¨ìˆ˜ì˜ ì¢…ë¥˜      Linear\\[K(\\overrightarrow{a}, \\overrightarrow{b}) = \\overrightarrow{a}^T \\overrightarrow{b}\\]        Polynomial\\[K(\\overrightarrow{a}, \\overrightarrow{b}) = (\\gamma \\overrightarrow{a}^T \\overrightarrow{b} + r)^d\\]        Radial Basis Function(RBF)\\[K(\\overrightarrow{a}, \\overrightarrow{b}) = \\exp(-\\gamma ||\\overrightarrow{a}- \\overrightarrow{b}||^2)\\]        Hyperbolic Tangent\\[K(\\overrightarrow{a}, \\overrightarrow{b}) = \\tanh(\\gamma \\overrightarrow{a}^T \\overrightarrow{b} + r)\\]  ğŸ’¡ SVR      ì´ˆí‰ë©´\\[\\begin{aligned}  f(\\overrightarrow{x}_{i})  &amp;= \\overrightarrow{w}^{T}\\overrightarrow{x}_{i}+b  \\end{aligned}\\]        ì°¨ì´ì  : ì œì•½ ì¡°ê±´                      íŒë³„ ë¶„ì„ : ë§ˆì§„ ë²”ìœ„ ì´ë‚´ì— ê´€ì¸¡ì¹˜ ë²¡í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ\\[\\begin{aligned}  \\text{s.t.} \\quad  &amp;y_{i}(\\overrightarrow{w}^{T}\\overrightarrow{x}_{i}+b) \\ge 1 + \\xi_{i},\\\\  &amp;\\xi_{i} \\ge 0  \\end{aligned}\\]                    íšŒê·€ ë¶„ì„ : ë§ˆì§„ ë²”ìœ„ ì´ë‚´ì— ëª¨ë“  ê´€ì¸¡ì¹˜ ë²¡í„°ê°€ ì¡´ì¬í•¨\\[\\begin{aligned}  \\text{s.t.} \\quad  &amp;-(\\varepsilon + \\xi_{i}) \\le f(\\overrightarrow{x}_{i}) - y_{i} \\le \\varepsilon + \\eta_{i},\\\\  &amp;\\xi_{i},\\eta_{i} \\ge 0  \\end{aligned}\\]                  SVR ìµœì í™” ë¬¸ì œ\\[\\overrightarrow{\\hat{w}},\\hat{b},\\hat{\\xi}_{i},\\hat{\\eta}_{i}  =\\argmin_{\\overrightarrow{w},b,\\xi,\\eta}{\\left[\\frac{1}{2}||w||^{2}+C\\sum_{i=1}^{n}{(\\xi_{i}+\\eta_{i})}\\right]}\\\\  \\begin{aligned}  \\\\ \\text{s.t.} \\quad  &amp; \\varepsilon + \\xi_{i} + f(\\overrightarrow{x}) - y_{i} \\ge 0,\\\\  &amp; \\varepsilon + \\eta_{i} - f(\\overrightarrow{x}) + y_{i} \\ge 0,\\\\  &amp; \\xi_{i}, \\eta_{i} \\ge 0  \\end{aligned}\\]  ğŸ’¡ sklearn.svm.SVCfrom sklearn.svm import SVCGeneral HyperParameter  random_state(default : None)Model HyperParameter  decision_function_shape(default : 'ovr') : ë‹¤í•­ ë¶„ë¥˜ ì‹œ ê²°ì • ê²½ê³„ íƒìƒ‰ ë°©ë²•          'ovo' : One VS One(ì¼ëŒ€ì¼ êµ¬ë¶„ ê²°ì • ê²½ê³„ íƒìƒ‰)      'ovr' : One VS Rest(ì¼ëŒ€ë‹¤ êµ¬ë¶„ ê²°ì • ê²½ê³„ íƒìƒ‰)      Soft Margin      C(default : 1.0) : ë§ˆì§„ ìœ„ë°˜ì— ëŒ€í•œ ê·œì œ ê°•ë„    gamma(default : 'scale') : ê²°ì • ê²½ê³„ë¥¼ ì–¼ë§ˆë‚˜ ìœ ì—°í•˜ê²Œ ê·¸ì„ ê²ƒì¸ê°€          'auto' : $\\displaystyle\\frac{1}{n(features)}$      'scale' : $\\displaystyle\\frac{1}{n(features) \\times X.var()}$      float type        tol(default : 0.001) : í—ˆìš© ì˜¤ì°¨Kernel Trick  kernel(default : 'rbf') : ì»¤ë„í•¨ìˆ˜ ì„¤ì •          'linear' : $\\overrightarrow{a}^T \\overrightarrow{b}$      'poly' : $(\\gamma \\overrightarrow{a}^T \\overrightarrow{b} + r)^d$      'rbf' : $\\tanh(\\gamma \\overrightarrow{a}^T \\overrightarrow{b} + r)$            degree(default : 3) : ë‹¤í•­ì‹ ì»¤ë„í•¨ìˆ˜ì˜ ì°¨ìˆ˜ $d$ ì„¤ì •    coef0(default : 0.0) : ë‹¤í•­ì‹ ì»¤ë„í•¨ìˆ˜ì˜ ìƒìˆ˜í•­ $r$ ì„¤ì •To Prevent Overfitting  max_iter(default : -1) : ê²°ì • ê²½ê³„ íƒìƒ‰ ìµœëŒ€ íšŸìˆ˜To Prevent Underfitting  Imbalance in the number of observations between categories          class_weight(default : None) : ê°€ì¤‘í•  ë²”ì£¼ì™€ ê·¸ ê°’                  'balanced'          dictionary type                    ì´ë¯¸ì§€ ì¶œì²˜  https://velog.io/@shlee0125  https://medium.com/@niousha.rf/support-vector-regressor-theory-and-coding-exercise-in-python-ca6a7dfda927"
  },
  
  {
    "title": "kNN",
    "url": "/posts/kNN/",
    "categories": "Artificial Intelligence, Machine Learning",
    "tags": "writing",
    "date": "2024-01-06 00:00:00 +0900",
    





    
    "snippet": "ğŸ’¡ k-Nearest Neighbors      ì •ì˜ : ê¸°í•˜ ê±°ë¦¬ë¥¼ ê·œì¹™ìœ¼ë¡œ í•˜ì—¬ ê´€ì¸¡ì¹˜ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜            í•œê³„ì           ê²€ìƒ‰ ë¹„ìš© ë¬¸ì œ                  ê´€ì¸¡ì¹˜ ê°¯ìˆ˜ë§Œí¼ ê±°ë¦¬ë¥¼ ê³„ì‚°í•´ì•¼ í•¨                    ê±°ë¦¬ ì¸¡ì • í•¨ìˆ˜ ì„¤ì • ë¬¸ì œ                  ë¶„ì„ê°€ê°€ ë¬¸ì œì— ì í•©í•œ í•¨...",
    "content": "ğŸ’¡ k-Nearest Neighbors      ì •ì˜ : ê¸°í•˜ ê±°ë¦¬ë¥¼ ê·œì¹™ìœ¼ë¡œ í•˜ì—¬ ê´€ì¸¡ì¹˜ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜            í•œê³„ì           ê²€ìƒ‰ ë¹„ìš© ë¬¸ì œ                  ê´€ì¸¡ì¹˜ ê°¯ìˆ˜ë§Œí¼ ê±°ë¦¬ë¥¼ ê³„ì‚°í•´ì•¼ í•¨                    ê±°ë¦¬ ì¸¡ì • í•¨ìˆ˜ ì„¤ì • ë¬¸ì œ                  ë¶„ì„ê°€ê°€ ë¬¸ì œì— ì í•©í•œ í•¨ìˆ˜ë¥¼ íŒë‹¨í•´ì•¼ í•¨          ê³ ì°¨ì›ìœ¼ë¡œ ê°ˆìˆ˜ë¡ ê±°ë¦¬ ê°œë…ì´ ë¬´ì˜ë¯¸í•´ì§          ë²”ì£¼í˜• ë³€ìˆ˜ì˜ ê²½ìš° ê° ë²”ì£¼ ê°„ ê±°ë¦¬ë¥¼ ì •ì˜í•´ì•¼ í•¨                    ğŸ’¡ ê¸°í•˜ ê±°ë¦¬ ì¸¡ì • ë°©ë²•ë§¨í•´íŠ¼ ê±°ë¦¬ ì¸¡ì •ë²•      ë§¨í•´íŠ¼ ê±°ë¦¬(Manhattan Distance; L1) : ë‘ ì  ì‚¬ì´ì˜ ì—£ì§€(Edge) ê°¯ìˆ˜          $\\overrightarrow{i_{1}}, \\overrightarrow{i_{2}}, \\cdots, \\overrightarrow{i_{n}}$ ë¥¼ ê¸°ì €ë²¡í„°ë¡œ ì‚¬ìš©í•˜ëŠ” $n$ ì°¨ì› ì¢Œí‘œê³„ì— ìœ„ì¹˜í•œ ë‘ ë²¡í„° $\\overrightarrow{a}, \\overrightarrow{b}$ ì— ëŒ€í•˜ì—¬ ê° ì¶• ë°©í–¥ìœ¼ë¡œì˜ ê¸°ì €ë²¡í„° ë‹¨ìœ„ ê±°ë¦¬ë¥¼ í•©ì‚°í•œ ê°’            ê³„ì‚° ë°©ë²•                  ë²¡í„° $\\overrightarrow{a},\\overrightarrow{b}$ ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•˜ì\\[\\begin{aligned}  \\overrightarrow{a}  &amp;= \\begin{pmatrix}a_1&amp;a_2&amp;\\cdots&amp;a_n\\end{pmatrix}\\\\  \\overrightarrow{b}  &amp;= \\begin{pmatrix}b_1&amp;b_2&amp;\\cdots&amp;b_n\\end{pmatrix}  \\end{aligned}\\]                    $\\overrightarrow{a}$ ì™€ $\\overrightarrow{b}$ ì˜ ë§¨í•´íŠ¼ ê±°ë¦¬ $d_{L1}(\\overrightarrow{a},\\overrightarrow{b})$ ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ\\[\\begin{aligned}  d(\\overrightarrow{a},\\overrightarrow{b})  &amp;= || \\overrightarrow{a} - \\overrightarrow{b} ||_{L1} \\\\  &amp;= \\displaystyle\\sum_{i=1}^{n} |a_i - b_i|  \\end{aligned}\\]            ìœ í´ë¦¬ë“œ ê±°ë¦¬ ì¸¡ì •ë²•  ìœ í´ë¦¬ë“œ ê±°ë¦¬(Euclidean Distance; L2) : ë‘ ì  ì‚¬ì´ì˜ ì§ì„  ê±°ë¦¬          $\\overrightarrow{i_{1}}, \\overrightarrow{i_{2}}, \\cdots, \\overrightarrow{i_{n}}$ ë¥¼ ê¸°ì €ë²¡í„°ë¡œ ì‚¬ìš©í•˜ëŠ” $n$ ì°¨ì› ì¢Œí‘œê³„ì— ìœ„ì¹˜í•œ ë‘ ë²¡í„° $\\overrightarrow{a}, \\overrightarrow{b}$ ì— ëŒ€í•˜ì—¬ ê° ì¶• ë°©í–¥ìœ¼ë¡œì˜ ê¸°ì €ë²¡í„° ë‹¨ìœ„ ê±°ë¦¬ì˜ ì œê³±ì„ í•©ì‚°í•œ í›„ ì œê³±ê·¼í•œ ê°’        ê³„ì‚° ë°©ë²•                  ë²¡í„° $\\overrightarrow{a},\\overrightarrow{b}$ ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•˜ì\\[\\begin{aligned}  \\overrightarrow{a}  &amp;= \\begin{pmatrix}a_1&amp;a_2&amp;\\cdots&amp;a_n\\end{pmatrix}\\\\  \\overrightarrow{b}  &amp;= \\begin{pmatrix}b_1&amp;b_2&amp;\\cdots&amp;b_n\\end{pmatrix}  \\end{aligned}\\]                    $\\overrightarrow{a}$ ì™€ $\\overrightarrow{b}$ ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬ $d_{L2}(\\overrightarrow{a},\\overrightarrow{b})$ ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ\\[\\begin{aligned}  d(\\overrightarrow{a},\\overrightarrow{b})  &amp;= || \\overrightarrow{a} - \\overrightarrow{b} ||_{L2} \\\\  &amp;= \\sqrt{\\displaystyle\\sum_{i=1}^{n} (a_i - b_i)^2}  \\end{aligned}\\]            ì½”ì‚¬ì¸ ê±°ë¦¬ ì¸¡ì •ë²•      ì½”ì‚¬ì¸ ê±°ë¦¬(Cosine Distance; $\\cos$) : ì„ì˜ì˜ ë‘ ì ì— ëŒ€í•˜ì—¬, ì›ì ê³¼ ê° ì ì„ ì‡ëŠ” ì§ì„ ì˜ ì‚¬ì´ê° $\\theta$ ì˜ ì½”ì‚¬ì¸ ê°’        ê³„ì‚° ë°©ë²•                  ë²¡í„° $\\overrightarrow{a},\\overrightarrow{b}$ ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•˜ì\\[\\begin{aligned}  \\overrightarrow{a}  &amp;= \\begin{pmatrix}a_1&amp;a_2&amp;\\cdots&amp;a_n\\end{pmatrix}\\\\  \\overrightarrow{b}  &amp;= \\begin{pmatrix}b_1&amp;b_2&amp;\\cdots&amp;b_n\\end{pmatrix}  \\end{aligned}\\]                    $\\overrightarrow{a}$ ì™€ $\\overrightarrow{b}$ ì˜ ì½”ì‚¬ì¸ ê±°ë¦¬ $d_{cos}(\\overrightarrow{a},\\overrightarrow{b})$ ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ\\[\\begin{aligned}  d(\\overrightarrow{a},\\overrightarrow{b})  &amp;= \\cos{\\theta}\\\\  &amp;= \\frac{\\overrightarrow{a}^{T}\\overrightarrow{b}}{||a||\\cdot||b||}\\\\  &amp;= \\frac{\\sum_{i=1}^{n}{a_{i}b_{i}}}{\\sqrt{\\sum_{i=1}^{n}{a_{i}^{2}}} \\cdot \\sqrt{\\sum_{i=1}^{n}{b_{i}^{2}}}}\\;(a_{i^{\\forall}}\\in\\overrightarrow{a},b_{i^{\\forall}}\\in\\overrightarrow{b})  \\end{aligned}\\]            í•˜ë²„ì‚¬ì¸ ê±°ë¦¬ ì¸¡ì •ë²•      í•˜ë²„ì‚¬ì¸ ê±°ë¦¬(Haversine Distance; $\\text{hav}$) : êµ¬ í‘œë©´ìƒì— ì¡´ì¬í•˜ëŠ” ë‘ ì§€ì ì— ëŒ€í•˜ì—¬, ìœ„ë„($\\varphi$), ê²½ë„($\\lambda$) ë° í˜¸ ì¤‘ì‹¬ê°($\\Theta$)ì„ í™œìš©í•˜ì—¬ ì¸¡ì •í•œ í˜¸ì˜ ê¸¸ì´        ê³„ì‚° ë°©ë²•                  ë°˜ì§€ë¦„ì´ $r$, í˜¸ $\\overline{AB}$ ì˜ ì¤‘ì‹¬ê°ì´ $\\Theta$ ì¼ ë•Œ, í˜¸ $\\overline{AB}$ ì˜ ê¸¸ì´ $d(A,B)$ ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ\\[\\begin{aligned}  d(A,B)  &amp;= r \\cdot \\Theta  \\end{aligned}\\]                    ì  $A$,$B$ ì˜ ìœ„ë„ê°€ $\\varphi_{A}$,$\\varphi_{B}$, ê²½ë„ê°€ $\\lambda_{A}$,$\\lambda_{B}$ ì´ê³ , í˜¸ $\\overline{AB}$ ì˜ ì¤‘ì‹¬ê°ì´ $\\Theta$ ì¼ ë•Œ, í˜¸ì˜ ê¸¸ì´ $\\text{hav}{\\Theta}$ ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ\\[\\begin{aligned}  \\text{hav}{\\Theta}  &amp;= \\text{hav}(\\varphi_{B}-\\varphi_{A})  + \\cos{\\varphi_{A}} \\cdot \\cos{\\varphi_{B}} \\cdot \\text{hav}(\\lambda_{B}-\\lambda_{A})  \\end{aligned}\\]                    $\\sin$, $\\cos$, $\\text{hav}$ ì˜ ê´€ê³„ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ\\[\\begin{aligned}  \\text{hav}{\\Theta}  &amp;= \\sin^{2}{\\frac{\\Theta}{2}}\\\\  &amp;= \\frac{1-\\cos{\\Theta}}{2}  \\end{aligned}\\]                    ë”°ë¼ì„œ ë°˜ì§€ë¦„, ê²½ë„, ìœ„ë„ê°€ ì£¼ì–´ì¡Œì„ ë•Œ í˜¸ $\\overline{AB}$ ì˜ ê¸¸ì´ $d(A,B)$ ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ\\[\\begin{aligned}  d(A,B)  &amp;= r \\cdot \\Theta\\\\  &amp;= r \\cdot \\text{archav}(\\text{hav}{\\Theta})\\\\  &amp;= 2r \\cdot \\arcsin(\\sqrt{\\text{hav}{\\Theta}})\\\\  &amp;= 2r \\cdot \\arcsin\\left(\\sqrt{\\text{hav}(\\varphi_{B}-\\varphi_{A}) + \\cos{\\varphi_{A}} \\cdot \\cos{\\varphi_{B}} \\cdot \\text{hav}(\\lambda_{B}-\\lambda_{A})}\\right)  \\end{aligned}\\]            ğŸ’¡ sklearn.neighbors.KNeighborsClassifierfrom sklearn.neighbors import KNeighborsClassifierGeneral HyperParameter  n_jobs(default : None) : ë³‘ë ¬ë¡œ ì‘ì—…í•  ì½”ì–´ ê°¯ìˆ˜Model HyperParameter      n_neighbors(default : 5) : ì°¸ì¡°í•  ê·¼ì ‘ ë²¡í„°ì˜ ê°¯ìˆ˜    metric(default : 'minkowski') : ê±°ë¦¬ ì¸¡ì • ë°©ë²•          'l1', 'manhattan' or 'cityblock' : ë§¨í•´íŠ¼ ê±°ë¦¬ ì¸¡ì •ë²•      'l2' or 'euclidean' : ìœ í´ë¦¬ë“œ ê±°ë¦¬ ì¸¡ì •ë²•      'cosine' : ì½”ì‚¬ì¸ ê±°ë¦¬ ì¸¡ì •ë²•      'haversine' : í•˜ë²„ì‚¬ì¸ ê±°ë¦¬ ì¸¡ì •ë²•      callable            p(default : 2) : metric ì˜ ì•„ê·œë¨¼íŠ¸ê°€ 'minkowski' ì¸ ê²½ìš° ì¶”ê°€ ì„¤ì •\\[minkowski=\\left(\\displaystyle\\sum_{i=1}^{n}{|x_i-y_i|^p}\\right)^{\\frac{1}{p}}\\]          1 : ë§¨í•´íŠ¼ ê±°ë¦¬ ì¸¡ì •ë²•      2 : ìœ í´ë¦¬ë“œ ê±°ë¦¬ ì¸¡ì •ë²•        weights(default : 'uniform') : ê·¼ì ‘ ë²¡í„°ì— ëŒ€í•˜ì—¬ ê°€ì¤‘ì¹˜ ë¶€ì—¬ ë°©ë²• ì„¤ì •          None      'uniform' : ê·¼ì ‘ ë²¡í„°ì— ëŒ€í•˜ì—¬ ë™ë“±í•œ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•¨      'distance' : ê·¼ì ‘ ë²¡í„°ì— ëŒ€í•˜ì—¬ ê±°ë¦¬ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•¨      callable      ì´ë¯¸ì§€ ì¶œì²˜  https://076923.github.io/posts/Python-opencv-43/"
  },
  
  {
    "title": "Naive Bayes",
    "url": "/posts/Naive_Bayes/",
    "categories": "Artificial Intelligence, Machine Learning",
    "tags": "writing",
    "date": "2024-01-05 00:00:00 +0900",
    





    
    "snippet": "ğŸ’¡ Naive Bayes      ì •ì˜ : ì¡°ê±´ë¶€ í™•ë¥ ì— ê¸°ì´ˆí•˜ì—¬ ê´€ì¸¡ì¹˜ì˜ ë²”ì£¼ë¥¼ íŒë³„í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜                  ì¡°ê±´ë¶€ í™•ë¥ ì˜ ì´í•´\\[\\begin{aligned}  P(B|A)  &amp;= \\frac{P(A,B)}{P(A)}  \\end{aligned}\\]                                            ...",
    "content": "ğŸ’¡ Naive Bayes      ì •ì˜ : ì¡°ê±´ë¶€ í™•ë¥ ì— ê¸°ì´ˆí•˜ì—¬ ê´€ì¸¡ì¹˜ì˜ ë²”ì£¼ë¥¼ íŒë³„í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜                  ì¡°ê±´ë¶€ í™•ë¥ ì˜ ì´í•´\\[\\begin{aligned}  P(B|A)  &amp;= \\frac{P(A,B)}{P(A)}  \\end{aligned}\\]                                                                              $$P(B                  A)\\(: ì‚¬ê±´\\)A\\(ê°€ ë°œìƒí•œ ìƒíƒœì—ì„œ ì‚¬ê±´\\)B$$ ê°€ ë°œìƒí•  í™•ë¥                                                               \\(P(A,B)\\) : ì‚¬ê±´ \\(A\\),\\(B\\) ê°€ ê³µë™ìœ¼ë¡œ ë°œìƒí•  í™•ë¥           \\(P(A)\\) : ì‚¬ê±´ \\(A\\) ê°€ ë°œìƒí•  í™•ë¥           \\(P(B)\\) : ì‚¬ê±´ \\(B\\) ê°€ ë°œìƒí•  í™•ë¥                           í•œê³„ì  : Class Conditional Independent Assumption                  Class Conditional Independent Assumption : ë²”ì£¼ ë‚´ì—ì„œ íŠ¹ì • íŠ¹ì„±ì˜ ì¡´ì¬ ë˜ëŠ” ë¶€ì¬ê°€ ë‹¤ë¥¸ íŠ¹ì„±ì˜ ì¡´ì¬ ë˜ëŠ” ë¶€ì¬ì™€ ë…ë¦½ì ì´ë¼ëŠ” ê°€ì •\\[\\begin{aligned}  P(X_{1},X_{2},\\cdots,X_{n}|Y)  &amp;= P(X_{1}|Y) \\times P(X_{2}|Y) \\times \\cdots \\times P(X_{n}|Y)  \\end{aligned}\\]            ğŸ’¡ ê²°ì • í•¨ìˆ˜ ë„ì¶œ      ë¬¸ì œ ì •ì˜\\[\\begin{aligned}  \\hat{Y}  &amp;= f(\\overrightarrow{x})\\\\  &amp;= \\text{arg} \\max_{Y}{P(Y=i|X_{1}=x_{1},X_{2}=x_{2},\\cdots,X_{n}=x_{n})}  \\end{aligned}\\]          ê´€ì¸¡ì¹˜ ë²¡í„° \\(\\overrightarrow{x}\\) ì˜ ë²”ì£¼ \\(\\hat{Y}\\) ëŠ” \\(\\overrightarrow{x}\\) ê°€ \\((x_{1},x_{2},\\cdots,x_{n})\\) ë¡œ ì£¼ì–´ì¡Œì„ ë•Œ, ë²”ì£¼ \\(Y\\) ê°€ \\(i=1,2,\\cdots\\) ì¼ í™•ë¥ ì´ ìµœëŒ€ì¸ \\(i\\) ì„            ë² ì´ì¦ˆ ì •ë¦¬ì— ì˜í•´ ë‹¤ìŒì´ ì„±ë¦½í•¨\\[\\begin{aligned}  &amp;P(Y=i|X_{1}=x_{1},X_{2}=x_{2},\\cdots,X_{n}=x_{n})\\\\  &amp;= \\frac{P(X_{1}=x_{1},X_{2}=x_{2},\\cdots,X_{n}=x_{n}|Y=i) \\cdot P(Y=i)}{P(X_{1}=x_{1},X_{2}=x_{2},\\cdots,X_{n}=x_{n})}  \\end{aligned}\\]                                                      $$P(Y=i              X_{1}=x_{1},X_{2}=x_{2},\\cdots,X_{n}=x_{n})\\(: ê´€ì¸¡ì¹˜ ë²¡í„°\\)\\overrightarrow{x}=(x_{1},x_{2},\\cdots,x_{n})\\(ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, í•´ë‹¹ ê´€ì¸¡ì¹˜ì˜ ë²”ì£¼\\)Y$$ ê°€ $i$ ì¼ í™•ë¥                                                                                       $$P(X_{1}=x_{1},X_{2}=x_{2},\\cdots,X_{n}=x_{n}              Y=i)\\(: ë²”ì£¼\\)Y=i\\(ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ê´€ì¸¡ì¹˜ ë²¡í„°\\)\\overrightarrow{x}\\(ê°€\\)(x_{1},x_{2},\\cdots,x_{n})$$ ì¼ í™•ë¥                                           \\(P(Y=i)\\) : ë²”ì£¼ \\(Y\\) ê°€ \\(i\\) ì¼ í™•ë¥       \\(P(X_{1}=x_{1},X_{2}=x_{2},\\cdots,X_{n}=x_{n})\\) : ê´€ì¸¡ì¹˜ ë²¡í„° \\(\\overrightarrow{x}$ ê°€ $(x_{1},x_{2},\\cdots,x_{n})\\) ì¼ í™•ë¥             í´ë˜ìŠ¤ ì¡°ê±´ë¶€ ë…ë¦½ ê°€ì • í•˜ ë‹¤ë³€ëŸ‰ ì¡°ê±´ë¶€ í™•ë¥ ì„ ë‹¨ë³€ëŸ‰ ì¡°ê±´ë¶€ í™•ë¥ ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŒ\\[\\begin{aligned}  &amp;P(X_{1},X_{2},\\cdots,X_{n}|Y)\\\\  &amp;= P(X_{1}|Y) \\times P(X_{2}|Y) \\times \\cdots \\times P(X_{n}|Y)  \\end{aligned}\\]        ë”°ë¼ì„œ ê´€ì¸¡ì¹˜ ë²¡í„° \\(\\overrightarrow{x}\\) ì˜ ë²”ì£¼ \\(Y\\) ê°€ \\(i\\) ì¼ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ìŒ\\[\\begin{aligned}  &amp;P(Y=i|X_{1}=x_{1},X_{2}=x_{2},\\cdots,X_{n}=x_{n})\\\\  &amp;= \\frac{P(X_{1}=x_{1},X_{2}=x_{2},\\cdots,X_{n}=x_{n}|Y=i) \\cdot P(Y=i)}{P(X_{1}=x_{1},X_{2}=x_{2},\\cdots,X_{n}=x_{n})}\\\\  &amp;= \\frac{P(X_{1}=x_{1},X_{2}=x_{2},\\cdots,X_{n}=x_{n}|Y=i) \\cdot P(Y=i)}{\\sum_{j}P(X_{1}=x_{1},X_{2}=x_{2},\\cdots,X_{n}=x_{n}|Y=j)}\\\\  &amp;= \\frac{\\prod_{k=1}^{n}{P(X_{k}=x_{k}|Y=i)} \\cdot P(Y=i)}{\\sum_{j}{\\prod_{k=1}^{n}{P(X_{k}=x_{k}|Y=j)}}}  \\end{aligned}\\]  ğŸ’¡ ë¼í”Œë¼ìŠ¤ í‰í™œí™”      ë¼í”Œë¼ìŠ¤ í‰í™œí™”(Laplace Smoothing) : í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì‚¬ë¡€ \\(\\overrightarrow{x}_{k}\\) ì— ëŒ€í•œ í™•ë¥ ì„ \\(0\\) ìœ¼ë¡œ ë¶€ì—¬í•˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ê¸°ë²•        ê³„ì‚° ë°©ë²•\\[P(\\overrightarrow{x}_{k}|Y)  = \\frac{\\text{count}(\\overrightarrow{x}_{k},Y)+\\alpha}{\\text{count}(Y)+2\\alpha}\\]                                                      $$P(\\overrightarrow{x}_{k}              Y)\\(: ê´€ì¸¡ì¹˜ ë²¡í„°\\)\\overrightarrow{x}_{k}\\(ê°€ ë²”ì£¼\\)Y$$ ì— ì†í•  ì¡°ê±´ë¶€ í™•ë¥                                           \\(\\text{count}(\\overrightarrow{x}_{k},Y)\\) : ê´€ì¸¡ì¹˜ ë²¡í„° \\(\\overrightarrow{x}_{k}\\) ì™€ ë²”ì£¼ \\(Y\\) ì˜ ë™ì‹œ ì¶œí˜„ ë¹ˆë„      \\(\\text{count}(Y)\\) : ë²”ì£¼ \\(Y\\) ì˜ ì¶œí˜„ ë¹ˆë„      \\(\\alpha\\) : ë¼í”Œë¼ìŠ¤ í‰í™œí™” ê°•ë„      ğŸ’¡ sklearn.naive_bayes.MultinomialNBfrom sklearn.naive_bayes import MultinomialNBLaplace Smoothing  alpha(default : 1.0) : ë¼í”Œë¼ìŠ¤ í‰í™œí™” ê°•ë„  force_alpha(default : False) : alpha í•˜í•œì„ ì„ \\(1e-10\\) ìœ¼ë¡œ ê°•ì œí• ì§€ ì—¬ë¶€Learn Class Prior Probabilities  fit_prior(default : True) : í´ë˜ìŠ¤ ì‚¬ì „ í™•ë¥  í•™ìŠµ ì—¬ë¶€          True : í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ì˜ í´ë˜ìŠ¤ ì‚¬ì „ í™•ë¥ ì„ í•™ìŠµí•¨      False : í´ë˜ìŠ¤ ì‚¬ì „ í™•ë¥ ì„ ê· ë“±í•˜ê²Œ ë¶€ì—¬í•¨        class_prior(default : None) : í´ë˜ìŠ¤ ì‚¬ì „ í™•ë¥  ê°•ì œ ì§€ì •"
  },
  
  {
    "title": "Decision Tree",
    "url": "/posts/Decision_Tree/",
    "categories": "Artificial Intelligence, Machine Learning",
    "tags": "writing",
    "date": "2024-01-04 00:00:00 +0900",
    





    
    "snippet": "ğŸ’¡ Decision Tree      ì •ì˜ : ìˆœë„(Uniformity)ë¥¼ ìµœëŒ€ë¡œ ê°€ì ¸ê°€ëŠ” ì´ì§„ íŒë³„ ê·œì¹™ë“¤ë¡œ êµ¬ì„±ëœ ìˆ˜í˜•ë„(Tree)ë¥¼ ì„¸ìš°ê³  ê´€ì¸¡ì¹˜ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜            êµ¬ì¡°              ë£¨íŠ¸ ë…¸ë“œ(Root Node) : ê¹Šì´ê°€ 0ì¸ ê¼­ëŒ€ê¸° ë…¸ë“œë¡œì„œ ìµœìƒìœ„ ë…¸ë“œ      ê²°ì • ë…¸ë“œ(Decision Node) : ê·œì¹™ ì¡°...",
    "content": "ğŸ’¡ Decision Tree      ì •ì˜ : ìˆœë„(Uniformity)ë¥¼ ìµœëŒ€ë¡œ ê°€ì ¸ê°€ëŠ” ì´ì§„ íŒë³„ ê·œì¹™ë“¤ë¡œ êµ¬ì„±ëœ ìˆ˜í˜•ë„(Tree)ë¥¼ ì„¸ìš°ê³  ê´€ì¸¡ì¹˜ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜            êµ¬ì¡°              ë£¨íŠ¸ ë…¸ë“œ(Root Node) : ê¹Šì´ê°€ 0ì¸ ê¼­ëŒ€ê¸° ë…¸ë“œë¡œì„œ ìµœìƒìœ„ ë…¸ë“œ      ê²°ì • ë…¸ë“œ(Decision Node) : ê·œì¹™ ì¡°ê±´      ë¦¬í”„ ë…¸ë“œ(Leaf Node) : í•˜ìœ„ ë…¸ë“œê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë…¸ë“œë¡œì„œ ìµœì¢… ë²”ì£¼      ì„œë¸ŒíŠ¸ë¦¬(Subtree) : ì–´ë– í•œ ê·œì¹™ ë…¸ë“œë¥¼ ë£¨íŠ¸ ë…¸ë“œë¡œ ê°€ì§€ëŠ” í•˜ìœ„ íŠ¸ë¦¬ë¡œì„œ íŒë³„ ê·œì¹™ ì§‘í•©ì˜ ë¶€ë¶„ì§‘í•©      ğŸ’¡ ì¬ê·€ì  ë¶„ê¸°(Recursive Partitioning)      ì •ì˜ : íŒë³„ ê·œì¹™ì„ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ ë…¸ë“œë¥¼ ë¶„í• í•˜ì—¬ ìˆœë„ê°€ ë†’ì€ í•˜ìœ„ ë…¸ë“œë¥¼ ìƒì„±í•˜ëŠ” ë°˜ë³µì ì¸ ê³¼ì •                  íŒë³„ ê·œì¹™ : ì–´ë–¤ ë¶„ê¸°ì—ì„œ í•˜ë‚˜ì˜ ì„¤ëª…ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±í•œ ë¶„í•  ì¡°ê±´                            ìˆœë„(Purity) : ì–´ë–¤ ë…¸ë“œì— ì†í•œ ê´€ì¸¡ì¹˜ë“¤ì´ ë™ì¼í•œ ë²”ì£¼ì— ì†í•˜ëŠ” ì •ë„                          ìˆœë„ë¥¼ ì •í™•íˆ ì¸¡ì •í•˜ê¸° ì–´ë ¤ìš°ë¯€ë¡œ ê·¸ ëŒ€ë¦¬ë³€ìˆ˜ë¡œì„œ ë¶ˆìˆœë„(Impurity)ë¥¼ ì‚¬ìš©í•¨                    íŒë³„ ê·œì¹™      ì–´ë–¤ ë…¸ë“œì— ëŒ€í•˜ì—¬, ì„¤ëª…ë³€ìˆ˜ $X_{i} \\ge x_{i}$ ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•´ë‹¹ ë…¸ë“œë¥¼ ë¶„í• í•œë‹¤ê³  í•˜ì\\[y=\\begin{cases}  \\begin{aligned}  N_{left},\\;&amp;if\\;X_{i} \\ge x_{i}\\\\  N_{right},\\;&amp;if\\;X_{i} &lt; x_{i}\\\\  \\end{aligned}  \\end{cases}\\]        íŒë³„ ê·œì¹™ $X_{i} \\ge x_{i}$ ì˜ ë¹„ìš© $J(X_{i} \\ge x_{i})$ ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ\\[\\begin{aligned}  J(X_{i} \\ge x_{i})  &amp;= \\frac{m_{left}}{m}I_{left} + \\frac{m_{right}}{m}I_{right}  \\end{aligned}\\]          $m$ : ê²°ì • ë…¸ë“œì— ì†í•œ ê´€ì¸¡ì¹˜ ê°¯ìˆ˜      $m_{left}$ : ì¢Œì¸¡ í•˜ìœ„ ë…¸ë“œë¡œ ë¶„ê¸°í•œ ê´€ì¸¡ì¹˜ ê°¯ìˆ˜      $I_{left}$ : ì¢Œì¸¡ í•˜ìœ„ ë…¸ë“œì˜ ë¶ˆìˆœë„      $m_{right}$ : ìš°ì¸¡ í•˜ìœ„ ë…¸ë“œë¡œ ë¶„ê¸°í•œ ê´€ì¸¡ì¹˜ ê°¯ìˆ˜      $I_{right}$ : ìš°ì¸¡ í•˜ìœ„ ë…¸ë“œì˜ ë¶ˆìˆœë„            ì„¤ëª…ë³€ìˆ˜ $X_{i}$ ê¸°ì¤€ ë¶„í•  ì‹œ ìµœì ì˜ ë¶„ê¸°ì  $\\hat{x}_{i}$ ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ\\[\\hat{x}_{i}  =\\text{arg} \\min_{x_{i}}{J(X_{i} \\ge x_{i})}\\]        íŠ¹ì • ë…¸ë“œë¥¼ ë¶„í• í•˜ëŠ” ìµœì ì˜ ì„¤ëª…ë³€ìˆ˜ $\\hat{X}_{i}$ ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ\\[\\hat{X}_{i}  = \\text{arg} \\min_{X_{i}}{\\left\\{\\min{J(X_{1})},\\min{J(X_{2})},\\cdots,\\min{J(X_{n})}\\right\\}}\\]  ë¶ˆìˆœë„      ì§€ë‹ˆ ì§€ìˆ˜(Gini Index) : ë¶ˆìˆœë„ë¥¼ ê²½ì œì  ë¶ˆí‰ë“± ê°œë…ì— ê¸°ì´ˆí•˜ì—¬ ê³„ì‚°í•œ ì§€í‘œ\\[\\begin{aligned}  I(N_{k})  &amp;= 1-\\sum_{i=1}^{c}{p_{i}^2}  \\end{aligned}\\]          $c$ : ë²”ì£¼ ê°¯ìˆ˜      $p_{i}$ : ë…¸ë“œ $N_{k}$ ì—ì„œ $i$ ë²ˆì§¸ ë²”ì£¼ì— ì†í•˜ëŠ” ê´€ì¸¡ì¹˜ ë¹„ìœ¨            ì—”íŠ¸ë¡œí”¼ ì§€ìˆ˜(Entropy Index) : ë¶ˆìˆœë„ë¥¼ ì •ë³´ íšë“ì˜ ë¶ˆí™•ì‹¤ì„± ê°œë…ì— ê¸°ì´ˆí•˜ì—¬ ê³„ì‚°í•œ ì§€í‘œ\\[\\begin{aligned}  I(N_{k})  &amp;= -\\sum_{i=1}^{c}{\\left[p_{i} \\cdot \\log_{2}{p_{i}}\\right]}  \\end{aligned}\\]          $c$ : ë²”ì£¼ ê°¯ìˆ˜      $p_{i}$ : ë…¸ë“œ $N_{k}$ ì—ì„œ $i$ ë²ˆì§¸ ë²”ì£¼ì— ì†í•˜ëŠ” ê´€ì¸¡ì¹˜ ë¹„ìœ¨      ğŸ’¡ ê°€ì§€ì¹˜ê¸°(Pruning)      ì •ì˜ : ìì„¸í•˜ê²Œ êµ¬ë¶„ëœ ì˜ì—­ì„ í†µí•©í•¨ìœ¼ë¡œì¨ ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ê¸°ë²•    ì ˆì°¨          Full Tree ìƒì„±      ëª¨ë“  ë…¸ë“œì— ëŒ€í•˜ì—¬ ë¹„ìš© ë³µì¡ë„ ì§€ìˆ˜ ê³„ì‚°      ë¹„ìš© ë³µì¡ë„ ì§€ìˆ˜ê°€ ê°€ì¥ ë‚®ì€ ë…¸ë“œì— ëŒ€í•˜ì—¬ ê°€ì§€ì¹˜ê¸° ìˆ˜í–‰      2~3 ë‹¨ê³„ë¥¼ ë°˜ë³µí•˜ë©° ìµœì ì˜ $\\alpha$ íƒìƒ‰      ìµœì ì˜ $\\alpha$ í•˜ Tree ë„ì¶œ            ë¹„ìš© ë³µì¡ë„ ì§€ìˆ˜(Cost-Complexity)\\[\\begin{aligned}  R_{\\alpha}(T)  &amp;= L(T) + \\alpha \\cdot |\\text{leaf}(T)|\\\\  L(T)  &amp;= \\sum_{m=1}^{|\\text{leaf}(T)|}{\\sum_{\\overrightarrow{x}_{i} \\in R_{m}}{(y_{i}-\\hat{y}_{i})^2}}  \\end{aligned}\\]          $T$ : íƒ€ê¹ƒ ë…¸ë“œë¥¼ ë£¨íŠ¸ ë…¸ë“œë¡œ í•˜ëŠ” ì„œë¸ŒíŠ¸ë¦¬      $\\text{leaf}(T)$ : $T$ ì˜ ë¦¬í”„ ë…¸ë“œ ì§‘í•©      $R_{m} \\in \\text{leaf}(T)$ : $T$ ì˜ $m$ ë²ˆì§¸ ë¦¬í”„ ë…¸ë“œ      \\(\\overrightarrow{x}_{i} \\in R_{m}\\) : \\(R_{m}\\) ì— ì†í•œ \\(i\\) ë²ˆì§¸ ê´€ì¸¡ì¹˜ ë²¡í„°      $y_{i}$ : $\\overrightarrow{x}_{i}$ ì˜ ì‹¤ì œê°’      \\(\\hat{y}_{i}\\) : \\(\\overrightarrow{x}_{i}\\) ì˜ ì˜ˆì¸¡ê°’      $\\alpha$ : ê°€ì§€ì¹˜ê¸° ê°•ë„      $L(T)$ : $T$ ì˜ í›ˆë ¨ ê´€ì¸¡ì¹˜ì— ëŒ€í•œ ì˜ˆì¸¡ ì†ì‹¤      $R_{\\alpha}(T)$ : íƒ€ê¹ƒ ë…¸ë“œì˜ ë¹„ìš© ë³µì¡ë„ ì§€ìˆ˜      ğŸ’¡ DTR      ì¬ê·€ì  ë¶„ê¸°\\[\\begin{aligned}  \\hat{X}_{i}  &amp;= \\text{arg} \\min_{X_{i}}{\\{J(X_{1},\\hat{x}_{1}),J(X_{2},\\hat{x}_{2}),\\cdots,J(X_{n},\\hat{x}_{n})\\}}\\\\  \\hat{x}_{i}  &amp;= \\text{arg| \\min_{x_{i}}{J(X_{i},x_{i})}\\\\  J(X_{i},x_{i})  &amp;= \\frac{m_{left}}{m}L_{left}+\\frac{m_{right}}{m}L_{right}  \\end{aligned}\\]        ì°¨ì´ì  : ì†ì‹¤ í•¨ìˆ˜                      íŒë³„ ë¶„ì„ : ë¶ˆìˆœë„(Impurity)ë¥¼ ìµœì†Œí™”í•˜ë„ë¡ ë¶„ê¸°\\[\\begin{aligned}  L_{gini}(N_{k})  &amp;= 1-\\sum_{i=1}^{c}{p_{i}^2}  \\end{aligned}\\]                    íšŒê·€ ë¶„ì„ : ì˜¤ì°¨(Error)ë¥¼ ìµœì†Œí™”í•˜ë„ë¡ ë¶„ê¸°\\[\\begin{aligned}  L_{MSE}(N_{k})  &amp;= \\sum_{i=1}^{m}{(y_{i}-\\hat{y}_{i})^2}  \\end{aligned}\\]            ğŸ’¡ sklearn.tree.DecisionTreeClassifierfrom sklearn.tree import DecisionTreeClassifierGeneral HyperParameter  random_state(default : None)Model HyperParameter  max_features(default : None) : ê·œì¹™ ì„¤ê³„ ì‹œ ê³ ë ¤í•  ì„¤ëª…ë³€ìˆ˜ ê°¯ìˆ˜          'sqrt' : $\\sqrt{n}$      'log2' : $\\log_2{n}$      None : $n$      Recursive Partitioning  criterion(default : 'gini') : ê· ì¼ë„ ì¸¡ì • ë°©ë²•          'gini' : ì§€ë‹ˆì§€ìˆ˜      'entropy' : ì—”íŠ¸ë¡œí”¼ì§€ìˆ˜      Pruning  ccp_alpha(default : 0)To Prevent Overfitting  max_depth(default : None) : íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´  max_leaf_nodes(default : None) : ë¦¬í”„ ë…¸ë“œì˜ ìµœëŒ€ ê°¯ìˆ˜  min_samples_split(default : 2) : í•˜ìœ„ ë…¸ë“œë¡œ ê°€ì§€ë¥¼ ë»—ê¸° ìœ„í•´ í•„ìš”í•œ ìµœì†Œí•œì˜ ê´€ì¸¡ì¹˜ ê°¯ìˆ˜  min_impurity_decrease(default : 0) : í•˜ìœ„ ë…¸ë“œë¡œ ê°€ì§€ë¥¼ ë»—ê¸° ìœ„í•´ í•„ìš”í•œ ìµœì†Œí•œì˜ ë¶ˆìˆœë„ ê°œì„  ì •ë„  min_samples_leaf(default : 1) : ë¦¬í”„ ë…¸ë“œì˜ ê´€ì¸¡ì¹˜ ìµœì†Œ ê°¯ìˆ˜To Prevent Underfitting  class_weight(default : None) : ê°€ì¤‘í•  ë²”ì£¼ì™€ ê·¸ ê°’          'balanced'      dictionary type      "
  },
  
  {
    "title": "Supervised Model Selection",
    "url": "/posts/Supervised_Model_Selection/",
    "categories": "Artificial Intelligence, Machine Learning",
    "tags": "writing",
    "date": "2024-01-03 00:00:00 +0900",
    





    
    "snippet": "Classification MetricsConfusion Matrix  TP(True Positive) : ê¸ì •ìœ¼ë¡œ ì˜ˆì¸¡í•œ ê²ƒ(Possitive) ì¤‘ ì˜³ê²Œ ì˜ˆì¸¡í•œ(True) í•­ëª©  TN(True Negative) : ë¶€ì •ì¸ ê²ƒ(Negative) ì¤‘ ì˜³ê²Œ ì˜ˆì¸¡í•œ(True) í•­ëª©  FP(False Possitive) : ê¸ì •ìœ¼ë¡œ ì˜ˆì¸¡í•œ ê²ƒ(Possitiv...",
    "content": "Classification MetricsConfusion Matrix  TP(True Positive) : ê¸ì •ìœ¼ë¡œ ì˜ˆì¸¡í•œ ê²ƒ(Possitive) ì¤‘ ì˜³ê²Œ ì˜ˆì¸¡í•œ(True) í•­ëª©  TN(True Negative) : ë¶€ì •ì¸ ê²ƒ(Negative) ì¤‘ ì˜³ê²Œ ì˜ˆì¸¡í•œ(True) í•­ëª©  FP(False Possitive) : ê¸ì •ìœ¼ë¡œ ì˜ˆì¸¡í•œ ê²ƒ(Possitive) ì¤‘ ì˜ëª» ì˜ˆì¸¡í•œ(False) í•­ëª©  FN(False Negative) : ë¶€ì •ìœ¼ë¡œ ì˜ˆì¸¡í•œ ê²ƒ(Negative) ì¤‘ ì˜ëª» ì˜ˆì¸¡í•œ(False) í•­ëª©Sensitive to Threshold      ì •í™•ë„(Accuracy) : ì „ì²´ ê´€ì¸¡ì¹˜ ëŒ€ë¹„ ì˜³ê²Œ ì˜ˆì¸¡í•œ ê´€ì¸¡ì¹˜ ë¹„ìœ¨\\[\\frac{TP + TN}{TP + TN + FP + FN}\\]        ë¯¼ê°ë„(Sensitivity) í˜¹ì€ ì¬í˜„ìœ¨(Recall) : ì‹¤ì œ ê¸ì •ì¸ ê´€ì¸¡ì¹˜ ëŒ€ë¹„ ì˜³ê²Œ ì˜ˆì¸¡í•œ ê´€ì¸¡ì¹˜ ë¹„ìœ¨\\[\\frac{TP}{TP + FN}\\]        íŠ¹ì´ë„(Specificity) : ì‹¤ì œ ë¶€ì •ì¸ ê´€ì¸¡ì¹˜ ëŒ€ë¹„ ì˜³ê²Œ ì˜ˆì¸¡í•œ ê´€ì¸¡ì¹˜ ë¹„ìœ¨\\[\\frac{TN}{TN + FP}\\]        ì •ë°€ë„(Precision) : ê¸ì •ìœ¼ë¡œ ì˜ˆì¸¡í•œ ê´€ì¸¡ì¹˜ ëŒ€ë¹„ ì˜³ê²Œ ì˜ˆì¸¡í•œ ê´€ì¸¡ì¹˜ ë¹„ìœ¨\\[\\frac{TP}{TP + FP}\\]        F1-Score : ì¬í˜„ìœ¨ê³¼ ì •ë°€ë„ì˜ ì¡°í™” í‰ê· \\[2 \\times \\frac{precision \\times recall}{precision + recall}\\]          ì¬í˜„ìœ¨ : ì œ1ì¢… ì˜¤ë¥˜(ì°¸ì„ ê±°ì§“ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” ì˜¤ë¥˜; FN)ë¥¼ ê°•ì¡°í•˜ëŠ” ì§€í‘œ      ì •ë°€ë„ : ì œ2ì¢… ì˜¤ë¥˜(ê±°ì§“ì„ ì°¸ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” ì˜¤ë¥˜; FP)ë¥¼ ê°•ì¡°í•˜ëŠ” ì§€í‘œ      AUROC : Robust to Threshold      AUROC                      ROC Curve(Receiver Operating Characteristic Curve) : FPR ê°’ì— ë”°ë¥¸ TPRì˜ ë³€í™” ì¶”ì´ë¥¼ ë‚˜íƒ€ë‚¸ ê³¡ì„                     AUROC(Area Under ROC) : ROC Curve ì•„ë˜ ë©´ì                   ì´ìƒì  ë¶„ë¥˜ê¸°(Ideal Classifier) : 1          ë¬´ì‘ìœ„ ë¶„ë¥˜ê¸°(Random Classifier) : 0.5                          ê°œë… ì„¤ëª…                  FNR(False Negative Rate) : ì‹¤ì œ ê¸ì •ì¸ ê´€ì¸¡ì¹˜(TP+FN) ëŒ€ë¹„ ì˜ëª» ì˜ˆì¸¡í•œ ê´€ì¸¡ì¹˜(FN) ë¹„ìœ¨\\[\\begin{aligned}  FNR  &amp;=\\frac{FN}{TP+FN}  \\end{aligned}\\]                    TPR(True Positive Rate) : ì‹¤ì œ ê¸ì •ì¸ ê´€ì¸¡ì¹˜(TP+FN) ëŒ€ë¹„ ì˜³ê²Œ ì˜ˆì¸¡í•œ ê´€ì¸¡ì¹˜(TP) ë¹„ìœ¨\\[\\begin{aligned}  TPR  &amp;=\\frac{TP}{TP+FN}\\\\  &amp;= 1-FNR  \\end{aligned}\\]                    FPR(False Possitive Rate) : ì‹¤ì œ ë¶€ì •ì¸ ê´€ì¸¡ì¹˜(TN+FP) ëŒ€ë¹„ ì˜ëª» ì˜ˆì¸¡í•œ ê´€ì¸¡ì¹˜(FP) ë¹„ìœ¨\\[\\begin{aligned}  FPR  &amp;=\\frac{FP}{TN+FP}  \\end{aligned}\\]                    TNR(True Negative Rate) : ì‹¤ì œ ë¶€ì •ì¸ ê´€ì¸¡ì¹˜(TN+FP) ëŒ€ë¹„ ì˜³ê²Œ ì˜ˆì¸¡í•œ ê´€ì¸¡ì¹˜(TN) ë¹„ìœ¨\\[\\begin{aligned}  TNR  &amp;=\\frac{TN}{TN+FP}\\\\  &amp;= 1-FPR  \\end{aligned}\\]            Regression Metrics      Average Error(AE)\\[AE=\\frac{1}{n}\\sum_{i=1}^{n}{y_{i}-\\hat{y}_{i}}\\]          ì •ì˜ : ì˜¤ì°¨ì˜ í•©ê³„      í•œê³„ì  : ì˜¤ì°¨ì˜ ë°©í–¥ì— ë”°ë¥¸ í¬ê¸° ìƒì‡„ ê°€ëŠ¥ì„±            Mean Squared Error(MSE) : ì˜¤ì°¨ ììŠ¹ì˜ í‰ê· \\[MSE = \\frac{1}{n}\\sum_{i=1}^{n}{(y_{i}-\\hat{y}_{i})^2}\\]        Root Mean Squared Error(RMSE) : ì˜¤ì°¨ ììŠ¹ì˜ í‰ê· ì˜ ììŠ¹ê·¼\\[RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}{(y_{i}-\\hat{y}_{i})^2}}\\]        Mean Absolute Error(MAE) : ì˜¤ì°¨ ì ˆëŒ€ê°’ì˜ í‰ê· \\[MAE = \\frac{1}{n}\\sum_{i=1}^{n}{|y_{i}-\\hat{y}_{i}|}\\]        Mean Absolute Percentage Error(MAPE) : ì‹¤ì œê°’ ëŒ€ë¹„ ì˜¤ì°¨ ë¹„ìœ¨ ì ˆëŒ€ê°’ì˜ í‰ê· \\[MAPE = \\frac{1}{n}\\sum_{i=1}^{n}|\\frac{y_{i}-\\hat{y}_{i}}{y_{i}}|\\]  Splitì¼ë°˜í™”ì˜ ë¬¸ì œ  ëª¨ë¸ë§ ëª©ì  : ì¼ë°˜í™”          ì¼ë°˜í™”(Generalization) : ëª¨ë¸ì´ í›ˆë ¨ ê´€ì¸¡ì¹˜ì—ì„œ í•™ìŠµí•œ íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ì´ì „ì— ë³´ì§€ ëª»í•œ ê´€ì¸¡ì¹˜ì— ëŒ€í•˜ì—¬ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ            ë¬¸ì œì  : ê³¼ëŒ€ì í•© í˜„ìƒ              ê³¼ëŒ€ì í•©(Overfitting) : ëª¨ë¸ì´ ì¼ë°˜ì ì´ì§€ ì•Šì€, ì¦‰ í›ˆë ¨ ê´€ì¸¡ì¹˜ì—ì„œë§Œ í¬ì°©ë˜ëŠ” ë…¸ì´ì¦ˆë‚˜ ì´ìƒì¹˜ê¹Œì§€ í•™ìŠµí•˜ì—¬ ì‹ ê·œ ê´€ì¸¡ì¹˜ì— ëŒ€í•´ì„œëŠ” ì œëŒ€ë¡œ ê¸°ëŠ¥í•˜ì§€ ëª»í•˜ëŠ” ìƒíƒœ      ê³¼ì†Œì í•©(Underfitting) : ëª¨ë¸ì´ í›ˆë ¨ ê´€ì¸¡ì¹˜ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ì¼ë°˜ì ì¸ íŒ¨í„´ì„ ì¶©ë¶„íˆ í•™ìŠµí•˜ì§€ ëª»í•˜ì—¬ ê´€ì¸¡ì¹˜ì˜ ë‹¤ì–‘ì„±ê³¼ ë³µì¡ì„±ì„ ì¡ì•„ë‚´ì§€ ëª»í•˜ëŠ” ìƒíƒœ            í•´ê²° ë°©ë²• : $E_{gen}$ ìµœì†Œí™”                      Training Error : Training Data Set ì— ëŒ€í•œ ì˜¤ì°¨\\[E_{trn} = \\sum^{N_{trn}}_{i=1}{L(y_{i},\\hat{y}_{i})}\\]                    Generalization Error : Unseen Data Set ì— ëŒ€í•œ ì˜¤ì°¨\\[E_{gen}=\\int{L(y_{i},\\hat{y}_{i})}\\]            ëª¨ìˆ˜ì˜ ì¶”ì •  $E_{gen}$ ì¸¡ì • ìƒì˜ ë¬¸ì œì           Unseen Data Set ìì²´ì— ëŒ€í•´ì„œ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì´ìƒì ì¸ ê°œë…ì„      í•´ë‹¹ ëª¨ìˆ˜ë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•˜ì—¬ ì¶”ì •ëŸ‰ $E_{val}$, $E_{tst}$ ë¥¼ ì œì‹œí•¨            Split Seen Data Set              Training : ëª¨ë¸ í›ˆë ¨ ì‹œ ì‚¬ìš©í•˜ëŠ” í‘œë³¸ìœ¼ë¡œì„œ, í•´ë‹¹ í‘œë³¸ìœ¼ë¡œë¶€í„° $E_{val}$ ì„ ì¶”ì •í•¨      Validation : ëª¨ë¸ ê°„ ì„±ëŠ¥ ë¹„êµ ì‹œ ì‚¬ìš©í•˜ëŠ” í‘œë³¸ìœ¼ë¡œì„œ, í•´ë‹¹ í‘œë³¸ìœ¼ë¡œë¶€í„° $E_{tst}$ ë¥¼ ì¶”ì •í•¨      Test : ìµœì¢… ì„ íƒëœ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì • ì‹œ ì‚¬ìš©í•˜ëŠ” í‘œë³¸ìœ¼ë¡œì„œ, í•´ë‹¹ í‘œë³¸ë¡œë¶€í„° $E_{gen}$ ë¥¼ ì¶”ì •í•¨      Cross Validation      êµì°¨ ê²€ì¦(Cross Validation)              ì •ì˜ : í‘œë³¸ì„ ì—¬ëŸ¬ ì„¸íŠ¸ë¡œ ë‚˜ëˆ„ì–´ ëª¨ë¸ì„ ì—¬ëŸ¬ ë²ˆ í•™ìŠµí•˜ê³  í‰ê°€í•¨ìœ¼ë¡œì¨ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ì¸¡ì •í•˜ëŠ” ì ˆì°¨      í•„ìš”ì„± : Training ì—ì„œ Test ë¥¼ ë¶„ë¦¬í•œ ìƒíƒœì—ì„œ Validation ì„ ì¬ì°¨ ë¶„ë¦¬í•˜ê¸°ì—ëŠ” í•™ìŠµì— ì‚¬ìš©í•  í‘œë³¸ í¬ê¸°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŒ            LOOCV(Leave-One-Out Cross Validation)              $n$ ê°œì˜ í‘œë³¸ì„ $n-1$ ê°œì˜ training ê³¼ $1$ ê°œì˜ validation ìœ¼ë¡œ ë‚˜ëˆ„ì–´ $n$ ë²ˆ í•™ìŠµí•˜ëŠ” ë°©ì‹            k-Fold Cross Validation              $n$ ê°œì˜ í‘œë³¸ì„ $k$ ê°œì˜ ë°ì´í„° ì„¸íŠ¸ë¡œ ë‚˜ëˆ„ê³ , $k-1$ ê°œëŠ” training ìœ¼ë¡œ, $1$ ê°œëŠ” validation ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ $k$ ë²ˆ í•™ìŠµí•˜ëŠ” ë°©ì‹      "
  },
  
  {
    "title": "Data Preprocessing",
    "url": "/posts/Data_Preprocessing/",
    "categories": "Artificial Intelligence, Machine Learning",
    "tags": "writing",
    "date": "2024-01-02 00:00:00 +0900",
    





    
    "snippet": "ğŸ’¡ Data Preprocessing      ì •ì˜ : ë°ì´í„°ë¥¼ ë¶„ì„ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•ì‹ì˜ ë°ì´í„°ë¡œ ë§Œë“œëŠ” ì¼ë ¨ì˜ ê³¼ì •    í•„ìš”ì„± : ë¶„ì„ì— ì™„ë²½í•˜ê²Œ ì í•©í•œ ë°ì´í„°ë¥¼ ì–»ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•¨          í˜¸í™˜ì„± ë¬¸ì œ                  ë°ì´í„° ë¶ˆì¼ì¹˜          ë°ì´í„° ì¤‘ë³µ                    ë°ì´í„° ìˆ˜ì§‘ ë¬¸ì œ     ...",
    "content": "ğŸ’¡ Data Preprocessing      ì •ì˜ : ë°ì´í„°ë¥¼ ë¶„ì„ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•ì‹ì˜ ë°ì´í„°ë¡œ ë§Œë“œëŠ” ì¼ë ¨ì˜ ê³¼ì •    í•„ìš”ì„± : ë¶„ì„ì— ì™„ë²½í•˜ê²Œ ì í•©í•œ ë°ì´í„°ë¥¼ ì–»ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•¨          í˜¸í™˜ì„± ë¬¸ì œ                  ë°ì´í„° ë¶ˆì¼ì¹˜          ë°ì´í„° ì¤‘ë³µ                    ë°ì´í„° ìˆ˜ì§‘ ë¬¸ì œ                  ì„¼ì„œì™€ ë°ì´í„°ë² ì´ìŠ¤ ê°„ í†µì‹  ë¬¸ì œ          ì„¼ì„œ ìì²´ ë¬¸ì œ          ìƒ˜í”Œë§ ê¸°ë°˜ ë°ì´í„° ìˆ˜ì§‘ ì •ì±…                      ë°ì´í„° í’ˆì§ˆì— ì˜í–¥ì„ ë¼ì¹˜ëŠ” ì¸ì          Nosie : ë°ì´í„° ì¸¡ì • ì‹œ ë¬´ì‘ìœ„ë¡œ ë°œìƒí•˜ì—¬ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚¤ëŠ” ë¬¸ì œ      Outlier : ëŒ€ë¶€ë¶„ì˜ ë°ì´í„°ì™€ ë‹¤ë¥¸ íŠ¹ì„±ì„ ë³´ì´ê±°ë‚˜ íŠ¹ì • ì†ì„±ì˜ ê°’ì´ ìœ ë³„ë‚œ ë°ì´í„°      Artifact : ì–´ë–¤ ìš”ì¸ìœ¼ë¡œ ì¸í•´ ë°˜ë³µì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ì™œê³¡ì´ë‚˜ ì˜¤ë¥˜      Precision : ë™ì¼í•œ ê²°ê³¼ë¬¼ì„ ë°˜ë³µì ìœ¼ë¡œ ì¸¡ì •í•˜ì˜€ì„ ë•Œ ê° ì¸¡ì •ê°’ ì‚¬ì´ì˜ ì¼ê´€ì„± ë¬¸ì œ      Bias : ì¸¡ì • ì¥ë¹„ì— í¬í•¨ëœ ì‹œìŠ¤í…œ ìƒ ë¬¸ì œ      Accuacy : ì¸¡ì • ì¥ë¹„ì˜ í•œê³„ë¡œ ì •í™•í•˜ì§€ ì•Šì€ ìˆ˜ë¥¼ ì¸¡ì •í•¨ì— ë”°ë¼ ë°œìƒí•˜ëŠ” ë¬¸ì œ      Inconsistent Value : ë°ì´í„° ë¶ˆì¼ì¹˜ ë¬¸ì œ      Duplicate : ë°ì´í„° ì¤‘ë³µ ë¬¸ì œ        ì ˆì°¨                  Data Integration : ë™ì¼í•œ ë‹¨ìœ„, ì–‘ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ê²°í•©í•˜ëŠ” ì ˆì°¨            Data Cleansing : ë‚®ì€ í’ˆì§ˆì˜ ë°ì´í„°ë¥¼ í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì ˆì°¨                  ì¤‘ë³µê°’ ì œê±°          ê²°ì¸¡ì¹˜ ì²˜ë¦¬          ì´ìƒì¹˜ ì²˜ë¦¬                    Data Transformation : ë°ì´í„° í˜•ì‹ ë° êµ¬ì¡°ë¥¼ í•™ìŠµì— ì í•©í•˜ë„ë¡ ë³€í™˜í•˜ëŠ” ì ˆì°¨                  í‘œì¤€í™”(Standardization)          ì •ê·œí™”(Normalization)                    Data Reduction : ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì› ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ì ˆì°¨      ğŸ’¡ Data Cleansingê²°ì¸¡ì¹˜ ì²˜ë¦¬  ê²°ì¸¡ì¹˜ ì¢…ë¥˜          ì™„ì „ ë¬´ì‘ìœ„ ê²°ì¸¡(Missing Completely At Random; MCAR) : ë°ì´í„°ê°€ ì–´ë–¤ íŒ¨í„´ì´ë‚˜ ê·œì¹™ ì—†ì´ ëˆ„ë½ë˜ëŠ” ê²½ìš°      ë¬´ì‘ìœ„ ê²°ì¸¡(Missing At Random; MAR) : ë°ì´í„°ì˜ ëˆ„ë½ì´ ë‹¤ë¥¸ ë³€ìˆ˜ì— ì¢…ì†ëœ ê²½ìš°      ë¹„ë¬´ì‘ìœ„ ê²°ì¸¡(Missing Not At Random; MNAR) : ê²°ì¸¡ì¹˜ê°€ ì–´ë–¤ ê·œì¹™ ë˜ëŠ” íŒ¨í„´ì„ ë”°ë¼ ë°œìƒí•˜ëŠ” ê²½ìš°        ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ê¶Œì¥ ì‚¬í•­          10% ë¯¸ë§Œ : ì œê±° ë˜ëŠ” ëŒ€ì²´      10~20% : ìµœë¹ˆê°’, í‰ê· , ì¤‘ì•™ê°’ ë“±ìœ¼ë¡œ ëŒ€ì²´      20% ì´ìƒ : model-based method      ì´ìƒì¹˜ ì²˜ë¦¬      ì´ìƒì¹˜ì˜ ì •ì˜ : ê´€ì¸¡ëœ ë°ì´í„°ì˜ ë²”ìœ„ì—ì„œ ì§€ë‚˜ì¹˜ê²Œ ë²—ì–´ë‚˜ ê°’ì´ ë§¤ìš° í¬ê±°ë‚˜ ì‘ì€ ê°’        ì´ìƒì¹˜ì˜ íƒì§€ : Turkey Fence ê¸°ë²•    \\[\\begin{aligned}  Outliers  &amp;=\\{X|X &lt; X_{lower} \\; or \\; X &gt; X_{upper}\\}  \\end{aligned}\\]          í•˜í•œê°’($X_{lower}$) : $Q_1-IQR \\times 1.5$      ìƒí•œê°’($X_{upper}$) : $Q_3+IQR \\times 1.5$      ì‚¬ë¶„ìœ„ ë²”ìœ„(InterQuartile Range; IQR) : $Q_3 - Q_1$      ğŸ’¡ Data Transformationz-Score ì •ê·œí™”from sklearn.preprocessing import StandardScaler      ì •ì˜ : ê°’ì˜ ë¶„í¬ë¥¼ í‰ê· ì´ 0, ë¶„ì‚°ì´ 1ì¸ í˜•íƒœë¡œ ë³€í™˜í•¨\\[X_{scaled}=\\frac{X_{origin}-E(X)}{\\sigma}\\]        ê¸°ëŠ¥ : ì •ê·œ ë¶„í¬í™”  (ì´ìƒì¹˜) ê°•ê±´ ì •ê·œí™”from sklearn.preprocessing import RobustScaler      ì •ì˜ : í‰ê· ê³¼ ë¶„ì‚° ëŒ€ì‹  ì¤‘ì•™ê°’ê³¼ ì‚¬ë¶„ìœ„ ë²”ìœ„ë¥¼ í™œìš©í•¨\\[X_{scaled}=\\frac{X_{origin}-median}{IQR}\\]        ê¸°ëŠ¥ : ì´ìƒì¹˜ ì˜í–¥ë ¥ ìµœì†Œí™”  ìµœëŒ€-ìµœì†Œ ì •ê·œí™”from sklearn.preprocessing import MinMaxScaler      ì •ì˜ : ê°’ì˜ ë¶„í¬ë¥¼ íŠ¹ì • ë²”ìœ„ë¡œ í™•ëŒ€ í˜¹ì€ ì¶•ì†Œí•¨\\[X_{scaled}=\\frac{X_{origin}-X_{min}}{X_{max}-X_{min}}\\]        ê¸°ëŠ¥ : ì²™ë„ í†µì¼  "
  },
  
  {
    "title": "What? Data Science",
    "url": "/posts/Data_Science/",
    "categories": "Artificial Intelligence, Machine Learning",
    "tags": "writing",
    "date": "2024-01-01 00:00:00 +0900",
    





    
    "snippet": "ğŸ’¡ Data-Driven Decision Makingë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì •  Descriptive : Explains What Happend          Comprehensive, Accurate, Live Data      Effective Visualisation        Diagnostic : Explains Why It Happend   ...",
    "content": "ğŸ’¡ Data-Driven Decision Makingë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì •  Descriptive : Explains What Happend          Comprehensive, Accurate, Live Data      Effective Visualisation        Diagnostic : Explains Why It Happend          Ability to Drill Down to the Root-cause      Ability to Isolate All Confounding Information        Predictive : Forcasts What Might Happned          Business Have Remained Fairly Consistent Over Time      Historical Patterns Being Used to Predict Specific Outcomes Using Algorithms      Decisions are Automated Using Algorithms and Tech.        Prescriptive : What Do I Need to Do?          Recommends Action Based On The Forecast      Applying Advanced Analytical Techs to Make Specific Recommendatons      ë°ì´í„° ê¸°ë°˜ ë¬¸ì œ í•´ê²° ê³¼ì •      ë¬¸ì œ ì •ì˜          ì–´ë–¤ ë¬¸ì œë¥¼ í•´ê²°í•  ê²ƒì¸ê°€?ì´ë¥¼ ìœ„í•´ í•„ìš”í•œ ë°ì´í„°ëŠ” ë¬´ì—‡ì¸ê°€?            ë°ì´í„° íšë“          ì–´ë–»ê²Œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  ê²ƒì¸ê°€?            ë°ì´í„° íƒìƒ‰          ë°ì´í„° ì „ì²˜ë¦¬íƒìƒ‰ì  ìë£Œ ë¶„ì„            ëª¨ë¸ë§          ë¬¸ì œì— ë§ëŠ” ê¸°ê³„í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ì„ íƒëª¨ë¸ êµ¬ì¶•            ë°°í¬          ì œí’ˆ ë°°í¬ ë° ì‹œìŠ¤í…œ ìœ ì§€ ë³´ìˆ˜      ğŸ’¡ Data Scienceë°ì´í„° ê³¼í•™ì˜ ì´í•´      ì •ì˜          ì •í˜•, ë¹„ì •í˜•ì˜ ë‹¤ì–‘í•œ ë°ì´í„°ë¡œë¶€í„° ì§€ì‹ ë° ì‹œì‚¬ì ì„ ë„ì¶œí•˜ëŠ” ë° ê³¼í•™ì  ë°©ë²•ë¡ ì„ ë™ì›í•˜ëŠ” ìœµí•© ë¶„ì•¼(ì¶œì²˜ : ìœ„í‚¤ë°±ê³¼)            ì£¼ìš” ê°œë…                  ë¹…ë°ì´í„°(Bigdata)                  í†µìƒì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë°ì´í„° ìˆ˜ì§‘, ê´€ë¦¬, ì²˜ë¦¬ ì†Œí”„íŠ¸ì›¨ì–´ì˜ ìˆ˜ìš© í•œê³„ë¥¼ ë„˜ì–´ì„œëŠ” í¬ê¸°ì˜ ë°ì´í„°(ì¶œì²˜ : ìœ„í‚¤ë°±ê³¼)                          Volume(Data Quantity)          Variety(Data Types)          Velocity(Data Speed)          Value(Data Impact)                            ë°ì´í„° ë§ˆì´ë‹(Data Mining)                  ëŒ€ê·œëª¨ë¡œ ì €ì¥ëœ ë°ì´í„° ì•ˆì—ì„œ ì²´ê³„ì ì´ê³  ìë™ì ìœ¼ë¡œ í†µê³„ì  ê·œì¹™ì´ë‚˜ ì§œì„ì„ ë¶„ì„í•˜ì—¬ ê°€ì¹˜ ìˆëŠ” ì •ë³´ë¥¼ ë¹¼ë‚´ëŠ” ê³¼ì •(ì¶œì²˜ : ìœ„í‚¤ë°±ê³¼)                            ê¸°ê³„í•™ìŠµ(Machine Learning)                  ê¸°ê³„ê°€ ì¼ì¼ì´ ì½”ë“œë¡œ ëª…ì‹œí•˜ì§€ ì•Šì€ ë™ì‘ì„ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµí•˜ì—¬ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ê°œë°œí•˜ëŠ” ì—°êµ¬ ë¶„ì•¼(ì¶œì²˜ : ìœ„í‚¤ë°±ê³¼)                            ì¸ê³µì§€ëŠ¥(Artificial Intelligence; AI)                  ì¸ê°„ì˜ í•™ìŠµ, ì¶”ë¡ , ì§€ê° ëŠ¥ë ¥ì„ ì¸ê³µì ìœ¼ë¡œ êµ¬í˜„í•˜ë ¤ëŠ” ì»´í“¨í„° ê³¼í•™ì˜ ì„¸ë¶€ ë¶„ì•¼(ì¶œì²˜ : ìœ„í‚¤ë°±ê³¼)                    ê¸°ê³„í•™ìŠµì˜ ë¶„ë¥˜  ì§€ë„í•™ìŠµ(Supervised Learning)                  ì •ì˜ : ì •ë‹µ ì„¸íŠ¸ê°€ ì¡´ì¬í•˜ëŠ” ë°ì´í„°ë¥¼ í™œìš©í•˜ëŠ” í•™ìŠµ ë°©ë²•                    ë¶„ë¥˜                  íŒë³„ ë¶„ì„(Classificaiton) : ë²”ì£¼í˜• ê°’ì„ ê°€ì§€ëŠ” ì¢…ì†ë³€ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ë¡           íšŒê·€ ë¶„ì„(Regression) : ìˆ˜ì¹˜í˜• ê°’ì„ ê°€ì§€ëŠ” ì¢…ì†ë³€ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ë¡                       ë¹„ì§€ë„í•™ìŠµ(Unsupervised Learning)                  ì •ì˜ : ì •ë‹µ ì„¸íŠ¸ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ë¥¼ í™œìš©í•˜ëŠ” í•™ìŠµ ë°©ë²•                    ë¶„ë¥˜                  êµ°ì§‘í™”(Clustering) : ìœ ì‚¬í•œ ê°œì²´ë“¤ì˜ ì§‘ë‹¨ì„ ë§Œë“  í›„ ìƒˆ ê°œì²´ê°€ ì–´ë–¤ ì§‘ë‹¨ê³¼ ìœ ì‚¬í•œì§€ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ë¡           ì°¨ì›ì¶•ì†Œ(Dimension Reduction) : ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì› ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ë¡                     ğŸ’¡ Scikit-Learn Library      API ì‚¬ìš© ë°©ë²•          ì ì ˆí•œ ì•Œê³ ë¦¬ì¦˜ í´ë˜ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸°      ì¸í„°í˜ì´ìŠ¤ì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì ì ˆí•œ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±      ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¬¸ì œì§€(Feature)ì™€ ì •ë‹µì§€(Traget)ë¡œ ë°°ì¹˜      fit() ì„ í†µí•´ ì¸ìŠ¤í„´ìŠ¤ë¥¼ í•™ìŠµìš© ë°ì´í„° ì„¸íŠ¸ë¡œ í›ˆë ¨      predict() ì„ í†µí•´ í›ˆë ¨ëœ ì¸ìŠ¤í„´ìŠ¤ì— í‰ê°€ìš© ë°ì´í„° ì„¸íŠ¸ë¥¼ ì ìš©í•˜ì—¬ ì„±ëŠ¥ í‰ê°€      ëª¨ë“ˆ      ì•Œê³ ë¦¬ì¦˜                            ëª¨ë“ˆ          ì„¤ëª…          ì˜ˆì‹œ                                      sklearn.tree          ê²°ì • íŠ¸ë¦¬ ì•Œê³ ë¦¬ì¦˜ ì œê³µ          Decision Tree ë“±                          sklearn.neighbors          ìµœê·¼ì ‘ ì´ì›ƒ ì•Œê³ ë¦¬ì¦˜ ì œê³µ          K-NN ë“±                          sklearn.svm          ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹  ì•Œê³ ë¦¬ì¦˜ ì œê³µ          Â                           sklearn.naive_bayes          ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ì•Œê³ ë¦¬ì¦˜ ì œê³µ          ê°€ìš°ì‹œì•ˆ NB, ë‹¤í•­ ë¶„í¬ NB ë“±                          sklearn.cluster          í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ ì œê³µ          K-Means, ê³„ì¸µí˜• í´ëŸ¬ìŠ¤í„°ë§, DBSCAN ë“±                          sklearn.linear_model          íšŒê·€ë¶„ì„ ì•Œê³ ë¦¬ì¦˜ ì œê³µ          ì„ í˜• íšŒê·€, í™•ë¥ ì  ê²½ì‚¬í•˜ê°• íšŒê·€(SGD), ë¦¿ì§€(Ridge), ë¼ì˜(Lasso), ë¡œì§€ìŠ¤í‹± íšŒê·€ ë“±                          sklearn.decomposition          ì°¨ì› ì¶•ì†Œ ì•Œê³ ë¦¬ì¦˜ ì œê³µ          PCA, NMF, Truncated SVD ë“±                          sklearn.ensemble          ì•™ìƒë¸” ì•Œê³ ë¦¬ì¦˜ ì œê³µ          Random Forest, AdaBoost, GradientBoost ë“±                          ì „ì²˜ë¦¬                            ëª¨ë“ˆ          ì„¤ëª…          ì˜ˆì‹œ                                      sklearn.preprocessing          ë°ì´í„° ì „ì²˜ë¦¬ ê¸°ëŠ¥ ì œê³µ          ì¸ì½”ë”, ìŠ¤ì¼€ì¼ëŸ¬ ë“±                          sklearn.feature_selection          íŠ¹ì„±(feature)ì„ ì„ íƒí•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ ì œê³µ          Â                           sklearn.feature_extraction          íŠ¹ì„±(feature)ì„ ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ ì œê³µ          Â                           sklearn.pipeline          íŠ¹ì„± ì²˜ë¦¬, í•™ìŠµ, ì˜ˆì¸¡ì„ ë¬¶ì–´ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ ì œê³µ          Â                           ê²€ì¦ ë° ì„±ëŠ¥ í‰ê°€ ì§€í‘œ                            ëª¨ë“ˆ          ì„¤ëª…          ì˜ˆì‹œ                                      sklearn.model_selection          êµì°¨ ê²€ì¦, ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ì¶œ API ì œê³µ          GridSearch ë“±                          sklearn.metrics          ì„±ëŠ¥ í‰ê°€ ì§€í‘œ ì œê³µ          Accuracy, Precision, Recall, ROC-AUC, RMSE ë“±                    ë‚´ì¥ ë°ì´í„° ì„¸íŠ¸sklearn.datasets      ë‚´ì¥ ë°ì´í„° í˜•ì‹                            ì´ë¦„          ì„¤ëª…                                      DESCR          ìë£Œì— ëŒ€í•œ ì„¤ëª…                          data          ì„¤ëª… ë³€ìˆ˜                          target          ë°˜ì‘ ë³€ìˆ˜                          feature_names          ì„¤ëª… ë³€ìˆ˜ ì´ë¦„ ë¦¬ìŠ¤íŠ¸                          target_names          ë°˜ì‘ ë³€ìˆ˜ ì´ë¦„ ë¦¬ìŠ¤íŠ¸                          ë‚´ì¥ ë°ì´í„° ì„¸íŠ¸ ëª©ë¡                            ë°ì´í„° ë¡œë“œ í•¨ìˆ˜          ë°ì´í„°          ì°¸ê³                                       load_boston          ë³´ìŠ¤í„´ ì§‘ê°’          ë‚´ì¥ ë°ì´í„°                          load_diabetes          ë‹¹ë‡¨ë³‘          Â                           load_linnerud          linnerud          Â                           load_iris          ë¶“ê½ƒ          Â                           load_digits          í•„ê¸° ìˆ«ì(digit) ì´ë¯¸ì§€          Â                           load_wine          í¬ë„ì£¼(wine) ë“±ê¸‰          Â                           load_breast_cancer          ìœ ë°©ì•” ì§„ë‹¨          Â                           fetch_california_housing          ìº˜ë¦¬í¬ë‹ˆì•„ ì§‘ê°’          ì¸í„°ë„· ë‹¤ìš´ë¡œë“œ                          fetch_covtype          í† ì§€ì¡°ì‚¬          Â                           fetch_20newsgroups          ë‰´ìŠ¤ ê·¸ë£¹ í…ìŠ¤íŠ¸          Â                           fetch_olivetti_faces          ì–¼êµ´ ì´ë¯¸ì§€          Â                           fetch_lfw_people          ìœ ëª…ì¸ ì–¼êµ´          Â                           fetch_lfw_pairs          ìœ ëª…ì¸ ì–¼êµ´          Â                           fetch_rcv1          ë¡œì´í„° ë‰´ìŠ¤ ë§ë­‰ì¹˜          Â                           fetch_kddcup99          Kddcup 99 Tcp dump          Â                           make_regression          íšŒê·€ë¶„ì„ìš©          ê°€ìƒ ë°ì´í„°                          make_classification          ë¶„ë¥˜ìš©          Â                           make_blobs          í´ëŸ¬ìŠ¤í„°ë§ìš©          Â                     "
  }
  
]

